{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from neuralnilm.data.loadactivations import load_nilmtk_activations\n",
    "from neuralnilm.data.syntheticaggregatesource import SyntheticAggregateSource\n",
    "from neuralnilm.data.datapipeline import DataPipeline\n",
    "from neuralnilm.data.processing import DivideBy, IndependentlyCenter\n",
    "from neuralnilm.data.datathread import DataThread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NILMTK_FILENAME = '/data/mine/vadeec/merged/ukdale.h5'\n",
    "TARGET_APPLIANCE = 'kettle'\n",
    "SEQ_LENGTH = 256\n",
    "SAMPLE_PERIOD = 6\n",
    "STRIDE = SEQ_LENGTH\n",
    "WINDOWS = {\n",
    "    'train': {\n",
    "        1: (\"2014-01-01\", \"2014-02-01\")\n",
    "    },\n",
    "    'unseen_activations_of_seen_appliances': {\n",
    "        1: (\"2014-02-02\", \"2014-02-08\")                \n",
    "    },\n",
    "    'unseen_appliances': {\n",
    "        2: (\"2013-06-01\", \"2013-06-07\")\n",
    "    }\n",
    "}\n",
    "\n",
    "LOADER_CONFIG = {\n",
    "    'nilmtk_activations': dict(\n",
    "        appliances=['kettle', 'microwave', 'washing machine'],\n",
    "        filename=NILMTK_FILENAME,\n",
    "        sample_period=SAMPLE_PERIOD,\n",
    "        windows=WINDOWS\n",
    "    )\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuralnilm.data.stridesource import StrideSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.data.stridesource:Loading NILMTK data...\n",
      "INFO:neuralnilm.data.stridesource:Loading data for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.stridesource:Loaded data from building UK-DALE_building_1 for fold train from 2014-01-01 00:00:06+00:00 to 2014-01-31 23:59:54+00:00.\n",
      "INFO:neuralnilm.data.stridesource:Loading data for UK-DALE_building_2...\n",
      "INFO:neuralnilm.data.stridesource:Loaded data from building UK-DALE_building_2 for fold unseen_appliances from 2013-06-01 00:00:00+01:00 to 2013-06-06 23:59:54+01:00.\n",
      "INFO:neuralnilm.data.stridesource:Loading data for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.stridesource:Loaded data from building UK-DALE_building_1 for fold unseen_activations_of_seen_appliances from 2014-02-02 00:00:00+00:00 to 2014-02-07 23:59:54+00:00.\n",
      "INFO:neuralnilm.data.stridesource:Done loading NILMTK mains data.\n"
     ]
    }
   ],
   "source": [
    "stride_source = StrideSource(\n",
    "    target_appliance=TARGET_APPLIANCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    filename=NILMTK_FILENAME,\n",
    "    windows=WINDOWS,\n",
    "    sample_period=SAMPLE_PERIOD,\n",
    "    stride=STRIDE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.data.loadactivations:Loading NILMTK activations...\n",
      "INFO:neuralnilm.data.loadactivations:Loading kettle for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 111 kettle activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Loading microwave for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 114 microwave activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Loading washing machine for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 23 washing machine activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Loading kettle for UK-DALE_building_2...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 31 kettle activations from UK-DALE_building_2.\n",
      "INFO:neuralnilm.data.loadactivations:Loading microwave for UK-DALE_building_2...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 31 microwave activations from UK-DALE_building_2.\n",
      "INFO:neuralnilm.data.loadactivations:Loading washing machine for UK-DALE_building_2...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 2 washing machine activations from UK-DALE_building_2.\n",
      "INFO:neuralnilm.data.loadactivations:Loading kettle for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 28 kettle activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Loading microwave for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 30 microwave activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Loading washing machine for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 5 washing machine activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Done loading NILMTK activations.\n"
     ]
    }
   ],
   "source": [
    "nilmtk_activations = load_nilmtk_activations(**LOADER_CONFIG['nilmtk_activations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'unseen_activations_of_seen_appliances', 'unseen_appliances']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nilmtk_activations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuralnilm.data.realaggregatesource import RealAggregateSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.data.realaggregatesource:Loading NILMTK mains...\n",
      "INFO:neuralnilm.data.realaggregatesource:Loading mains for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.realaggregatesource:Loaded mains data from building UK-DALE_building_1 for fold train from 2014-01-01 00:00:00+00:00 to 2014-01-31 23:59:54+00:00.\n",
      "INFO:neuralnilm.data.realaggregatesource:Loading mains for UK-DALE_building_2...\n",
      "INFO:neuralnilm.data.realaggregatesource:Loaded mains data from building UK-DALE_building_2 for fold unseen_appliances from 2013-06-01 00:00:00+01:00 to 2013-06-06 23:59:54+01:00.\n",
      "INFO:neuralnilm.data.realaggregatesource:Loading mains for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.realaggregatesource:Loaded mains data from building UK-DALE_building_1 for fold unseen_activations_of_seen_appliances from 2014-02-02 00:00:00+00:00 to 2014-02-07 23:59:54+00:00.\n",
      "INFO:neuralnilm.data.realaggregatesource:Done loading NILMTK mains data.\n",
      "INFO:neuralnilm.data.realaggregatesource:Found 89 sections without target for train UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.realaggregatesource:Found 20 sections without target for unseen_activations_of_seen_appliances UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.realaggregatesource:Found 24 sections without target for unseen_appliances UK-DALE_building_2.\n"
     ]
    }
   ],
   "source": [
    "ras = RealAggregateSource(\n",
    "    activations=nilmtk_activations,\n",
    "    target_appliance=TARGET_APPLIANCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    filename=NILMTK_FILENAME,\n",
    "    windows=WINDOWS,\n",
    "    sample_period=SAMPLE_PERIOD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ras.target_inclusion_prob = 0.5\n",
    "for i in range(50):\n",
    "    seq = ras.get_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(2)\n",
    "#axes[0].plot(seq.input)\n",
    "#axes[1].plot(seq.target)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'building', 'appliance', 'fold']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nilmtk_activations['train']['kettle']['UK-DALE_building_1'][0]._metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source = SyntheticAggregateSource(\n",
    "    activations=nilmtk_activations,\n",
    "    target_appliance=TARGET_APPLIANCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    allow_incomplete_target=False,\n",
    "    sample_period=SAMPLE_PERIOD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FOLD = 'train'\n",
    "#FOLD = 'unseen_activations_of_seen_appliances'\n",
    "#FOLD = 'unseen_appliances'\n",
    "seq = source.get_sequence(enable_all_appliances=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all(seq.all_appliances.sum(axis=1) == seq.input[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(2, sharex=True)\n",
    "#seq.all_appliances.plot(ax=axes[0])\n",
    "#axes[1].plot(seq.input)\n",
    "#fig.suptitle(FOLD)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = source.get_batch(num_seq_per_batch=1024).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = DataPipeline(\n",
    "    sources=[source, ras, stride_source],\n",
    "    num_seq_per_batch=64,\n",
    "    input_processing=[DivideBy(sample.before_processing.input.flatten().std()), IndependentlyCenter()],\n",
    "    target_processing=[DivideBy(sample.before_processing.target.flatten().std())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "nilmtk_disag_source = NILMTKDisagSource(\n",
    "    filename=NILMTK_FILENAME,\n",
    "    target_appliance=TARGET_APPLIANCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    buildings=[5],\n",
    "    window_per_building={},\n",
    "    stride=STRIDE,\n",
    "    sample_period=SAMPLE_PERIOD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "disag_pipeline = deepcopy(pipeline)\n",
    "disag_pipeline.source = nilmtk_disag_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "disaggregator = Disaggregator(\n",
    "    pipeline=disag_pipeline,\n",
    "    output_path=PATH  # \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disagregator ideas:\n",
    "\n",
    "* make a copy of pipeline but swap source for a NILMTKDisagSource\n",
    "* NILMTKDisagSource loads all data into memory (?) and iterates over chunks of it (get seq_length from pipeline.source.seq_length)\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pipeline': {'input_processing': [{'divisor': 593.3847, 'name': 'DivideBy'},\n",
       "   {'name': 'IndependentlyCenter'}],\n",
       "  'num_seq_per_batch': 64,\n",
       "  'rng_seed': None,\n",
       "  'source_probabilities': [0.3333333333333333,\n",
       "   0.3333333333333333,\n",
       "   0.3333333333333333],\n",
       "  'sources': {0: {'allow_incomplete_distractors': True,\n",
       "    'allow_incomplete_target': False,\n",
       "    'distractor_inclusion_prob': 0.25,\n",
       "    'include_incomplete_target_in_output': True,\n",
       "    'name': 'SyntheticAggregateSource',\n",
       "    'num_batches_for_validation': 16,\n",
       "    'rng_seed': None,\n",
       "    'sample_period': 6,\n",
       "    'seq_length': 256,\n",
       "    'target_appliance': 'kettle',\n",
       "    'target_inclusion_prob': 0.5,\n",
       "    'uniform_prob_of_selecting_each_building': True},\n",
       "   1: {'allow_incomplete_target': True,\n",
       "    'allow_multiple_target_activations_in_aggregate': False,\n",
       "    'filename': '/data/mine/vadeec/merged/ukdale.h5',\n",
       "    'include_incomplete_target_in_output': True,\n",
       "    'include_multiple_targets_in_output': False,\n",
       "    'name': 'RealAggregateSource',\n",
       "    'num_batches_for_validation': 16,\n",
       "    'rng_seed': None,\n",
       "    'sample_period': 6,\n",
       "    'seq_length': 256,\n",
       "    'target_appliance': 'kettle',\n",
       "    'target_inclusion_prob': 0.5,\n",
       "    'uniform_prob_of_selecting_each_building': True,\n",
       "    'windows': {'train': {1: ('2014-01-01', '2014-02-01')},\n",
       "     'unseen_activations_of_seen_appliances': {1: ('2014-02-02',\n",
       "       '2014-02-08')},\n",
       "     'unseen_appliances': {2: ('2013-06-01', '2013-06-07')}}},\n",
       "   2: {'filename': '/data/mine/vadeec/merged/ukdale.h5',\n",
       "    'name': 'StrideSource',\n",
       "    'num_batches_for_validation': None,\n",
       "    'rng_seed': None,\n",
       "    'sample_period': 6,\n",
       "    'seq_length': 256,\n",
       "    'stride': 256,\n",
       "    'target_appliance': 'kettle',\n",
       "    'windows': {'train': {1: ('2014-01-01', '2014-02-01')},\n",
       "     'unseen_activations_of_seen_appliances': {1: ('2014-02-02',\n",
       "       '2014-02-08')},\n",
       "     'unseen_appliances': {2: ('2013-06-01', '2013-06-07')}}}},\n",
       "  'target_processing': [{'divisor': 427.3772, 'name': 'DivideBy'}]}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pipeline.report()\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer, RecurrentLayer, DenseLayer, ReshapeLayer\n",
    "\n",
    "def get_net_0(input_shape, target_shape=None):\n",
    "    NUM_UNITS = {\n",
    "        'dense_layer_0': 100,\n",
    "        'dense_layer_1':  50,\n",
    "        'dense_layer_2': 100\n",
    "    }\n",
    "\n",
    "    if target_shape is None:\n",
    "        target_shape = input_shape\n",
    "    \n",
    "    # Define layers\n",
    "    input_layer = InputLayer(\n",
    "        shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Dense layers\n",
    "    dense_layer_0 = DenseLayer(\n",
    "        input_layer, \n",
    "        num_units=NUM_UNITS['dense_layer_0']\n",
    "    )\n",
    "    dense_layer_1 = DenseLayer(\n",
    "        dense_layer_0, \n",
    "        num_units=NUM_UNITS['dense_layer_1']\n",
    "    )\n",
    "    dense_layer_2 = DenseLayer(\n",
    "        dense_layer_1, \n",
    "        num_units=NUM_UNITS['dense_layer_2']\n",
    "    )\n",
    "    \n",
    "    # Output\n",
    "    final_dense_layer = DenseLayer(\n",
    "        dense_layer_2,\n",
    "        num_units=target_shape[1] * target_shape[2],\n",
    "        nonlinearity=None\n",
    "    )\n",
    "    output_layer = ReshapeLayer(\n",
    "        final_dense_layer,\n",
    "        shape=target_shape\n",
    "    )\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuralnilm.net import Net\n",
    "\n",
    "batch = pipeline.get_batch()\n",
    "output_layer = get_net_0(\n",
    "    batch.after_processing.input.shape, \n",
    "    batch.after_processing.target.shape\n",
    ")\n",
    "net = Net(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database already has an experiment with _id == 1. Should the old experiment be deleted (both from the database and from disk)? Or quit? [Q/d] d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Deleting documents for old experiment.\n",
      "INFO:neuralnilm.trainer:Directory exists = '~/temp/neural_nilm/output/1'\n",
      "INFO:neuralnilm.trainer:  Deleting directory.\n"
     ]
    }
   ],
   "source": [
    "from neuralnilm.trainer import Trainer\n",
    "from neuralnilm.metrics import Metrics\n",
    "\n",
    "trainer = Trainer(\n",
    "    net=net,\n",
    "    data_pipeline=pipeline,\n",
    "    experiment_id=[\"1\"],\n",
    "    metrics=Metrics(state_boundaries=[4]),\n",
    "    learning_rates={0: 1E-2},\n",
    "    repeat_callbacks=[\n",
    "        (1000, Trainer.validate)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '1',\n",
       " 'data': {'activations': {'nilmtk_activations': {'appliances': ['kettle',\n",
       "     'microwave',\n",
       "     'washing machine'],\n",
       "    'filename': '/data/mine/vadeec/merged/ukdale.h5',\n",
       "    'sample_period': 6,\n",
       "    'windows': {'train': {'1': ('2014-01-01', '2014-02-01')},\n",
       "     'unseen_activations_of_seen_appliances': {'1': ('2014-02-02',\n",
       "       '2014-02-08')},\n",
       "     'unseen_appliances': {'2': ('2013-06-01', '2013-06-07')}}}},\n",
       "  'pipeline': {'input_processing': [{'divisor': 593.3847045898438,\n",
       "     'name': 'DivideBy'},\n",
       "    {'name': 'IndependentlyCenter'}],\n",
       "   'num_seq_per_batch': 64,\n",
       "   'rng_seed': None,\n",
       "   'source_probabilities': [0.3333333333333333,\n",
       "    0.3333333333333333,\n",
       "    0.3333333333333333],\n",
       "   'sources': {'0': {'allow_incomplete_distractors': True,\n",
       "     'allow_incomplete_target': False,\n",
       "     'distractor_inclusion_prob': 0.25,\n",
       "     'include_incomplete_target_in_output': True,\n",
       "     'name': 'SyntheticAggregateSource',\n",
       "     'num_batches_for_validation': 16,\n",
       "     'rng_seed': None,\n",
       "     'sample_period': 6,\n",
       "     'seq_length': 256,\n",
       "     'target_appliance': 'kettle',\n",
       "     'target_inclusion_prob': 0.5,\n",
       "     'uniform_prob_of_selecting_each_building': True},\n",
       "    '1': {'allow_incomplete_target': True,\n",
       "     'allow_multiple_target_activations_in_aggregate': False,\n",
       "     'filename': '/data/mine/vadeec/merged/ukdale.h5',\n",
       "     'include_incomplete_target_in_output': True,\n",
       "     'include_multiple_targets_in_output': False,\n",
       "     'name': 'RealAggregateSource',\n",
       "     'num_batches_for_validation': 16,\n",
       "     'rng_seed': None,\n",
       "     'sample_period': 6,\n",
       "     'seq_length': 256,\n",
       "     'target_appliance': 'kettle',\n",
       "     'target_inclusion_prob': 0.5,\n",
       "     'uniform_prob_of_selecting_each_building': True,\n",
       "     'windows': {'train': {'1': ('2014-01-01', '2014-02-01')},\n",
       "      'unseen_activations_of_seen_appliances': {'1': ('2014-02-02',\n",
       "        '2014-02-08')},\n",
       "      'unseen_appliances': {'2': ('2013-06-01', '2013-06-07')}}},\n",
       "    '2': {'filename': '/data/mine/vadeec/merged/ukdale.h5',\n",
       "     'name': 'StrideSource',\n",
       "     'num_batches_for_validation': None,\n",
       "     'rng_seed': None,\n",
       "     'sample_period': 6,\n",
       "     'seq_length': 256,\n",
       "     'stride': 256,\n",
       "     'target_appliance': 'kettle',\n",
       "     'windows': {'train': {'1': ('2014-01-01', '2014-02-01')},\n",
       "      'unseen_activations_of_seen_appliances': {'1': ('2014-02-02',\n",
       "        '2014-02-08')},\n",
       "      'unseen_appliances': {'2': ('2013-06-01', '2013-06-07')}}}},\n",
       "   'target_processing': [{'divisor': 427.377197265625, 'name': 'DivideBy'}]}},\n",
       " 'trainer': {'learning_rates': {'0': 0.01},\n",
       "  'loss_aggregation_mode': 'mean',\n",
       "  'loss_func_name': 'squared_error',\n",
       "  'metrics': {'clip_to_zero': False,\n",
       "   'name': 'Metrics',\n",
       "   'state_boundaries': [4]},\n",
       "  'min_train_cost': inf,\n",
       "  'output_path': '~/temp/neural_nilm/output/1',\n",
       "  'updates_func_kwards': {},\n",
       "  'updates_func_name': 'nesterov_momentum'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = trainer.report()\n",
    "report['data']['activations'] = LOADER_CONFIG\n",
    "from neuralnilm.utils import sanitise_dict_for_mongo\n",
    "sanitise_dict_for_mongo(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Starting training for 50000 iterations.\n",
      "INFO:neuralnilm.trainer:Iteration 0: Change learning rate to 1.0E-02\n",
      "INFO:neuralnilm.trainer:Compiling train cost function...\n",
      "INFO:neuralnilm.trainer:Done compiling cost function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Update # |  Train cost  | Secs per update | Source ID\n",
      "------------|--------------|-----------------|-----------\n",
      "          0 | \u001b[94m  1.071809\u001b[0m  |    1.430238     |   1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.net:Compiling deterministic output function...\n",
      "INFO:neuralnilm.net:Done compiling deterministic output function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          1 | \u001b[94m  1.055399\u001b[0m  |    0.060915     |   1\n",
      "          2 | \u001b[94m  0.259938\u001b[0m  |    0.050006     |   2\n",
      "          3 |   0.306064  |    0.074706     |   2\n",
      "          4 |   1.230501  |    0.066232     |   0\n",
      "          5 |   1.187141  |    0.191933     |   1\n",
      "          6 |   1.013916  |    0.123669     |   1\n",
      "          7 | \u001b[94m  0.191955\u001b[0m  |    0.074716     |   2\n",
      "          8 |   1.337786  |    0.129204     |   0\n",
      "          9 |   0.192465  |    0.057607     |   2\n",
      "         10 |   1.123701  |    0.202472     |   1\n",
      "         11 |   1.279556  |    0.094854     |   1\n",
      "         12 |   0.765603  |    0.195416     |   1\n",
      "         13 |   1.178034  |    0.050371     |   0\n",
      "         14 |   0.256388  |    0.069898     |   2\n",
      "         15 |   0.765505  |    0.098000     |   0\n",
      "         16 |   1.104850  |    0.148255     |   1\n",
      "         17 |   0.239071  |    0.055918     |   2\n",
      "         18 |   0.805824  |    0.184585     |   1\n",
      "         19 |   1.000085  |    0.132294     |   1\n",
      "         20 |   1.017266  |    0.081101     |   0\n",
      "         21 | \u001b[94m  0.120832\u001b[0m  |    0.076523     |   2\n",
      "         22 |   0.412598  |    0.091475     |   2\n",
      "         23 |   0.197516  |    0.053887     |   2\n",
      "         24 |   1.338168  |    0.203165     |   1\n",
      "         25 |   1.262057  |    0.094111     |   1\n",
      "         26 |   1.059056  |    0.188830     |   1\n",
      "         27 |   0.283498  |    0.054919     |   2\n",
      "         28 |   1.223038  |    0.182459     |   1\n",
      "         29 |   1.050371  |    0.049437     |   0\n",
      "         30 |   0.984555  |    0.067567     |   0\n",
      "         31 |   0.960871  |    0.182309     |   1\n",
      "         32 |   0.975296  |    0.053930     |   0\n",
      "         33 |   1.110003  |    0.077848     |   0\n",
      "         34 |   1.076638  |    0.084570     |   0\n",
      "         35 |   0.239443  |    0.069866     |   2\n",
      "         36 |   1.209394  |    0.185734     |   1\n",
      "         37 |   0.152307  |    0.070271     |   2\n",
      "         38 | \u001b[94m  0.000119\u001b[0m  |    0.064508     |   2\n",
      "         39 |   0.843547  |    0.091768     |   0\n",
      "         40 |   1.004009  |    0.055265     |   0\n",
      "         41 |   0.052432  |    0.089174     |   2\n",
      "         42 |   0.454941  |    0.059720     |   2\n",
      "         43 |   1.071341  |    0.067507     |   0\n",
      "         44 |   0.215216  |    0.087140     |   2\n",
      "         45 |   0.570161  |    0.179606     |   1\n",
      "         46 |   0.231648  |    0.070698     |   2\n",
      "         47 |   0.966801  |    0.203581     |   1\n",
      "         48 |   1.030838  |    0.156637     |   1\n",
      "         49 |   1.042168  |    0.056477     |   0\n",
      "         50 |   1.089623  |    0.095268     |   0\n",
      "         51 |   0.974491  |    0.137805     |   1\n",
      "         52 |   0.160311  |    0.061349     |   2\n",
      "         53 |   1.058727  |    0.099346     |   0\n",
      "         54 |   1.023616  |    0.182793     |   1\n",
      "         55 |   0.705883  |    0.049226     |   0\n",
      "         56 |   0.054625  |    0.060638     |   2\n",
      "         57 |   0.914372  |    0.075481     |   0\n",
      "         58 |   0.099549  |    0.079522     |   2\n",
      "         59 |   0.149254  |    0.076028     |   2\n",
      "         60 |   0.794315  |    0.067074     |   0\n",
      "         61 |   1.178879  |    0.056441     |   0\n",
      "         62 |   1.279537  |    0.091624     |   0\n",
      "         63 |   0.820828  |    0.181829     |   1\n",
      "         64 |   1.015344  |    0.060381     |   0\n",
      "         65 |   1.124911  |    0.190031     |   1\n",
      "         66 |   1.160222  |    0.136139     |   1\n",
      "         67 |   1.121208  |    0.194072     |   1\n",
      "         68 |   0.960082  |    0.049046     |   0\n",
      "         69 |   1.117624  |    0.057826     |   0\n",
      "         70 |   0.843315  |    0.149591     |   1\n",
      "         71 |   0.000189  |    0.054754     |   2\n",
      "         72 |   0.000172  |    0.087116     |   2\n",
      "         73 |   1.148354  |    0.070853     |   0\n",
      "         74 |   0.888714  |    0.071318     |   0\n",
      "         75 |   0.000169  |    0.058974     |   2\n",
      "         76 |   0.743674  |    0.090162     |   0\n",
      "         77 |   1.051667  |    0.177697     |   1\n",
      "         78 |   0.000217  |    0.049094     |   2\n",
      "         79 |   0.985771  |    0.064768     |   0\n",
      "         80 |   1.028778  |    0.083333     |   0\n",
      "         81 |   0.986971  |    0.193755     |   1\n",
      "         82 |   0.000201  |    0.048876     |   2\n",
      "         83 |   0.000395  |    0.081402     |   2\n",
      "         84 |   0.301633  |    0.106194     |   2\n",
      "         85 |   0.953448  |    0.075049     |   0\n",
      "         86 |   0.773935  |    0.085613     |   0\n",
      "         87 |   0.871341  |    0.056775     |   0\n",
      "         88 |   0.209658  |    0.091168     |   2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "INFO:neuralnilm.trainer:Iteration 89: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "         89 |   0.864616  |    0.137235     |   1\n",
      "         90 |   1.019595  |    0.067932     |   0\n",
      "         91 |   0.149327  |    0.101556     |   2\n",
      "         92 |   0.822846  |    0.167371     |   1\n",
      "         93 |   0.978129  |    0.139559     |   1\n",
      "         94 |   0.914496  |    0.144444     |   1\n",
      "         95 |   0.693680  |    0.135608     |   1\n",
      "         96 |   0.758511  |    0.194231     |   1\n",
      "         97 |   0.170972  |    0.050876     |   2\n",
      "         98 |   0.955701  |    0.052619     |   0\n",
      "         99 |   0.974353  |    0.098594     |   0\n",
      "        100 |   0.935906  |    0.177531     |   1\n",
      "        101 |   0.894542  |    0.155370     |   1\n",
      "        102 |   0.238751  |    0.052184     |   2\n",
      "        103 |   0.881318  |    0.129389     |   1\n",
      "        104 |   0.218298  |    0.076211     |   2\n",
      "        105 |   0.105857  |    0.069128     |   2\n",
      "        106 |   0.238999  |    0.062365     |   2\n",
      "        107 |   0.190364  |    0.121120     |   2\n",
      "        108 |   0.881521  |    0.075911     |   0\n",
      "        109 |   0.835338  |    0.078739     |   0\n",
      "        110 |   0.990469  |    0.181961     |   1\n",
      "        111 |   0.875609  |    0.050005     |   0\n",
      "        112 |   0.794814  |    0.158033     |   1\n",
      "        113 |   0.259657  |    0.053034     |   2\n",
      "        114 |   0.215105  |    0.090327     |   2\n",
      "        115 |   0.114657  |    0.059765     |   2\n",
      "        116 |   1.026702  |    0.179928     |   1\n",
      "        117 |   1.093518  |    0.050763     |   0\n",
      "        118 |   0.000205  |    0.069240     |   2\n",
      "        119 |   0.046747  |    0.087386     |   2\n",
      "        120 |   0.394699  |    0.067402     |   2\n",
      "        121 |   0.193550  |    0.074478     |   2\n",
      "        122 |   0.198963  |    0.061139     |   2\n",
      "        123 |   0.909900  |    0.173737     |   1\n",
      "        124 |   0.584891  |    0.048572     |   0\n",
      "        125 |   1.076362  |    0.054564     |   0\n",
      "        126 |   0.142408  |    0.071121     |   2\n",
      "        127 |   0.045115  |    0.091225     |   2\n",
      "        128 |   0.802652  |    0.212312     |   1\n",
      "        129 |   0.814754  |    0.132693     |   1\n",
      "        130 |   0.948918  |    0.066557     |   0\n",
      "        131 |   1.205141  |    0.191197     |   1\n",
      "        132 |   0.714708  |    0.056625     |   0\n",
      "        133 |   0.811956  |    0.202004     |   1\n",
      "        134 |   1.033191  |    0.053252     |   0\n",
      "        135 |   0.939952  |    0.139761     |   1\n",
      "        136 |   0.887652  |    0.135127     |   1\n",
      "        137 |   0.091732  |    0.082263     |   2\n",
      "        138 |   1.051897  |    0.073425     |   0\n",
      "        139 |   0.990538  |    0.097729     |   0\n",
      "        140 |   0.876716  |    0.177020     |   1\n",
      "        141 |   1.029320  |    0.051995     |   0\n",
      "        142 |   0.135221  |    0.057402     |   2\n",
      "        143 |   0.000406  |    0.073389     |   2\n",
      "        144 |   0.891777  |    0.180497     |   1\n",
      "        145 |   0.000440  |    0.049939     |   2\n",
      "        146 |   1.098422  |    0.058150     |   0\n",
      "        147 |   0.823475  |    0.090673     |   0\n",
      "        148 |   0.901651  |    0.074340     |   0\n",
      "        149 |   1.009933  |    0.175333     |   1\n",
      "        150 |   1.032778  |    0.063960     |   0\n",
      "        151 |   0.719199  |    0.183698     |   1\n",
      "        152 |   0.881274  |    0.138322     |   1\n",
      "        153 |   0.000329  |    0.075050     |   2\n",
      "        154 |   0.882763  |    0.195436     |   1\n",
      "        155 |   0.758689  |    0.178259     |   1\n",
      "        156 |   0.616186  |    0.137301     |   1\n",
      "        157 |   0.000443  |    0.067088     |   2\n",
      "        158 |   0.743370  |    0.078295     |   0\n",
      "        159 |   0.805710  |    0.195983     |   1\n",
      "        160 |   0.000387  |    0.051222     |   2\n",
      "        161 |   0.000492  |    0.062404     |   2\n",
      "        162 |   0.828515  |    0.209182     |   1\n",
      "        163 |   0.736321  |    0.136902     |   1\n",
      "        164 |   0.780386  |    0.148097     |   1\n",
      "        165 |   0.691519  |    0.073645     |   0\n",
      "        166 |   0.856268  |    0.091319     |   0\n",
      "        167 |   0.702562  |    0.218738     |   1\n",
      "        168 |   0.253813  |    0.050795     |   2\n",
      "        169 |   0.822123  |    0.057136     |   0\n",
      "        170 |   0.687606  |    0.061100     |   0\n",
      "        171 |   0.986093  |    0.161680     |   1\n",
      "        172 |   0.826707  |    0.130144     |   1\n",
      "        173 |   0.203417  |    0.078961     |   2\n",
      "        174 |   0.847916  |    0.103885     |   0\n",
      "        175 |   0.537557  |    0.189307     |   1\n",
      "        176 |   0.858913  |    0.054000     |   0\n",
      "        177 |   0.925985  |    0.066569     |   0\n",
      "        178 |   0.828435  |    0.208855     |   1\n",
      "        179 |   0.803111  |    0.064350     |   0\n",
      "        180 |   1.027410  |    0.097494     |   1\n",
      "        181 |   0.909175  |    0.074285     |   0\n",
      "        182 |   0.944645  |    0.183073     |   1\n",
      "        183 |   0.903320  |    0.133677     |   1\n",
      "        184 |   0.929821  |    0.059401     |   0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 185: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        185 |   0.735072  |    0.179050     |   1\n",
      "        186 |   0.748839  |    0.065868     |   0\n",
      "        187 |   0.552265  |    0.141120     |   1\n",
      "        188 |   0.718145  |    0.171685     |   1\n",
      "        189 |   0.997319  |    0.094841     |   1\n",
      "        190 |   0.931121  |    0.092344     |   0\n",
      "        191 |   0.922619  |    0.132623     |   1\n",
      "        192 |   0.735273  |    0.184901     |   1\n",
      "        193 |   0.711161  |    0.140733     |   1\n",
      "        194 |   0.155390  |    0.084189     |   2\n",
      "        195 |   0.157225  |    0.070737     |   2\n",
      "        196 |   0.215368  |    0.088040     |   2\n",
      "        197 |   0.212586  |    0.061051     |   2\n",
      "        198 |   0.787990  |    0.066048     |   0\n",
      "        199 |   0.722543  |    0.092561     |   0\n",
      "        200 |   0.703418  |    0.064076     |   0\n",
      "        201 |   0.763061  |    0.095793     |   0\n",
      "        202 |   0.092377  |    0.060063     |   2\n",
      "        203 |   0.775752  |    0.138789     |   1\n",
      "        204 |   0.837114  |    0.188740     |   1\n",
      "        205 |   0.728917  |    0.052998     |   0\n",
      "        206 |   0.692677  |    0.129086     |   1\n",
      "        207 |   0.848321  |    0.177201     |   1\n",
      "        208 |   0.892465  |    0.067496     |   0\n",
      "        209 |   0.916126  |    0.092042     |   0\n",
      "        210 |   0.853819  |    0.091277     |   0\n",
      "        211 |   0.809045  |    0.188174     |   1\n",
      "        212 |   0.201071  |    0.064729     |   2\n",
      "        213 |   0.713007  |    0.145483     |   1\n",
      "        214 |   0.754137  |    0.186287     |   1\n",
      "        215 |   0.997945  |    0.052948     |   0\n",
      "        216 |   0.178439  |    0.068099     |   2\n",
      "        217 |   0.629054  |    0.078144     |   0\n",
      "        218 |   0.698997  |    0.186682     |   1\n",
      "        219 |   0.635985  |    0.156427     |   1\n",
      "        220 |   0.890562  |    0.128822     |   1\n",
      "        221 |   0.212732  |    0.078742     |   2\n",
      "        222 |   0.193462  |    0.053214     |   2\n",
      "        223 |   0.568555  |    0.063508     |   1\n",
      "        224 |   0.666715  |    0.192314     |   1\n",
      "        225 |   0.924535  |    0.133867     |   1\n",
      "        226 |   0.727260  |    0.068608     |   0\n",
      "        227 |   0.055612  |    0.109027     |   2\n",
      "        228 |   0.687095  |    0.184140     |   1\n",
      "        229 |   0.879117  |    0.131903     |   1\n",
      "        230 |   0.871819  |    0.096266     |   0\n",
      "        231 |   0.933124  |    0.178885     |   1\n",
      "        232 |   0.817383  |    0.133389     |   1\n",
      "        233 |   0.000572  |    0.077247     |   2\n",
      "        234 |   0.699580  |    0.200643     |   1\n",
      "        235 |   0.778439  |    0.052013     |   0\n",
      "        236 |   0.035397  |    0.078897     |   2\n",
      "        237 |   0.874357  |    0.195413     |   1\n",
      "        238 |   0.663997  |    0.055985     |   0\n",
      "        239 |   0.530231  |    0.136538     |   1\n",
      "        240 |   0.334114  |    0.074532     |   2\n",
      "        241 |   0.753883  |    0.185195     |   1\n",
      "        242 |   0.820211  |    0.126171     |   1\n",
      "        243 |   0.162732  |    0.077170     |   2\n",
      "        244 |   0.623833  |    0.218252     |   1\n",
      "        245 |   0.813155  |    0.105389     |   1\n",
      "        246 |   0.786946  |    0.069979     |   0\n",
      "        247 |   0.173924  |    0.075191     |   2\n",
      "        248 |   0.694091  |    0.188898     |   1\n",
      "        249 |   0.113966  |    0.063952     |   2\n",
      "        250 |   0.669123  |    0.137165     |   1\n",
      "        251 |   0.836759  |    0.063535     |   0\n",
      "        252 |   0.041182  |    0.074177     |   2\n",
      "        253 |   0.777460  |    0.099083     |   0\n",
      "        254 |   0.100651  |    0.056758     |   2\n",
      "        255 |   0.813186  |    0.093387     |   0\n",
      "        256 |   0.578895  |    0.133563     |   1\n",
      "        257 |   0.126226  |    0.074482     |   2\n",
      "        258 |   0.631134  |    0.206820     |   1\n",
      "        259 |   0.738951  |    0.136262     |   1\n",
      "        260 |   0.000595  |    0.058399     |   2\n",
      "        261 |   0.856903  |    0.172612     |   1\n",
      "        262 |   0.808738  |    0.150536     |   1\n",
      "        263 |   0.909274  |    0.172952     |   1\n",
      "        264 |   0.811958  |    0.179737     |   1\n",
      "        265 |   0.000847  |    0.057859     |   2\n",
      "        266 |   0.892741  |    0.131458     |   1\n",
      "        267 |   0.762598  |    0.079979     |   0\n",
      "        268 |   0.000559  |    0.099199     |   2\n",
      "        269 |   0.000768  |    0.088508     |   2\n",
      "        270 |   0.709600  |    0.072931     |   0\n",
      "        271 |   0.724890  |    0.203528     |   1\n",
      "        272 |   0.000608  |    0.081188     |   2\n",
      "        273 |   0.000665  |    0.089205     |   2\n",
      "        274 |   0.785095  |    0.200375     |   1\n",
      "        275 |   0.627126  |    0.048061     |   0\n",
      "        276 |   0.195651  |    0.063832     |   2\n",
      "        277 |   0.200190  |    0.086844     |   2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 278: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        278 |   0.143460  |    0.059195     |   2\n",
      "        279 |   0.136662  |    0.094637     |   2\n",
      "        280 |   0.614181  |    0.063700     |   0\n",
      "        281 |   0.660192  |    0.087192     |   0\n",
      "        282 |   0.189529  |    0.068573     |   2\n",
      "        283 |   0.811166  |    0.107672     |   0\n",
      "        284 |   0.673671  |    0.072211     |   0\n",
      "        285 |   0.198265  |    0.071297     |   2\n",
      "        286 |   1.011926  |    0.139550     |   1\n",
      "        287 |   0.804921  |    0.205561     |   1\n",
      "        288 |   0.843251  |    0.132055     |   1\n",
      "        289 |   0.669876  |    0.069734     |   0\n",
      "        290 |   0.083063  |    0.088783     |   2\n",
      "        291 |   0.175878  |    0.089983     |   2\n",
      "        292 |   0.158585  |    0.099283     |   2\n",
      "        293 |   0.725466  |    0.109747     |   0\n",
      "        294 |   0.178175  |    0.074038     |   2\n",
      "        295 |   0.179014  |    0.096872     |   2\n",
      "        296 |   0.047252  |    0.060566     |   2\n",
      "        297 |   0.000624  |    0.092079     |   2\n",
      "        298 |   0.649414  |    0.180102     |   1\n",
      "        299 |   0.626452  |    0.055292     |   0\n",
      "        300 |   0.028966  |    0.065142     |   2\n",
      "        301 |   0.614021  |    0.197712     |   1\n",
      "        302 |   0.297811  |    0.054034     |   2\n",
      "        303 |   0.624586  |    0.124944     |   1\n",
      "        304 |   0.530194  |    0.075401     |   0\n",
      "        305 |   0.724656  |    0.182400     |   1\n",
      "        306 |   0.141359  |    0.049993     |   2\n",
      "        307 |   0.453689  |    0.049265     |   0\n",
      "        308 |   0.157144  |    0.076886     |   2\n",
      "        309 |   0.572211  |    0.078355     |   0\n",
      "        310 |   0.109551  |    0.070917     |   2\n",
      "        311 |   0.037326  |    0.085270     |   2\n",
      "        312 |   0.749900  |    0.193625     |   1\n",
      "        313 |   0.779566  |    0.084972     |   1\n",
      "        314 |   0.084381  |    0.085444     |   2\n",
      "        315 |   0.124181  |    0.054987     |   2\n",
      "        316 |   0.580069  |    0.101639     |   0\n",
      "        317 |   0.719145  |    0.139304     |   1\n",
      "        318 |   0.654197  |    0.182353     |   1\n",
      "        319 |   0.000527  |    0.049233     |   2\n",
      "        320 |   0.000791  |    0.089934     |   2\n",
      "        321 |   0.709153  |    0.061511     |   0\n",
      "        322 |   0.674963  |    0.178438     |   1\n",
      "        323 |   0.598642  |    0.054825     |   0\n",
      "        324 |   0.463209  |    0.093771     |   0\n",
      "        325 |   0.000616  |    0.059134     |   2\n",
      "        326 |   0.690931  |    0.090965     |   0\n",
      "        327 |   0.522103  |    0.053027     |   0\n",
      "        328 |   0.000801  |    0.071438     |   2\n",
      "        329 |   0.714979  |    0.080026     |   0\n",
      "        330 |   0.699207  |    0.155693     |   1\n",
      "        331 |   0.662860  |    0.073836     |   0\n",
      "        332 |   0.627521  |    0.189681     |   1\n",
      "        333 |   0.655818  |    0.050354     |   0\n",
      "        334 |   0.694424  |    0.053470     |   0\n",
      "        335 |   0.570101  |    0.090370     |   0\n",
      "        336 |   0.535390  |    0.053294     |   0\n",
      "        337 |   0.000727  |    0.093854     |   2\n",
      "        338 |   0.000711  |    0.056610     |   2\n",
      "        339 |   0.530081  |    0.086793     |   0\n",
      "        340 |   0.181882  |    0.066681     |   2\n",
      "        341 |   0.831752  |    0.125005     |   1\n",
      "        342 |   0.711827  |    0.084740     |   0\n",
      "        343 |   0.201142  |    0.088149     |   2\n",
      "        344 |   0.632484  |    0.062323     |   0\n",
      "        345 |   0.606902  |    0.114954     |   0\n",
      "        346 |   0.602882  |    0.174335     |   1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 347: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        347 |   0.149021  |    0.048339     |   2\n",
      "        348 |   0.130940  |    0.056453     |   2\n",
      "        349 |   0.701593  |    0.181864     |   1\n",
      "        350 |   0.172677  |    0.051092     |   2\n",
      "        351 |   0.649998  |    0.071676     |   0\n",
      "        352 |   0.623449  |    0.151178     |   1\n",
      "        353 |   0.611942  |    0.127778     |   1\n",
      "        354 |   0.523851  |    0.201190     |   1\n",
      "        355 |   0.539638  |    0.051355     |   0\n",
      "        356 |   0.389245  |    0.061869     |   0\n",
      "        357 |   0.572179  |    0.181097     |   1\n",
      "        358 |   0.676020  |    0.139908     |   1\n",
      "        359 |   0.892908  |    0.133912     |   1\n",
      "        360 |   0.658616  |    0.191384     |   1\n",
      "        361 |   0.711302  |    0.052330     |   0\n",
      "        362 |   0.195122  |    0.058448     |   2\n",
      "        363 |   0.586846  |    0.085991     |   0\n",
      "        364 |   0.078420  |    0.060385     |   2\n",
      "        365 |   0.591659  |    0.078015     |   0\n",
      "        366 |   0.837298  |    0.172617     |   1\n",
      "        367 |   0.645102  |    0.126878     |   1\n",
      "        368 |   0.161028  |    0.071451     |   2\n",
      "        369 |   0.658321  |    0.071030     |   0\n",
      "        370 |   0.494211  |    0.193901     |   1\n",
      "        371 |   0.573437  |    0.119680     |   1\n",
      "        372 |   0.535305  |    0.071994     |   0\n",
      "        373 |   0.581611  |    0.084727     |   0\n",
      "        374 |   0.630043  |    0.074424     |   0\n",
      "        375 |   0.577672  |    0.140657     |   1\n",
      "        376 |   0.480356  |    0.091536     |   0\n",
      "        377 |   0.564816  |    0.143634     |   1\n",
      "        378 |   0.152423  |    0.070518     |   2\n",
      "        379 |   0.149149  |    0.091003     |   2\n",
      "        380 |   0.542314  |    0.066957     |   0\n",
      "        381 |   0.571724  |    0.083844     |   0\n",
      "        382 |   0.454985  |    0.159416     |   1\n",
      "        383 |   0.539966  |    0.060482     |   0\n",
      "        384 |   0.590858  |    0.077875     |   0\n",
      "        385 |   0.153414  |    0.090653     |   2\n",
      "        386 |   0.554718  |    0.084527     |   0\n",
      "        387 |   0.049403  |    0.062789     |   2\n",
      "        388 |   0.000649  |    0.099188     |   2\n",
      "        389 |   0.021470  |    0.063422     |   2\n",
      "        390 |   0.280718  |    0.071769     |   2\n",
      "        391 |   0.125507  |    0.090787     |   2\n",
      "        392 |   0.657115  |    0.206565     |   1\n",
      "        393 |   0.151136  |    0.054110     |   2\n",
      "        394 |   0.559695  |    0.117195     |   1\n",
      "        395 |   0.085404  |    0.074219     |   2\n",
      "        396 |   0.507643  |    0.073389     |   0\n",
      "        397 |   0.634534  |    0.088770     |   0\n",
      "        398 |   0.574127  |    0.196151     |   1\n",
      "        399 |   0.726344  |    0.101494     |   1\n",
      "        400 |   0.594701  |    0.163000     |   1\n",
      "        401 |   0.537517  |    0.060352     |   0\n",
      "        402 |   0.034347  |    0.143157     |   2\n",
      "        403 |   0.087366  |    0.075306     |   2\n",
      "        404 |   0.573322  |    0.148428     |   1\n",
      "        405 |   0.650802  |    0.074294     |   0\n",
      "        406 |   0.570713  |    0.183627     |   1\n",
      "        407 |   0.124139  |    0.051824     |   2\n",
      "        408 |   0.584944  |    0.063276     |   0\n",
      "        409 |   0.589919  |    0.207912     |   1\n",
      "        410 |   0.551625  |    0.053130     |   0\n",
      "        411 |   0.000520  |    0.056350     |   2\n",
      "        412 |   0.541753  |    0.186130     |   1\n",
      "        413 |   0.000734  |    0.052457     |   2\n",
      "        414 |   0.000572  |    0.072404     |   2\n",
      "        415 |   0.544904  |    0.192762     |   1\n",
      "        416 |   0.000717  |    0.053402     |   2\n",
      "        417 |   0.572707  |    0.136189     |   1\n",
      "        418 |   0.000691  |    0.082448     |   2\n",
      "        419 |   0.597172  |    0.074234     |   0\n",
      "        420 |   0.551006  |    0.064756     |   0\n",
      "        421 |   0.679863  |    0.143500     |   1\n",
      "        422 |   0.000617  |    0.087178     |   2\n",
      "        423 |   0.535902  |    0.064863     |   0\n",
      "        424 |   0.162971  |    0.092625     |   2\n",
      "        425 |   0.190905  |    0.075545     |   2\n",
      "        426 |   0.445271  |    0.089417     |   0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 427: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        427 |   0.141281  |    0.070929     |   2\n",
      "        428 |   0.114956  |    0.066457     |   2\n",
      "        429 |   0.567481  |    0.159541     |   1\n",
      "        430 |   0.630031  |    0.135378     |   1\n",
      "        431 |   0.601650  |    0.059857     |   0\n",
      "        432 |   0.543429  |    0.211668     |   1\n",
      "        433 |   0.157098  |    0.056433     |   2\n",
      "        434 |   0.181390  |    0.068804     |   2\n",
      "        435 |   0.535192  |    0.076267     |   0\n",
      "        436 |   0.065674  |    0.073929     |   2\n",
      "        437 |   0.545960  |    0.161553     |   1\n",
      "        438 |   0.148415  |    0.071563     |   2\n",
      "        439 |   0.499666  |    0.187392     |   1\n",
      "        440 |   0.609575  |    0.131169     |   1\n",
      "        441 |   0.720173  |    0.134705     |   1\n",
      "        442 |   0.442578  |    0.178092     |   1\n",
      "        443 |   0.136002  |    0.061887     |   2\n",
      "        444 |   0.530629  |    0.185138     |   1\n",
      "        445 |   0.569157  |    0.150516     |   1\n",
      "        446 |   0.449907  |    0.091631     |   0\n",
      "        447 |   0.497123  |    0.083103     |   0\n",
      "        448 |   0.528299  |    0.060686     |   0\n",
      "        449 |   0.139926  |    0.091692     |   2\n",
      "        450 |   0.768027  |    0.133018     |   1\n",
      "        451 |   0.577437  |    0.096386     |   0\n",
      "        452 |   0.490629  |    0.200871     |   1\n",
      "        453 |   0.140509  |    0.059055     |   2\n",
      "        454 |   0.500163  |    0.190121     |   1\n",
      "        455 |   0.450877  |    0.069724     |   0\n",
      "        456 |   0.570073  |    0.138278     |   1\n",
      "        457 |   0.036738  |    0.074036     |   2\n",
      "        458 |   0.675927  |    0.187969     |   1\n",
      "        459 |   0.000621  |    0.062048     |   2\n",
      "        460 |   0.640351  |    0.072192     |   0\n",
      "        461 |   0.018317  |    0.067744     |   2\n",
      "        462 |   0.263417  |    0.082403     |   2\n",
      "        463 |   0.422009  |    0.060582     |   0\n",
      "        464 |   0.117679  |    0.087202     |   2\n",
      "        465 |   0.501759  |    0.073400     |   0\n",
      "        466 |   0.575229  |    0.178766     |   1\n",
      "        467 |   0.133136  |    0.056975     |   2\n",
      "        468 |   0.504772  |    0.085636     |   0\n",
      "        469 |   0.503591  |    0.082477     |   0\n",
      "        470 |   0.614779  |    0.069914     |   0\n",
      "        471 |   0.386547  |    0.074666     |   0\n",
      "        472 |   0.477211  |    0.087183     |   0\n",
      "        473 |   0.320105  |    0.187071     |   1\n",
      "        474 |   0.075043  |    0.061740     |   2\n",
      "        475 |   0.502542  |    0.185186     |   1\n",
      "        476 |   0.446837  |    0.141757     |   1\n",
      "        477 |   0.037220  |    0.084007     |   2\n",
      "        478 |   0.080756  |    0.069944     |   2\n",
      "        479 |   0.643366  |    0.074156     |   0\n",
      "        480 |   0.612843  |    0.172947     |   1\n",
      "        481 |   0.109672  |    0.050805     |   2\n",
      "        482 |   0.537105  |    0.060278     |   0\n",
      "        483 |   0.572179  |    0.203930     |   1\n",
      "        484 |   0.521670  |    0.066143     |   0\n",
      "        485 |   0.550114  |    0.121721     |   1\n",
      "        486 |   0.391084  |    0.201307     |   1\n",
      "        487 |   0.445877  |    0.049909     |   0\n",
      "        488 |   0.000494  |    0.055179     |   2\n",
      "        489 |   0.497704  |    0.204080     |   1\n",
      "        490 |   0.411431  |    0.137583     |   1\n",
      "        491 |   0.558614  |    0.185582     |   1\n",
      "        492 |   0.449820  |    0.184167     |   1\n",
      "        493 |   0.511953  |    0.050510     |   0\n",
      "        494 |   0.544238  |    0.125010     |   1\n",
      "        495 |   0.514658  |    0.193904     |   1\n",
      "        496 |   0.000651  |    0.083309     |   2\n",
      "        497 |   0.496111  |    0.131362     |   0\n",
      "        498 |   0.000512  |    0.062844     |   2\n",
      "        499 |   0.595763  |    0.182824     |   1\n",
      "        500 |   0.399531  |    0.069732     |   0\n",
      "        501 |   0.469641  |    0.136503     |   1\n",
      "        502 |   0.399804  |    0.063098     |   0\n",
      "        503 |   0.564081  |    0.124986     |   0\n",
      "        504 |   0.559566  |    0.187880     |   1\n",
      "        505 |   0.466452  |    0.090533     |   1\n",
      "        506 |   0.000617  |    0.089413     |   2\n",
      "        507 |   0.000762  |    0.073669     |   2\n",
      "        508 |   0.461289  |    0.084572     |   0\n",
      "        509 |   0.000642  |    0.058906     |   2\n",
      "        510 |   0.501688  |    0.088981     |   0\n",
      "        511 |   0.368123  |    0.066587     |   0\n",
      "        512 |   0.504206  |    0.073199     |   0\n",
      "        513 |   0.546371  |    0.076381     |   0\n",
      "        514 |   0.610146  |    0.154629     |   1\n",
      "        515 |   0.539734  |    0.089666     |   0\n",
      "        516 |   0.542070  |    0.091670     |   0\n",
      "        517 |   0.620838  |    0.124866     |   1\n",
      "        518 |   0.355324  |    0.060305     |   0\n",
      "        519 |   0.334139  |    0.077871     |   0\n",
      "        520 |   0.553838  |    0.171069     |   1\n",
      "        521 |   0.525490  |    0.193567     |   1\n",
      "        522 |   0.458136  |    0.060457     |   0\n",
      "        523 |   0.451773  |    0.184951     |   1\n",
      "        524 |   0.534687  |    0.054962     |   0\n",
      "        525 |   0.460378  |    0.092357     |   0\n",
      "        526 |   0.143855  |    0.089096     |   2\n",
      "        527 |   0.528008  |    0.084411     |   0\n",
      "        528 |   0.572171  |    0.077061     |   0\n",
      "        529 |   0.555892  |    0.198313     |   1\n",
      "        530 |   0.179811  |    0.056099     |   2\n",
      "        531 |   0.622488  |    0.209134     |   1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 532: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        532 |   0.461646  |    0.052963     |   0\n",
      "        533 |   0.555840  |    0.135019     |   1\n",
      "        534 |   0.132102  |    0.064347     |   2\n",
      "        535 |   0.108283  |    0.075049     |   2\n",
      "        536 |   0.520334  |    0.067812     |   0\n",
      "        537 |   0.548537  |    0.183696     |   1\n",
      "        538 |   0.155868  |    0.056764     |   2\n",
      "        539 |   0.367188  |    0.082414     |   0\n",
      "        540 |   0.541469  |    0.173359     |   1\n",
      "        541 |   0.171591  |    0.055044     |   2\n",
      "        542 |   0.055466  |    0.060064     |   2\n",
      "        543 |   0.482297  |    0.106326     |   0\n",
      "        544 |   0.329700  |    0.201354     |   1\n",
      "        545 |   0.613430  |    0.099618     |   1\n",
      "        546 |   0.489423  |    0.088802     |   0\n",
      "        547 |   0.480580  |    0.198976     |   1\n",
      "        548 |   0.508262  |    0.129591     |   1\n",
      "        549 |   0.131959  |    0.054384     |   2\n",
      "        550 |   0.339365  |    0.077925     |   0\n",
      "        551 |   0.548170  |    0.090518     |   0\n",
      "        552 |   0.381204  |    0.075977     |   0\n",
      "        553 |   0.442588  |    0.183639     |   1\n",
      "        554 |   0.471269  |    0.195671     |   1\n",
      "        555 |   0.123518  |    0.057454     |   2\n",
      "        556 |   0.400038  |    0.137854     |   1\n",
      "        557 |   0.131207  |    0.055339     |   2\n",
      "        558 |   0.474873  |    0.076956     |   0\n",
      "        559 |   0.135326  |    0.064553     |   2\n",
      "        560 |   0.412334  |    0.094138     |   0\n",
      "        561 |   0.387947  |    0.057975     |   0\n",
      "        562 |   0.043846  |    0.056843     |   2\n",
      "        563 |   0.513757  |    0.101684     |   0\n",
      "        564 |   0.480188  |    0.147128     |   1\n",
      "        565 |   0.636941  |    0.176742     |   1\n",
      "        566 |   0.471848  |    0.061027     |   0\n",
      "        567 |   0.000742  |    0.075145     |   2\n",
      "        568 |   0.548297  |    0.187557     |   1\n",
      "        569 |   0.014900  |    0.065445     |   2\n",
      "        570 |   0.232499  |    0.088454     |   2\n",
      "        571 |   0.441427  |    0.182185     |   1\n",
      "        572 |   0.096967  |    0.063818     |   2\n",
      "        573 |   0.453611  |    0.183701     |   1\n",
      "        574 |   0.488096  |    0.068587     |   0\n",
      "        575 |   0.463872  |    0.211807     |   1\n",
      "        576 |   0.454790  |    0.127731     |   1\n",
      "        577 |   0.513832  |    0.184590     |   1\n",
      "        578 |   0.469036  |    0.050268     |   0\n",
      "        579 |   0.396397  |    0.050462     |   0\n",
      "        580 |   0.120240  |    0.085278     |   2\n",
      "        581 |   0.461533  |    0.062038     |   0"
     ]
    }
   ],
   "source": [
    "trainer.fit(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.train_iterations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
