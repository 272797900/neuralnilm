{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from neuralnilm.data.loadactivations import load_nilmtk_activations\n",
    "from neuralnilm.data.syntheticaggregatesource import SyntheticAggregateSource\n",
    "from neuralnilm.data.datapipeline import DataPipeline\n",
    "from neuralnilm.data.processing import DivideBy, IndependentlyCenter\n",
    "from neuralnilm.data.datathread import DataThread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NILMTK_FILENAME = '/data/mine/vadeec/merged/ukdale.h5'\n",
    "TARGET_APPLIANCE = 'kettle'\n",
    "SEQ_LENGTH = 256\n",
    "SAMPLE_PERIOD = 6\n",
    "STRIDE = SEQ_LENGTH\n",
    "WINDOWS = {\n",
    "    'train': {\n",
    "        1: (\"2014-01-01\", \"2014-02-01\")\n",
    "    },\n",
    "    'unseen_activations_of_seen_appliances': {\n",
    "        1: (\"2014-02-02\", \"2014-02-08\")                \n",
    "    },\n",
    "    'unseen_appliances': {\n",
    "        2: (\"2013-06-01\", \"2013-06-07\")\n",
    "    }\n",
    "}\n",
    "\n",
    "LOADER_CONFIG = {\n",
    "    'nilmtk_activations': dict(\n",
    "        appliances=['kettle', 'microwave', 'washing machine'],\n",
    "        filename=NILMTK_FILENAME,\n",
    "        sample_period=SAMPLE_PERIOD,\n",
    "        windows=WINDOWS\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuralnilm.data.stridesource import StrideSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stride_source = StrideSource(\n",
    "    target_appliance=TARGET_APPLIANCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    filename=NILMTK_FILENAME,\n",
    "    windows=WINDOWS,\n",
    "    sample_period=SAMPLE_PERIOD,\n",
    "    stride=STRIDE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nilmtk_activations = load_nilmtk_activations(**LOADER_CONFIG['nilmtk_activations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuralnilm.data.realaggregatesource import RealAggregateSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ras = RealAggregateSource(\n",
    "    activations=nilmtk_activations,\n",
    "    target_appliance=TARGET_APPLIANCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    filename=NILMTK_FILENAME,\n",
    "    windows=WINDOWS,\n",
    "    sample_period=SAMPLE_PERIOD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "%prun ras.get_sequence().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source = SyntheticAggregateSource(\n",
    "    activations=nilmtk_activations,\n",
    "    target_appliance=TARGET_APPLIANCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    allow_incomplete_target=False,\n",
    "    sample_period=SAMPLE_PERIOD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = source.get_batch(num_seq_per_batch=1024).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = DataPipeline(\n",
    "    sources=[source, ras, stride_source],\n",
    "    num_seq_per_batch=64,\n",
    "    input_processing=[DivideBy(sample.before_processing.input.flatten().std()), IndependentlyCenter()],\n",
    "    target_processing=[DivideBy(sample.before_processing.target.flatten().std())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "nilmtk_disag_source = NILMTKDisagSource(\n",
    "    filename=NILMTK_FILENAME,\n",
    "    target_appliance=TARGET_APPLIANCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    buildings=[5],\n",
    "    window_per_building={},\n",
    "    stride=STRIDE,\n",
    "    sample_period=SAMPLE_PERIOD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "disag_pipeline = deepcopy(pipeline)\n",
    "disag_pipeline.source = nilmtk_disag_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "disaggregator = Disaggregator(\n",
    "    pipeline=disag_pipeline,\n",
    "    output_path=PATH  # \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disagregator ideas:\n",
    "\n",
    "* make a copy of pipeline but swap source for a NILMTKDisagSource\n",
    "* NILMTKDisagSource loads all data into memory (?) and iterates over chunks of it (get seq_length from pipeline.source.seq_length)\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer, RecurrentLayer, DenseLayer, ReshapeLayer\n",
    "\n",
    "def get_net_0(input_shape, target_shape=None):\n",
    "    NUM_UNITS = {\n",
    "        'dense_layer_0': 100,\n",
    "        'dense_layer_1':  50,\n",
    "        'dense_layer_2': 100\n",
    "    }\n",
    "\n",
    "    if target_shape is None:\n",
    "        target_shape = input_shape\n",
    "    \n",
    "    # Define layers\n",
    "    input_layer = InputLayer(\n",
    "        shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Dense layers\n",
    "    dense_layer_0 = DenseLayer(\n",
    "        input_layer, \n",
    "        num_units=NUM_UNITS['dense_layer_0']\n",
    "    )\n",
    "    dense_layer_1 = DenseLayer(\n",
    "        dense_layer_0, \n",
    "        num_units=NUM_UNITS['dense_layer_1']\n",
    "    )\n",
    "    dense_layer_2 = DenseLayer(\n",
    "        dense_layer_1, \n",
    "        num_units=NUM_UNITS['dense_layer_2']\n",
    "    )\n",
    "    \n",
    "    # Output\n",
    "    final_dense_layer = DenseLayer(\n",
    "        dense_layer_2,\n",
    "        num_units=target_shape[1] * target_shape[2],\n",
    "        nonlinearity=None\n",
    "    )\n",
    "    output_layer = ReshapeLayer(\n",
    "        final_dense_layer,\n",
    "        shape=target_shape\n",
    "    )\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuralnilm.net import Net\n",
    "\n",
    "batch = pipeline.get_batch()\n",
    "output_layer = get_net_0(\n",
    "    batch.after_processing.input.shape, \n",
    "    batch.after_processing.target.shape\n",
    ")\n",
    "net = Net(output_layer, tags=['AE'], description=\"Just testing new NeuralNILM code!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database already has an experiment with _id == 5. Should the old experiment be deleted (both from the database and from disk)? Or quit? [Q/d] d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Deleting documents for old experiment.\n",
      "INFO:neuralnilm.trainer:Directory exists = '/home/dk3810/temp/neural_nilm/output/5'\n",
      "INFO:neuralnilm.trainer:  Deleting directory.\n"
     ]
    }
   ],
   "source": [
    "from neuralnilm.trainer import Trainer\n",
    "from neuralnilm.metrics import Metrics\n",
    "\n",
    "trainer = Trainer(\n",
    "    net=net,\n",
    "    data_pipeline=pipeline,\n",
    "    experiment_id=[\"5\"],\n",
    "    metrics=Metrics(state_boundaries=[4]),\n",
    "    learning_rates={0: 1E-2, 10000: 1E-3},\n",
    "    repeat_callbacks=[\n",
    "        (5000, Trainer.save_params),\n",
    "        (5000, Trainer.plot_estimates),\n",
    "        ( 500, Trainer.validate)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '5',\n",
       " 'data': {'activations': {'nilmtk_activations': {'appliances': ['kettle',\n",
       "     'microwave',\n",
       "     'washing machine'],\n",
       "    'filename': '/data/mine/vadeec/merged/ukdale.h5',\n",
       "    'sample_period': 6,\n",
       "    'windows': {'train': {'1': ('2014-01-01', '2014-02-01')},\n",
       "     'unseen_activations_of_seen_appliances': {'1': ('2014-02-02',\n",
       "       '2014-02-08')},\n",
       "     'unseen_appliances': {'2': ('2013-06-01', '2013-06-07')}}}},\n",
       "  'pipeline': {'input_processing': [{'divisor': 621.045654296875,\n",
       "     'name': 'DivideBy'},\n",
       "    {'name': 'IndependentlyCenter'}],\n",
       "   'num_seq_per_batch': 64,\n",
       "   'rng_seed': None,\n",
       "   'source_probabilities': [0.3333333333333333,\n",
       "    0.3333333333333333,\n",
       "    0.3333333333333333],\n",
       "   'sources': {'0': {'allow_incomplete_distractors': True,\n",
       "     'allow_incomplete_target': False,\n",
       "     'distractor_inclusion_prob': 0.25,\n",
       "     'include_incomplete_target_in_output': True,\n",
       "     'name': 'SyntheticAggregateSource',\n",
       "     'num_batches_for_validation': 16,\n",
       "     'rng_seed': None,\n",
       "     'sample_period': 6,\n",
       "     'seq_length': 256,\n",
       "     'target_appliance': 'kettle',\n",
       "     'target_inclusion_prob': 0.5,\n",
       "     'uniform_prob_of_selecting_each_building': True},\n",
       "    '1': {'allow_incomplete_target': True,\n",
       "     'allow_multiple_target_activations_in_aggregate': False,\n",
       "     'filename': '/data/mine/vadeec/merged/ukdale.h5',\n",
       "     'include_incomplete_target_in_output': True,\n",
       "     'include_multiple_targets_in_output': False,\n",
       "     'name': 'RealAggregateSource',\n",
       "     'num_batches_for_validation': 16,\n",
       "     'rng_seed': None,\n",
       "     'sample_period': 6,\n",
       "     'seq_length': 256,\n",
       "     'target_appliance': 'kettle',\n",
       "     'target_inclusion_prob': 0.5,\n",
       "     'uniform_prob_of_selecting_each_building': True,\n",
       "     'windows': {'train': {'1': ('2014-01-01', '2014-02-01')},\n",
       "      'unseen_activations_of_seen_appliances': {'1': ('2014-02-02',\n",
       "        '2014-02-08')},\n",
       "      'unseen_appliances': {'2': ('2013-06-01', '2013-06-07')}}},\n",
       "    '2': {'filename': '/data/mine/vadeec/merged/ukdale.h5',\n",
       "     'name': 'StrideSource',\n",
       "     'num_batches_for_validation': None,\n",
       "     'rng_seed': None,\n",
       "     'sample_period': 6,\n",
       "     'seq_length': 256,\n",
       "     'stride': 256,\n",
       "     'target_appliance': 'kettle',\n",
       "     'windows': {'train': {'1': ('2014-01-01', '2014-02-01')},\n",
       "      'unseen_activations_of_seen_appliances': {'1': ('2014-02-02',\n",
       "        '2014-02-08')},\n",
       "      'unseen_appliances': {'2': ('2013-06-01', '2013-06-07')}}}},\n",
       "   'target_processing': [{'divisor': 443.1661071777344, 'name': 'DivideBy'}]}},\n",
       " 'net': {'net': {'architecture': {'0': {'layers': [{'output_shape': (64,\n",
       "        256,\n",
       "        1),\n",
       "       'type': 'InputLayer'},\n",
       "      {'nonlinearity': 'rectify',\n",
       "       'num_units': 100,\n",
       "       'output_shape': (64, 100),\n",
       "       'type': 'DenseLayer'},\n",
       "      {'nonlinearity': 'rectify',\n",
       "       'num_units': 50,\n",
       "       'output_shape': (64, 50),\n",
       "       'type': 'DenseLayer'},\n",
       "      {'nonlinearity': 'rectify',\n",
       "       'num_units': 100,\n",
       "       'output_shape': (64, 100),\n",
       "       'type': 'DenseLayer'},\n",
       "      {'nonlinearity': 'linear',\n",
       "       'num_units': 256,\n",
       "       'output_shape': (64, 256),\n",
       "       'type': 'DenseLayer'},\n",
       "      {'output_shape': (64, 256, 1), 'type': 'ReshapeLayer'}],\n",
       "     'num_trainable_parameters': 61706}},\n",
       "   'description': 'Just testing new NeuralNILM code!',\n",
       "   'predecessor_experiment': '',\n",
       "   'tags': ['AE']}},\n",
       " 'trainer': {'loss_aggregation_mode': 'mean',\n",
       "  'loss_func_name': 'squared_error',\n",
       "  'metrics': {'clip_to_zero': False,\n",
       "   'name': 'Metrics',\n",
       "   'state_boundaries': [4]},\n",
       "  'min_train_cost': inf,\n",
       "  'output_path': '/home/dk3810/temp/neural_nilm/output/5',\n",
       "  'requested_learning_rates': {'0': 0.01, '10000': 0.001},\n",
       "  'updates_func_kwards': {},\n",
       "  'updates_func_name': 'nesterov_momentum'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = [(['data'], {'activations': LOADER_CONFIG})]\n",
    "trainer.submit_report(additional_report_contents=contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Starting training for 500000 iterations.\n",
      "INFO:neuralnilm.trainer:Iteration 0: Change learning rate to 1.0E-02\n",
      "INFO:neuralnilm.trainer:Compiling train cost function...\n",
      "INFO:neuralnilm.trainer:Done compiling cost function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Update # |  Train cost  | Secs per update | Source ID\n",
      "------------|--------------|-----------------|-----------\n",
      "          0 | \u001b[94m  0.189608\u001b[0m  |    2.470165     |   2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 0: Saving params.\n",
      "INFO:neuralnilm.trainer:Done saving params.\n",
      "INFO:neuralnilm.trainer:Iteration 0: Plotting estimates.\n",
      "INFO:neuralnilm.net:Compiling deterministic output function...\n",
      "INFO:neuralnilm.net:Done compiling deterministic output function.\n",
      "INFO:neuralnilm.trainer:Done plotting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          1 | \u001b[94m  0.165158\u001b[0m  |    0.081116     |   2\n",
      "          2 |   1.137131  |    0.036718     |   0\n",
      "          3 |   0.993763  |    0.049879     |   0\n",
      "          4 |   0.918286  |    0.151728     |   1\n",
      "          5 |   1.164499  |    0.148896     |   1\n",
      "          6 |   0.770006  |    0.024396     |   0\n",
      "          7 |   0.186775  |    0.043550     |   2\n",
      "          8 |   0.242785  |    0.050035     |   2\n",
      "          9 |   0.789430  |    0.044947     |   0\n",
      "         10 |   0.232419  |    0.045485     |   2\n",
      "         11 |   1.252085  |    0.080370     |   0\n",
      "         12 |   0.877916  |    0.042871     |   0\n",
      "         13 | \u001b[94m  0.106668\u001b[0m  |    0.045997     |   2\n",
      "         14 |   1.141130  |    0.073120     |   0\n",
      "         15 |   0.236916  |    0.049077     |   2\n",
      "         16 |   0.978012  |    0.042017     |   0\n",
      "         17 |   1.090046  |    0.051714     |   0\n",
      "         18 |   0.777272  |    0.047713     |   0\n",
      "         19 |   0.883186  |    0.026989     |   0\n",
      "         20 |   0.183250  |    0.081775     |   2\n",
      "         21 |   0.862558  |    0.159039     |   1\n",
      "         22 |   0.917566  |    0.209451     |   1\n",
      "         23 |   1.126190  |    0.152275     |   1\n",
      "         24 |   1.166884  |    0.015634     |   0\n",
      "         25 |   0.268374  |    0.077540     |   2\n",
      "         26 |   0.236040  |    0.021495     |   2\n",
      "         27 | \u001b[94m  0.055018\u001b[0m  |    0.077256     |   2\n",
      "         28 |   0.980661  |    0.210403     |   1\n",
      "         29 |   0.830826  |    0.171725     |   1\n",
      "         30 |   1.070557  |    0.159437     |   1\n",
      "         31 |   1.066686  |    0.100091     |   0\n",
      "         32 |   1.142070  |    0.166712     |   1\n",
      "         33 |   0.884610  |    0.093696     |   1\n",
      "         34 |   1.081133  |    0.140204     |   1\n",
      "         35 |   0.826068  |    0.198192     |   1\n",
      "         36 |   1.077955  |    0.131889     |   1\n",
      "         37 | \u001b[94m  0.000059\u001b[0m  |    0.018368     |   2\n",
      "         38 |   0.983480  |    0.194480     |   1\n",
      "         39 |   1.105650  |    0.005241     |   0\n",
      "         40 |   0.995142  |    0.044179     |   0\n",
      "         41 |   1.044275  |    0.192113     |   1\n",
      "         42 |   0.909544  |    0.168916     |   1\n",
      "         43 |   0.048220  |    0.052965     |   2\n",
      "         44 |   0.972772  |    0.090093     |   0\n",
      "         45 |   0.904912  |    0.157898     |   1\n",
      "         46 |   0.388077  |    0.003287     |   2\n",
      "         47 |   0.199991  |    0.048590     |   2\n",
      "         48 |   0.954071  |    0.084973     |   0\n",
      "         49 |   0.201022  |    0.027930     |   2\n",
      "         50 |   0.756562  |    0.043983     |   0\n",
      "         51 |   0.147864  |    0.056556     |   2\n",
      "         52 |   0.940904  |    0.047856     |   0\n",
      "         53 |   1.031341  |    0.057349     |   0\n",
      "         54 |   0.043724  |    0.041281     |   2\n",
      "         55 |   1.034133  |    0.074429     |   0\n",
      "         56 |   0.086187  |    0.035217     |   2\n",
      "         57 |   0.963335  |    0.220460     |   1\n",
      "         58 |   0.134648  |    0.020371     |   2\n",
      "         59 |   0.675017  |    0.161414     |   1\n",
      "         60 |   0.000079  |    0.052414     |   2\n",
      "         61 |   0.000078  |    0.049014     |   2\n",
      "         62 |   0.906528  |    0.168989     |   1\n",
      "         63 |   0.000078  |    0.020740     |   2\n",
      "         64 |   0.672927  |    0.047167     |   0\n",
      "         65 |   1.026015  |    0.081979     |   0\n",
      "         66 |   0.877804  |    0.026813     |   0\n",
      "         67 |   1.025393  |    0.082501     |   0\n",
      "         68 |   0.997557  |    0.020238     |   0\n",
      "         69 |   0.879340  |    0.189378     |   1\n",
      "         70 |   0.000081  |    0.026086     |   2\n",
      "         71 |   1.248310  |    0.077551     |   0\n",
      "         72 |   0.882654  |    0.152148     |   1\n",
      "         73 |   1.174852  |    0.142165     |   1\n",
      "         74 |   0.979815  |    0.045931     |   0\n",
      "         75 |   0.964858  |    0.073523     |   0\n",
      "         76 |   0.842527  |    0.201452     |   1\n",
      "         77 |   0.000089  |    0.007690     |   2\n",
      "         78 |   0.000082  |    0.080130     |   2\n",
      "         79 |   0.264195  |    0.023455     |   2\n",
      "         80 |   1.019778  |    0.192825     |   1\n",
      "         81 |   0.196659  |    0.033071     |   2\n",
      "         82 |   1.011864  |    0.082596     |   0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "INFO:neuralnilm.trainer:Iteration 83: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "         83 |   0.146120  |    0.006648     |   2\n",
      "         84 |   1.055282  |    0.081386     |   0\n",
      "         85 |   0.730446  |    0.033136     |   0\n",
      "         86 |   0.167740  |    0.075695     |   2\n",
      "         87 |   0.229373  |    0.024011     |   2\n",
      "         88 |   1.045265  |    0.077378     |   0\n",
      "         89 |   1.004057  |    0.011973     |   0\n",
      "         90 |   0.212298  |    0.075599     |   2\n",
      "         91 |   0.101851  |    0.039537     |   2\n",
      "         92 |   0.643727  |    0.171553     |   1\n",
      "         93 |   0.969674  |    0.191239     |   1\n",
      "         94 |   1.055108  |    0.019488     |   0\n",
      "         95 |   0.225060  |    0.045756     |   2\n",
      "         96 |   0.940348  |    0.050391     |   0\n",
      "         97 |   0.171224  |    0.068156     |   2\n",
      "         98 |   0.804357  |    0.027395     |   0\n",
      "         99 |   0.897211  |    0.166117     |   1\n",
      "        100 |   0.952372  |    0.062391     |   0\n",
      "        101 |   0.885221  |    0.199605     |   1\n",
      "        102 |   0.254692  |    0.009766     |   2\n",
      "        103 |   0.924193  |    0.052854     |   0\n",
      "        104 |   0.925032  |    0.153376     |   1\n",
      "        105 |   0.219505  |    0.082352     |   2\n",
      "        106 |   0.971242  |    0.011144     |   0\n",
      "        107 |   0.050970  |    0.080792     |   2\n",
      "        108 |   0.967268  |    0.009803     |   0\n",
      "        109 |   0.000138  |    0.078533     |   2\n",
      "        110 |   0.726943  |    0.020254     |   0\n",
      "        111 |   0.835096  |    0.078913     |   0\n",
      "        112 |   0.998196  |    0.142843     |   1\n",
      "        113 |   0.046777  |    0.007571     |   2\n",
      "        114 |   0.376055  |    0.053584     |   2\n",
      "        115 |   1.083014  |    0.195381     |   1\n",
      "        116 |   0.193907  |    0.006589     |   2\n",
      "        117 |   0.987150  |    0.065185     |   0\n",
      "        118 |   0.194846  |    0.026618     |   2\n",
      "        119 |   0.905971  |    0.051611     |   0\n",
      "        120 |   0.143162  |    0.076856     |   2\n",
      "        121 |   0.042669  |    0.038275     |   2\n",
      "        122 |   0.943806  |    0.203300     |   1\n",
      "        123 |   0.085532  |    0.016428     |   2\n",
      "        124 |   0.131977  |    0.081279     |   2\n",
      "        125 |   0.871808  |    0.152555     |   1\n",
      "        126 |   1.120461  |    0.046265     |   0\n",
      "        127 |   0.000186  |    0.039630     |   2\n",
      "        128 |   0.834795  |    0.076508     |   0\n",
      "        129 |   0.000189  |    0.040526     |   2\n",
      "        130 |   0.000184  |    0.045340     |   2\n",
      "        131 |   0.905130  |    0.079472     |   0\n",
      "        132 |   0.860526  |    0.157347     |   1\n",
      "        133 |   0.000187  |    0.052611     |   2\n",
      "        134 |   0.000185  |    0.038450     |   2\n",
      "        135 |   0.000177  |    0.053439     |   2\n",
      "        136 |   0.256898  |    0.054099     |   2\n",
      "        137 |   0.808911  |    0.159484     |   1\n",
      "        138 |   0.194732  |    0.017071     |   2\n",
      "        139 |   0.973931  |    0.209150     |   1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 140: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        140 |   0.144615  |    0.008310     |   2\n",
      "        141 |   0.898291  |    0.191475     |   1\n",
      "        142 |   0.165868  |    0.003733     |   2\n",
      "        143 |   1.171165  |    0.040663     |   0\n",
      "        144 |   0.944722  |    0.165662     |   1\n",
      "        145 |   0.609683  |    0.145538     |   1\n",
      "        146 |   0.757971  |    0.164612     |   1\n",
      "        147 |   0.787673  |    0.186132     |   1\n",
      "        148 |   1.102141  |    0.006984     |   0\n",
      "        149 |   1.091392  |    0.078261     |   0\n",
      "        150 |   0.984653  |    0.033872     |   0\n",
      "        151 |   0.224336  |    0.051557     |   2\n",
      "        152 |   0.744833  |    0.201743     |   1\n",
      "        153 |   0.874299  |    0.019421     |   0\n",
      "        154 |   1.065594  |    0.076252     |   0\n",
      "        155 |   0.805435  |    0.183550     |   1\n",
      "        156 |   0.208694  |    0.015227     |   2\n",
      "        157 |   0.101658  |    0.049583     |   2\n",
      "        158 |   0.221086  |    0.080499     |   2\n",
      "        159 |   0.168313  |    0.073539     |   2\n",
      "        160 |   0.248701  |    0.048908     |   2\n",
      "        161 |   0.787069  |    0.265057     |   1\n",
      "        162 |   0.861710  |    0.030419     |   0\n",
      "        163 |   0.970888  |    0.209932     |   1\n",
      "        164 |   1.038692  |    0.015158     |   0\n",
      "        165 |   0.861992  |    0.199101     |   1\n",
      "        166 |   0.960373  |    0.048020     |   0\n",
      "        167 |   0.901101  |    0.040713     |   0\n",
      "        168 |   0.860611  |    0.049275     |   0\n",
      "        169 |   0.831580  |    0.070642     |   0\n",
      "        170 |   0.839567  |    0.154057     |   1\n",
      "        171 |   0.930778  |    0.151103     |   1\n",
      "        172 |   1.104282  |    0.029975     |   0\n",
      "        173 |   0.210982  |    0.044884     |   2\n",
      "        174 |   0.972303  |    0.086091     |   0\n",
      "        175 |   1.034880  |    0.150281     |   1\n",
      "        176 |   0.941674  |    0.136320     |   1\n",
      "        177 |   0.916437  |    0.088964     |   0\n",
      "        178 |   0.799621  |    0.142021     |   1\n",
      "        179 |   0.893875  |    0.045941     |   0\n",
      "        180 |   0.051912  |    0.072226     |   2\n",
      "        181 |   0.000347  |    0.024720     |   2\n",
      "        182 |   0.045432  |    0.087499     |   2\n",
      "        183 |   0.367873  |    0.029457     |   2\n",
      "        184 |   0.187845  |    0.085562     |   2\n",
      "        185 |   0.191279  |    0.013760     |   2\n",
      "        186 |   0.142572  |    0.138876     |   2\n",
      "        187 |   0.936404  |    0.154427     |   1\n",
      "        188 |   0.948080  |    0.165444     |   1\n",
      "        189 |   0.688012  |    0.190160     |   1\n",
      "        190 |   0.658071  |    0.197400     |   1\n",
      "        191 |   0.864760  |    0.156511     |   1\n",
      "        192 |   0.951270  |    0.032400     |   0\n",
      "        193 |   0.042637  |    0.078370     |   2\n",
      "        194 |   0.088949  |    0.019677     |   2\n",
      "        195 |   0.643161  |    0.296378     |   1\n",
      "        196 |   0.991742  |    0.082665     |   0\n",
      "        197 |   0.130698  |    0.077495     |   2\n",
      "        198 |   1.096880  |    0.055363     |   0\n",
      "        199 |   0.808797  |    0.242150     |   1\n",
      "        200 |   0.663602  |    0.145161     |   1\n",
      "        201 |   0.976591  |    0.186339     |   1\n",
      "        202 |   0.897250  |    0.025121     |   0\n",
      "        203 |   0.795384  |    0.040343     |   0\n",
      "        204 |   0.000446  |    0.074901     |   2\n",
      "        205 |   0.910752  |    0.042664     |   0\n",
      "        206 |   0.000453  |    0.058968     |   2\n",
      "        207 |   0.877823  |    0.210008     |   1\n",
      "        208 |   1.035490  |    0.023488     |   0\n",
      "        209 |   1.088436  |    0.084561     |   0\n",
      "        210 |   1.021922  |    0.029194     |   0\n",
      "        211 |   0.000465  |    0.061432     |   2\n",
      "        212 |   0.763355  |    0.197005     |   1\n",
      "        213 |   0.959891  |    0.144476     |   1\n",
      "        214 |   0.857893  |    0.148295     |   1\n",
      "        215 |   0.000472  |    0.055574     |   2\n",
      "        216 |   0.938629  |    0.255319     |   1\n",
      "        217 |   0.822183  |    0.146506     |   1\n",
      "        218 |   0.959100  |    0.189901     |   1\n",
      "        219 |   0.000492  |    0.047561     |   2\n",
      "        220 |   0.000475  |    0.055155     |   2\n",
      "        221 |   0.956595  |    0.151452     |   1\n",
      "        222 |   0.245836  |    0.044728     |   2\n",
      "        223 |   0.757684  |    0.188019     |   1\n",
      "        224 |   0.859443  |    0.163080     |   1\n",
      "        225 |   0.659318  |    0.144751     |   1\n",
      "        226 |   0.967681  |    0.138386     |   1\n",
      "        227 |   0.794887  |    0.155201     |   1\n",
      "        228 |   0.195613  |    0.042052     |   2\n",
      "        229 |   0.742797  |    0.161803     |   1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 231: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        230 |   0.835133  |    0.007707     |   0\n",
      "        231 |   0.731186  |    0.059266     |   0\n",
      "        232 |   1.077196  |    0.142674     |   1\n",
      "        233 |   0.146835  |    0.021211     |   2\n",
      "        234 |   0.166596  |    0.083717     |   2\n",
      "        235 |   0.216905  |    0.022370     |   2\n",
      "        236 |   0.852179  |    0.077772     |   0\n",
      "        237 |   0.206893  |    0.028695     |   2\n",
      "        238 |   0.893238  |    0.237439     |   1\n",
      "        239 |   0.848365  |    0.151123     |   1\n",
      "        240 |   0.980546  |    0.039048     |   0\n",
      "        241 |   0.752686  |    0.075628     |   0\n",
      "        242 |   0.703813  |    0.029386     |   0\n",
      "        243 |   0.730398  |    0.218759     |   1\n",
      "        244 |   0.739441  |    0.213812     |   1\n",
      "        245 |   0.104018  |    0.004792     |   2\n",
      "        246 |   0.831278  |    0.085248     |   0\n",
      "        247 |   0.216132  |    0.005210     |   2\n",
      "        248 |   0.165741  |    0.081908     |   2\n",
      "        249 |   0.238338  |    0.011651     |   2\n",
      "        250 |   0.202157  |    0.089941     |   2\n",
      "        251 |   0.826316  |    0.157585     |   1\n",
      "        252 |   1.060470  |    0.031726     |   0\n",
      "        253 |   1.015231  |    0.052531     |   0\n",
      "        254 |   0.903012  |    0.059267     |   0\n",
      "        255 |   0.792786  |    0.041751     |   0\n",
      "        256 |   0.053323  |    0.049483     |   2\n",
      "        257 |   0.651483  |    0.186742     |   1\n",
      "        258 |   0.827075  |    0.004625     |   0\n",
      "        259 |   0.000651  |    0.041782     |   2\n",
      "        260 |   1.127126  |    0.147039     |   1\n",
      "        261 |   0.043909  |    0.039465     |   2\n",
      "        262 |   0.358036  |    0.079132     |   2\n",
      "        263 |   0.179194  |    0.025796     |   2\n",
      "        264 |   0.955973  |    0.085257     |   0\n",
      "        265 |   0.187683  |    0.005821     |   2\n",
      "        266 |   0.142850  |    0.075985     |   2\n",
      "        267 |   0.043081  |    0.043560     |   2\n",
      "        268 |   0.094748  |    0.047650     |   2\n",
      "        269 |   0.130408  |    0.054613     |   2\n",
      "        270 |   0.875754  |    0.201062     |   1\n",
      "        271 |   0.000699  |    0.014935     |   2\n",
      "        272 |   0.654140  |    0.192826     |   1\n",
      "        273 |   0.000699  |    0.008423     |   2\n",
      "        274 |   1.028151  |    0.089409     |   0\n",
      "        275 |   0.000682  |    0.014400     |   2\n",
      "        276 |   0.749926  |    0.076960     |   0\n",
      "        277 |   0.000665  |    0.047345     |   2\n",
      "        278 |   0.836431  |    0.078363     |   0\n",
      "        279 |   0.914211  |    0.021670     |   0\n",
      "        280 |   0.789090  |    0.205128     |   1\n",
      "        281 |   0.946197  |    0.159608     |   1\n",
      "        282 |   0.899941  |    0.194924     |   1\n",
      "        283 |   1.009605  |    0.158020     |   1\n",
      "        284 |   0.991618  |    0.007592     |   0\n",
      "        285 |   0.000704  |    0.078796     |   2\n",
      "        286 |   0.000676  |    0.072560     |   2\n",
      "        287 |   0.237941  |    0.075704     |   2\n",
      "        288 |   0.728274  |    0.199284     |   1\n",
      "        289 |   0.836457  |    0.015559     |   0\n",
      "        290 |   0.872191  |    0.191687     |   1\n",
      "        291 |   0.772618  |    0.214197     |   1\n",
      "        292 |   1.013832  |    0.103132     |   1\n",
      "        293 |   0.823972  |    0.076581     |   0\n",
      "        294 |   0.197264  |    0.046822     |   2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 295: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        295 |   0.149347  |    0.074851     |   2\n",
      "        296 |   0.166486  |    0.148722     |   2\n",
      "        297 |   0.631409  |    0.218802     |   1\n",
      "        298 |   0.953262  |    0.140257     |   1\n",
      "        299 |   0.212046  |    0.005360     |   2\n",
      "        300 |   0.709170  |    0.079551     |   0\n",
      "        301 |   0.207678  |    0.015772     |   2\n",
      "        302 |   0.706920  |    0.132968     |   0\n",
      "        303 |   0.758840  |    0.039411     |   0\n",
      "        304 |   0.913177  |    0.073662     |   0\n",
      "        305 |   0.639115  |    0.061153     |   0\n",
      "        306 |   0.650077  |    0.203308     |   1\n",
      "        307 |   0.964721  |    0.146266     |   1\n",
      "        308 |   0.886618  |    0.208445     |   1\n",
      "        309 |   0.980117  |    0.004221     |   0\n",
      "        310 |   0.685212  |    0.241521     |   1\n",
      "        311 |   0.798631  |    0.082321     |   0\n",
      "        312 |   0.635938  |    0.026190     |   0\n",
      "        313 |   0.883127  |    0.084611     |   0\n",
      "        314 |   1.102405  |    0.045575     |   0\n",
      "        315 |   0.839159  |    0.318081     |   1\n",
      "        316 |   0.775209  |    0.136336     |   1\n",
      "        317 |   0.628856  |    0.220390     |   1\n",
      "        318 |   0.726869  |    0.160353     |   1\n",
      "        319 |   0.841533  |    0.201479     |   1\n",
      "        320 |   0.107064  |    0.043171     |   2\n",
      "        321 |   0.998359  |    0.208186     |   1\n",
      "        322 |   0.213947  |    0.039706     |   2\n",
      "        323 |   0.912546  |    0.201804     |   1\n",
      "        324 |   0.720879  |    0.010498     |   0\n",
      "        325 |   0.673570  |    0.202614     |   1\n",
      "        326 |   0.687501  |    0.212412     |   1\n",
      "        327 |   1.056221  |    0.160504     |   1\n",
      "        328 |   0.971969  |    0.141703     |   1\n",
      "        329 |   0.573485  |    0.200368     |   1\n",
      "        330 |   1.031827  |    0.004354     |   0\n",
      "        331 |   0.761489  |    0.077921     |   0\n",
      "        332 |   0.167010  |    0.124668     |   2\n",
      "        333 |   0.680797  |    0.223964     |   1\n",
      "        334 |   0.942073  |    0.148680     |   1\n",
      "        335 |   0.958811  |    0.052733     |   0\n",
      "        336 |   0.745367  |    0.041354     |   0\n",
      "        337 |   0.826738  |    0.199421     |   1\n",
      "        338 |   0.225553  |    0.012716     |   2\n",
      "        339 |   0.194622  |    0.075137     |   2\n",
      "        340 |   0.056071  |    0.039974     |   2\n",
      "        341 |   0.000969  |    0.046080     |   2\n",
      "        342 |   0.041444  |    0.050134     |   2\n",
      "        343 |   0.839296  |    0.168727     |   1\n",
      "        344 |   0.734182  |    0.014127     |   0\n",
      "        345 |   0.938304  |    0.071607     |   0\n",
      "        346 |   0.958735  |    0.034551     |   0\n",
      "        347 |   0.559210  |    0.190810     |   1\n",
      "        348 |   0.634824  |    0.028932     |   0\n",
      "        349 |   0.871346  |    0.207790     |   1\n",
      "        350 |   0.756043  |    0.095001     |   1\n",
      "        351 |   0.747996  |    0.206374     |   1\n",
      "        352 |   0.349762  |    0.003885     |   2\n",
      "        353 |   0.167731  |    0.035495     |   2\n",
      "        354 |   1.084236  |    0.072323     |   0\n",
      "        355 |   0.732378  |    0.022590     |   0\n",
      "        356 |   0.693614  |    0.051469     |   0\n",
      "        357 |   0.803902  |    0.199658     |   1\n",
      "        358 |   0.931427  |    0.091849     |   1\n",
      "        359 |   0.770652  |    0.053056     |   0\n",
      "        360 |   0.183994  |    0.052977     |   2\n",
      "        361 |   0.731119  |    0.203605     |   1\n",
      "        362 |   0.590518  |    0.008114     |   0\n",
      "        363 |   0.142247  |    0.047459     |   2\n",
      "        364 |   0.993942  |    0.076799     |   0\n",
      "        365 |   0.762891  |    0.171842     |   1\n",
      "        366 |   0.042787  |    0.023174     |   2\n",
      "        367 |   0.106711  |    0.051503     |   2\n",
      "        368 |   0.131322  |    0.029997     |   2\n",
      "        369 |   0.764551  |    0.203799     |   1\n",
      "        370 |   0.945937  |    0.018857     |   0\n",
      "        371 |   0.674170  |    0.216210     |   1\n",
      "        372 |   1.001644  |    0.145002     |   1\n",
      "        373 |   0.001030  |    0.032028     |   2\n",
      "        374 |   0.001030  |    0.079713     |   2\n",
      "        375 |   0.996451  |    0.064504     |   0\n",
      "        376 |   1.015837  |    0.066729     |   0\n",
      "        377 |   0.843369  |    0.074342     |   0\n",
      "        378 |   0.001012  |    0.104355     |   2\n",
      "        379 |   0.000977  |    0.039739     |   2\n",
      "        380 |   0.919446  |    0.234241     |   1\n",
      "        381 |   0.001016  |    0.020229     |   2\n",
      "        382 |   0.617649  |    0.191067     |   1\n",
      "        383 |   0.925180  |    0.048134     |   0\n",
      "        384 |   0.000967  |    0.041777     |   2\n",
      "        385 |   0.826602  |    0.186573     |   1\n",
      "        386 |   0.226060  |    0.010835     |   2\n",
      "        387 |   0.201267  |    0.048364     |   2\n",
      "        388 |   0.783004  |    0.207755     |   1\n",
      "        389 |   0.786853  |    0.161384     |   1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 390: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        390 |   0.761090  |    0.164629     |   1\n",
      "        391 |   0.848625  |    0.056043     |   0\n",
      "        392 |   0.154815  |    0.058795     |   2\n",
      "        393 |   0.169365  |    0.051111     |   2\n",
      "        394 |   0.708246  |    0.050919     |   0\n",
      "        395 |   0.203272  |    0.042247     |   2\n",
      "        396 |   0.752339  |    0.148907     |   1\n",
      "        397 |   0.206007  |    0.076864     |   2\n",
      "        398 |   0.106874  |    0.028244     |   2\n",
      "        399 |   0.812719  |    0.194678     |   1\n",
      "        400 |   0.959113  |    0.157492     |   1\n",
      "        401 |   0.834534  |    0.013654     |   0\n",
      "        402 |   0.899071  |    0.078290     |   0\n",
      "        403 |   0.816780  |    0.029910     |   0\n",
      "        404 |   0.209759  |    0.074672     |   2\n",
      "        405 |   0.912325  |    0.043301     |   0\n",
      "        406 |   0.163992  |    0.049231     |   2\n",
      "        407 |   0.217084  |    0.055871     |   2\n",
      "        408 |   0.741053  |    0.143935     |   1\n",
      "        409 |   0.188534  |    0.090284     |   2\n",
      "        410 |   0.775698  |    0.149716     |   1\n",
      "        411 |   0.675794  |    0.153721     |   1\n",
      "        412 |   0.775550  |    0.048807     |   0\n",
      "        413 |   0.583302  |    0.218364     |   1\n",
      "        414 |   0.874985  |    0.137872     |   1\n",
      "        415 |   0.055046  |    0.041583     |   2\n",
      "        416 |   0.001027  |    0.044817     |   2\n",
      "        417 |   0.680391  |    0.085464     |   0\n",
      "        418 |   0.678721  |    0.026955     |   0\n",
      "        419 |   0.625192  |    0.204495     |   1\n",
      "        420 |   0.950921  |    0.140999     |   1\n",
      "        421 |   0.040074  |    0.030964     |   2\n",
      "        422 |   0.340895  |    0.078249     |   2\n",
      "        423 |   0.658091  |    0.029475     |   0\n",
      "        424 |   0.706654  |    0.038166     |   0\n",
      "        425 |   0.616962  |    0.078267     |   0\n",
      "        426 |   0.160811  |    0.010048     |   2\n",
      "        427 |   0.943417  |    0.192909     |   1\n",
      "        428 |   0.926526  |    0.019381     |   0\n",
      "        429 |   0.748129  |    0.085685     |   0\n",
      "        430 |   0.709779  |    0.141762     |   1\n",
      "        431 |   0.644346  |    0.150738     |   1\n",
      "        432 |   0.698070  |    0.042575     |   0\n",
      "        433 |   0.742352  |    0.188938     |   1\n",
      "        434 |   0.180349  |    0.036402     |   2\n",
      "        435 |   0.139936  |    0.044941     |   2\n",
      "        436 |   0.623621  |    0.193224     |   1\n",
      "        437 |   0.043287  |    0.003458     |   2\n",
      "        438 |   0.109576  |    0.081690     |   2\n",
      "        439 |   0.804564  |    0.195693     |   1\n",
      "        440 |   0.129823  |    0.014139     |   2\n",
      "        441 |   1.007305  |    0.084162     |   0\n",
      "        442 |   0.001051  |    0.028312     |   2\n",
      "        443 |   0.624218  |    0.195042     |   1\n",
      "        444 |   0.001061  |    0.025755     |   2\n",
      "        445 |   0.001018  |    0.042945     |   2\n",
      "        446 |   0.699777  |    0.043942     |   0\n",
      "        447 |   0.940329  |    0.077243     |   0\n",
      "        448 |   0.654560  |    0.017369     |   0\n",
      "        449 |   0.757515  |    0.197976     |   1\n",
      "        450 |   0.792493  |    0.005462     |   0\n",
      "        451 |   0.001012  |    0.052737     |   2\n",
      "        452 |   0.715797  |    0.199924     |   1\n",
      "        453 |   0.869505  |    0.010674     |   0\n",
      "        454 |   0.698688  |    0.198262     |   1\n",
      "        455 |   0.001071  |    0.007826     |   2\n",
      "        456 |   0.664741  |    0.186997     |   1\n",
      "        457 |   0.795381  |    0.050012     |   0\n",
      "        458 |   0.808905  |    0.058628     |   0\n",
      "        459 |   0.681854  |    0.170447     |   1\n",
      "        460 |   0.001009  |    0.046894     |   2\n",
      "        461 |   0.917914  |    0.077627     |   0\n",
      "        462 |   0.215478  |    0.009742     |   2\n",
      "        463 |   0.204087  |    0.052747     |   2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 464: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        464 |   0.474753  |    0.040273     |   0\n",
      "        465 |   0.154931  |    0.049947     |   2\n",
      "        466 |   0.614433  |    0.204836     |   1\n",
      "        467 |   0.570396  |    0.145772     |   1\n",
      "        468 |   0.636297  |    0.023078     |   0\n",
      "        469 |   0.824250  |    0.191844     |   1\n",
      "        470 |   0.793992  |    0.053220     |   0\n",
      "        471 |   0.167899  |    0.045441     |   2\n",
      "        472 |   0.198015  |    0.074380     |   2\n",
      "        473 |   0.201165  |    0.031818     |   2\n",
      "        474 |   0.765369  |    0.147617     |   1\n",
      "        475 |   0.105194  |    0.031997     |   2\n",
      "        476 |   0.522291  |    0.210440     |   1\n",
      "        477 |   0.697815  |    0.209301     |   1\n",
      "        478 |   0.634249  |    0.221405     |   1\n",
      "        479 |   0.842745  |    0.063353     |   1\n",
      "        480 |   0.914940  |    0.041685     |   0\n",
      "        481 |   0.699609  |    0.082086     |   0\n",
      "        482 |   1.011962  |    0.012410     |   0\n",
      "        483 |   0.205922  |    0.095012     |   2\n",
      "        484 |   0.570764  |    0.145895     |   1\n",
      "        485 |   0.707189  |    0.045445     |   0\n",
      "        486 |   0.624137  |    0.221224     |   1\n",
      "        487 |   0.889237  |    0.014952     |   0\n",
      "        488 |   0.157775  |    0.076238     |   2\n",
      "        489 |   0.857709  |    0.055141     |   0\n",
      "        490 |   0.767907  |    0.062502     |   0\n",
      "        491 |   0.640105  |    0.195553     |   1\n",
      "        492 |   0.736443  |    0.146111     |   1\n",
      "        493 |   0.914033  |    0.011305     |   0\n",
      "        494 |   0.770190  |    0.044008     |   0\n",
      "        495 |   0.204565  |    0.029499     |   2\n",
      "        496 |   0.182923  |    0.078470     |   2\n",
      "        497 |   0.635220  |    0.070357     |   0\n",
      "        498 |   0.636098  |    0.147868     |   1\n",
      "        499 |   0.759444  |    0.028056     |   0\n",
      "        500 |   0.054035  |    0.071327     |   2\n",
      "        501 |   0.778526  |    0.212633     |   1\n",
      "        502 |   0.154912  |    0.006519     |   2\n",
      "        503 |   0.165957  |    0.078752     |   2\n",
      "        504 |   0.195958  |    0.024846     |   2\n",
      "        505 |   0.538524  |    0.056939     |   0\n",
      "        506 |   0.199895  |    0.054686     |   2\n",
      "        507 |   0.794789  |    0.046664     |   0\n",
      "        508 |   0.843439  |    0.047479     |   0\n",
      "        509 |   0.751816  |    0.049964     |   0\n",
      "        510 |   0.104424  |    0.051933     |   2\n",
      "        511 |   0.735508  |    0.023116     |   0\n",
      "        512 |   0.203429  |    0.090548     |   2\n",
      "        513 |   0.738547  |    0.160311     |   1\n",
      "        514 |   0.664284  |    0.004962     |   0\n",
      "        515 |   0.759213  |    0.130262     |   0\n",
      "        516 |   0.945925  |    0.010747     |   0\n",
      "        517 |   0.913329  |    0.190873     |   1\n",
      "        518 |   0.155839  |    0.033235     |   2\n",
      "        519 |   0.759893  |    0.160544     |   1\n",
      "        520 |   0.200349  |    0.003961     |   2\n",
      "        521 |   0.180025  |    0.069751     |   2\n",
      "        522 |   0.053478  |    0.039374     |   2\n",
      "        523 |   0.743999  |    0.137853     |   1\n",
      "        524 |   0.900874  |    0.046584     |   0\n",
      "        525 |   0.001076  |    0.079311     |   2\n",
      "        526 |   0.037126  |    0.014995     |   2\n",
      "        527 |   0.625605  |    0.074745     |   0\n",
      "        528 |   0.661017  |    0.145679     |   1\n",
      "        529 |   0.325491  |    0.052087     |   2\n",
      "        530 |   0.818695  |    0.182141     |   1\n",
      "        531 |   0.646352  |    0.161868     |   1\n",
      "        532 |   0.691681  |    0.202595     |   1\n",
      "        533 |   0.147701  |    0.003775     |   2\n",
      "        534 |   0.696822  |    0.044465     |   0\n",
      "        535 |   0.771038  |    0.054096     |   0\n",
      "        536 |   0.823503  |    0.050452     |   0\n",
      "        537 |   0.884671  |    0.157913     |   1\n",
      "        538 |   0.748396  |    0.163077     |   1\n",
      "        539 |   0.753115  |    0.158348     |   1\n",
      "        540 |   0.718736  |    0.151401     |   1\n",
      "        541 |   0.175694  |    0.049088     |   2\n",
      "        542 |   0.134070  |    0.047361     |   2\n",
      "        543 |   0.440728  |    0.195858     |   1\n",
      "        544 |   0.043188  |    0.025712     |   2\n",
      "        545 |   0.118596  |    0.072565     |   2\n",
      "        546 |   0.831671  |    0.010552     |   0\n",
      "        547 |   0.595829  |    0.203024     |   1\n",
      "        548 |   0.624220  |    0.145626     |   1\n",
      "        549 |   0.127628  |    0.016139     |   2\n",
      "        550 |   0.743336  |    0.172111     |   1\n",
      "        551 |   0.793861  |    0.075415     |   0\n",
      "        552 |   0.001075  |    0.029405     |   2\n",
      "        553 |   0.610794  |    0.034267     |   0\n",
      "        554 |   0.001079  |    0.059080     |   2\n",
      "        555 |   0.795103  |    0.039937     |   0\n",
      "        556 |   0.862273  |    0.075717     |   0\n",
      "        557 |   0.673429  |    0.154607     |   1\n",
      "        558 |   0.820544  |    0.051189     |   0\n",
      "        559 |   0.001036  |    0.042780     |   2\n",
      "        560 |   0.001072  |    0.074239     |   2\n",
      "        561 |   0.683560  |    0.183697     |   1\n",
      "        562 |   0.738506  |    0.105264     |   1\n",
      "        563 |   0.880968  |    0.029773     |   0\n",
      "        564 |   0.001154  |    0.059906     |   2\n",
      "        565 |   0.001047  |    0.027850     |   2\n",
      "        566 |   0.667140  |    0.219033     |   1\n",
      "        567 |   0.801308  |    0.011219     |   0\n",
      "        568 |   0.200445  |    0.079121     |   2\n",
      "        569 |   0.707312  |    0.036262     |   0\n",
      "        570 |   0.208982  |    0.074825     |   2\n",
      "        571 |   0.768573  |    0.161608     |   1\n",
      "        572 |   0.591793  |    0.050382     |   0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 573: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        573 |   0.773002  |    0.138586     |   1\n",
      "        574 |   0.786564  |    0.206675     |   1\n",
      "        575 |   0.963066  |    0.053051     |   0\n",
      "        576 |   0.160136  |    0.032006     |   2\n",
      "        577 |   0.805979  |    0.214223     |   1\n",
      "        578 |   0.164843  |    0.025166     |   2\n",
      "        579 |   0.708965  |    0.191574     |   1\n",
      "        580 |   0.563302  |    0.047475     |   0\n",
      "        581 |   0.511195  |    0.043928     |   0\n",
      "        582 |   0.738554  |    0.030036     |   0\n",
      "        583 |   0.190756  |    0.060970     |   2\n",
      "        584 |   0.699436  |    0.085365     |   0\n",
      "        585 |   0.198594  |    0.091070     |   2\n",
      "        586 |   0.455716  |    0.194950     |   1\n",
      "        587 |   0.680396  |    0.136285     |   1\n",
      "        588 |   0.681418  |    0.045356     |   0\n",
      "        589 |   0.591342  |    0.075722     |   0\n",
      "        590 |   0.587482  |    0.221336     |   1\n",
      "        591 |   0.619317  |    0.009820     |   0\n",
      "        592 |   0.104929  |    0.061539     |   2\n",
      "        593 |   0.200836  |    0.074864     |   2\n",
      "        594 |   0.767361  |    0.159013     |   1\n",
      "        595 |   0.752749  |    0.003425     |   0\n",
      "        596 |   0.152150  |    0.047562     |   2\n",
      "        597 |   0.189659  |    0.042548     |   2\n",
      "        598 |   0.779148  |    0.081147     |   0\n",
      "        599 |   0.173306  |    0.022946     |   2\n",
      "        600 |   0.752023  |    0.081120     |   0\n",
      "        601 |   0.654934  |    0.160136     |   1\n",
      "        602 |   0.053542  |    0.016834     |   2\n",
      "        603 |   0.001117  |    0.082057     |   2\n",
      "        604 |   0.676985  |    0.157724     |   1\n",
      "        605 |   0.034321  |    0.008533     |   2\n",
      "        606 |   0.626269  |    0.127469     |   0\n",
      "        607 |   0.680917  |    0.143652     |   1\n",
      "        608 |   0.637007  |    0.032417     |   0\n",
      "        609 |   0.763251  |    0.205637     |   1\n",
      "        610 |   0.678495  |    0.004130     |   0\n",
      "        611 |   0.317200  |    0.084394     |   2\n",
      "        612 |   0.689532  |    0.136733     |   1\n",
      "        613 |   0.136845  |    0.047230     |   2\n",
      "        614 |   0.772216  |    0.147707     |   1\n",
      "        615 |   0.783859  |    0.036113     |   0\n",
      "        616 |   0.803714  |    0.204454     |   1\n",
      "        617 |   0.640218  |    0.048259     |   0\n",
      "        618 |   0.641513  |    0.188918     |   1\n",
      "        619 |   0.171060  |    0.007572     |   2\n",
      "        620 |   0.130760  |    0.085659     |   2\n",
      "        621 |   0.044297  |    0.013453     |   2\n",
      "        622 |   0.665050  |    0.079740     |   0\n",
      "        623 |   0.123437  |    0.030597     |   2\n",
      "        624 |   0.751894  |    0.233502     |   1\n",
      "        625 |   0.742341  |    0.202317     |   1\n",
      "        626 |   0.128151  |    0.005756     |   2\n",
      "        627 |   0.001054  |    0.060991     |   2\n",
      "        628 |   0.660787  |    0.202080     |   1\n",
      "        629 |   0.001070  |    0.030083     |   2\n",
      "        630 |   0.596601  |    0.204118     |   1\n",
      "        631 |   0.623566  |    0.154090     |   1\n",
      "        632 |   0.000992  |    0.037552     |   2\n",
      "        633 |   0.686875  |    0.051944     |   0\n",
      "        634 |   0.703746  |    0.153150     |   1\n",
      "        635 |   0.001030  |    0.026401     |   2\n",
      "        636 |   0.695137  |    0.051661     |   0\n",
      "        637 |   0.625822  |    0.052007     |   0\n",
      "        638 |   0.697118  |    0.197500     |   1\n",
      "        639 |   0.668961  |    0.015542     |   0\n",
      "        640 |   0.001099  |    0.056033     |   2\n",
      "        641 |   0.660109  |    0.194910     |   1\n",
      "        642 |   0.583745  |    0.152833     |   1\n",
      "        643 |   0.694535  |    0.240481     |   1\n",
      "        644 |   0.644886  |    0.136186     |   1\n",
      "        645 |   0.000990  |    0.021961     |   2\n",
      "        646 |   0.188420  |    0.078802     |   2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 648: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        647 |   0.209191  |    0.006631     |   2\n",
      "        648 |   0.157120  |    0.074206     |   2\n",
      "        649 |   0.155164  |    0.009874     |   2\n",
      "        650 |   0.184465  |    0.083520     |   2\n",
      "        651 |   0.190515  |    0.009740     |   2\n",
      "        652 |   0.665900  |    0.092494     |   0\n",
      "        653 |   0.574355  |    0.149538     |   1\n",
      "        654 |   0.616429  |    0.059507     |   0\n",
      "        655 |   0.679473  |    0.153882     |   1\n",
      "        656 |   0.731945  |    0.198350     |   1\n",
      "        657 |   0.099509  |    0.099316     |   2\n",
      "        658 |   0.599201  |    0.252625     |   1\n",
      "        659 |   0.192138  |    0.013225     |   2\n",
      "        660 |   0.143362  |    0.085660     |   2\n",
      "        661 |   0.180485  |    0.035424     |   2\n",
      "        662 |   0.168070  |    0.075105     |   2\n",
      "        663 |   0.604868  |    0.041875     |   0\n",
      "        664 |   0.530504  |    0.046545     |   0\n",
      "        665 |   0.632198  |    0.054796     |   0\n",
      "        666 |   0.595295  |    0.081976     |   0\n",
      "        667 |   0.651034  |    0.137179     |   1\n",
      "        668 |   0.049593  |    0.044244     |   2\n",
      "        669 |   0.617001  |    0.056138     |   0\n",
      "        670 |   0.767340  |    0.195656     |   1\n",
      "        671 |   0.848114  |    0.042926     |   0\n",
      "        672 |   0.000974  |    0.082648     |   2\n",
      "        673 |   0.031359  |    0.016198     |   2\n",
      "        674 |   0.302559  |    0.090233     |   2\n",
      "        675 |   0.130174  |    0.017058     |   2\n",
      "        676 |   0.487424  |    0.252071     |   1\n",
      "        677 |   0.163930  |    0.018564     |   2\n",
      "        678 |   0.125221  |    0.043879     |   2\n",
      "        679 |   0.042737  |    0.085346     |   2\n",
      "        680 |   0.757280  |    0.033101     |   0\n",
      "        681 |   0.732397  |    0.196103     |   1\n",
      "        682 |   0.717712  |    0.138618     |   1\n",
      "        683 |   0.611847  |    0.051142     |   0\n",
      "        684 |   0.595205  |    0.201273     |   1\n",
      "        685 |   0.699810  |    0.006983     |   0\n",
      "        686 |   0.118056  |    0.074247     |   2\n",
      "        687 |   0.739440  |    0.221544     |   1\n",
      "        688 |   0.495592  |    0.180989     |   1\n",
      "        689 |   0.569164  |    0.021650     |   0\n",
      "        690 |   0.124417  |    0.068079     |   2\n",
      "        691 |   0.000988  |    0.029510     |   2\n",
      "        692 |   0.472967  |    0.166582     |   1\n",
      "        693 |   0.688716  |    0.048122     |   0\n",
      "        694 |   0.001016  |    0.042263     |   2\n",
      "        695 |   0.000946  |    0.037531     |   2\n",
      "        696 |   0.652402  |    0.080757     |   0\n",
      "        697 |   0.001021  |    0.010093     |   2\n",
      "        698 |   0.534880  |    0.197932     |   1\n",
      "        699 |   0.621262  |    0.004359     |   0\n",
      "        700 |   0.608745  |    0.102159     |   0\n",
      "        701 |   0.556226  |    0.149010     |   1\n",
      "        702 |   0.001117  |    0.046151     |   2\n",
      "        703 |   0.640963  |    0.088122     |   0\n",
      "        704 |   0.000994  |    0.026056     |   2\n",
      "        705 |   0.693772  |    0.148219     |   1\n",
      "        706 |   0.617449  |    0.011572     |   0\n",
      "        707 |   0.625672  |    0.208627     |   1\n",
      "        708 |   0.640807  |    0.011471     |   0\n",
      "        709 |   0.182954  |    0.080647     |   2\n",
      "        710 |   0.651011  |    0.155072     |   1\n",
      "        711 |   0.605425  |    0.220971     |   1\n",
      "        712 |   0.209445  |    0.019755     |   2\n",
      "        713 |   0.538625  |    0.185580     |   1\n",
      "        714 |   0.685342  |    0.098322     |   0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 715: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        715 |   0.561788  |    0.184319     |   1\n",
      "        716 |   0.630258  |    0.203774     |   1\n",
      "        717 |   0.161918  |    0.007739     |   2\n",
      "        718 |   0.618757  |    0.199510     |   1\n",
      "        719 |   0.675324  |    0.045372     |   0\n",
      "        720 |   0.624441  |    0.052823     |   0\n",
      "        721 |   0.572017  |    0.078414     |   0\n",
      "        722 |   0.507715  |    0.238856     |   1\n",
      "        723 |   0.735668  |    0.004999     |   0\n",
      "        724 |   0.531956  |    0.093581     |   0\n",
      "        725 |   0.506275  |    0.156507     |   1\n",
      "        726 |   0.567174  |    0.047058     |   0\n",
      "        727 |   0.580564  |    0.195971     |   1\n",
      "        728 |   0.572111  |    0.141011     |   1\n",
      "        729 |   0.533542  |    0.006475     |   0\n",
      "        730 |   0.153026  |    0.072999     |   2\n",
      "        731 |   0.179995  |    0.057587     |   2\n",
      "        732 |   0.743056  |    0.152445     |   1\n",
      "        733 |   0.685539  |    0.022811     |   0\n",
      "        734 |   0.190717  |    0.064165     |   2\n",
      "        735 |   0.686538  |    0.308382     |   1\n",
      "        736 |   0.455712  |    0.026687     |   0\n",
      "        737 |   0.099863  |    0.080488     |   2\n",
      "        738 |   0.189225  |    0.032521     |   2\n",
      "        739 |   0.144210  |    0.077539     |   2\n",
      "        740 |   0.666735  |    0.026604     |   0\n",
      "        741 |   0.669595  |    0.084017     |   0\n",
      "        742 |   0.651686  |    0.092804     |   1\n",
      "        743 |   0.618852  |    0.096805     |   0\n",
      "        744 |   0.657727  |    0.201594     |   1\n",
      "        745 |   0.595297  |    0.014434     |   0\n",
      "        746 |   0.480582  |    0.054231     |   0\n",
      "        747 |   0.525770  |    0.078482     |   0\n",
      "        748 |   0.827708  |    0.198771     |   1\n",
      "        749 |   0.172953  |    0.034907     |   2\n",
      "        750 |   0.616160  |    0.176026     |   1\n",
      "        751 |   0.575092  |    0.147747     |   1\n",
      "        752 |   0.710684  |    0.198227     |   1\n",
      "        753 |   0.160595  |    0.004899     |   2\n",
      "        754 |   0.647016  |    0.083928     |   0\n",
      "        755 |   0.519286  |    0.144838     |   1\n",
      "        756 |   0.731924  |    0.056458     |   0\n",
      "        757 |   0.684643  |    0.152711     |   1\n",
      "        758 |   0.570163  |    0.019629     |   0\n",
      "        759 |   0.619464  |    0.088965     |   0\n",
      "        760 |   0.055266  |    0.013239     |   2\n",
      "        761 |   0.622110  |    0.086863     |   0\n",
      "        762 |   0.001075  |    0.077976     |   2\n",
      "        763 |   0.590098  |    0.042647     |   0\n",
      "        764 |   0.588125  |    0.190753     |   1\n",
      "        765 |   0.027361  |    0.029302     |   2\n",
      "        766 |   0.566608  |    0.041255     |   0\n",
      "        767 |   0.301388  |    0.081676     |   2\n",
      "        768 |   0.470925  |    0.021844     |   0\n",
      "        769 |   0.123140  |    0.051927     |   2\n",
      "        770 |   0.711301  |    0.182936     |   1\n",
      "        771 |   0.168222  |    0.013529     |   2\n",
      "        772 |   0.585028  |    0.198310     |   1\n",
      "        773 |   0.562926  |    0.017603     |   0\n",
      "        774 |   0.539612  |    0.080982     |   0\n",
      "        775 |   0.562676  |    0.024897     |   0\n",
      "        776 |   0.549851  |    0.211917     |   1\n",
      "        777 |   0.130037  |    0.017783     |   2\n",
      "        778 |   0.046170  |    0.082505     |   2\n",
      "        779 |   0.585275  |    0.019121     |   0\n",
      "        780 |   0.544846  |    0.040136     |   0\n",
      "        781 |   0.132627  |    0.047623     |   2\n",
      "        782 |   0.762425  |    0.132564     |   1\n",
      "        783 |   0.125976  |    0.072784     |   2\n",
      "        784 |   0.001077  |    0.012002     |   2\n",
      "        785 |   0.001117  |    0.081766     |   2\n",
      "        786 |   0.000998  |    0.005694     |   2\n",
      "        787 |   0.001117  |    0.086215     |   2\n",
      "        788 |   0.551187  |    0.137906     |   1\n",
      "        789 |   0.001213  |    0.041959     |   2\n",
      "        790 |   0.001047  |    0.052324     |   2\n",
      "        791 |   0.175032  |    0.038028     |   2\n",
      "        792 |   0.586452  |    0.150873     |   1\n",
      "        793 |   0.210819  |    0.042875     |   2\n",
      "        794 |   0.387904  |    0.043709     |   0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 795: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        795 |   0.623937  |    0.081733     |   0\n",
      "        796 |   0.442293  |    0.052827     |   0\n",
      "        797 |   0.614707  |    0.198732     |   1\n",
      "        798 |   0.677347  |    0.143430     |   1\n",
      "        799 |   0.570984  |    0.075734     |   0\n",
      "        800 |   0.534331  |    0.023561     |   0\n",
      "        801 |   0.445073  |    0.075908     |   0\n",
      "        802 |   0.593836  |    0.046669     |   0\n",
      "        803 |   0.170032  |    0.041951     |   2\n",
      "        804 |   0.595138  |    0.072955     |   0\n",
      "        805 |   0.155199  |    0.005766     |   2\n",
      "        806 |   0.592017  |    0.079410     |   0\n",
      "        807 |   0.178773  |    0.029846     |   2\n",
      "        808 |   0.189794  |    0.063525     |   2\n",
      "        809 |   0.497116  |    0.199044     |   1\n",
      "        810 |   0.097207  |    0.008049     |   2\n",
      "        811 |   0.180514  |    0.086221     |   2\n",
      "        812 |   0.599732  |    0.153919     |   1\n",
      "        813 |   0.140706  |    0.034333     |   2\n",
      "        814 |   0.641338  |    0.188920     |   1\n",
      "        815 |   0.535620  |    0.163868     |   1\n",
      "        816 |   0.646156  |    0.137809     |   1\n",
      "        817 |   0.571995  |    0.029904     |   0\n",
      "        818 |   0.160851  |    0.073986     |   2\n",
      "        819 |   0.156143  |    0.008552     |   2\n",
      "        820 |   0.605173  |    0.082450     |   0\n",
      "        821 |   0.052473  |    0.049112     |   2\n",
      "        822 |   0.507914  |    0.236060     |   1\n",
      "        823 |   0.609257  |    0.056300     |   0\n",
      "        824 |   0.619078  |    0.191695     |   1\n",
      "        825 |   0.506852  |    0.164732     |   1\n",
      "        826 |   0.685218  |    0.097635     |   1\n",
      "        827 |   0.000919  |    0.034899     |   2\n",
      "        828 |   0.510563  |    0.049304     |   0\n",
      "        829 |   0.024295  |    0.045049     |   2\n",
      "        830 |   0.533458  |    0.049730     |   0\n",
      "        831 |   0.571849  |    0.049484     |   0\n",
      "        832 |   0.549739  |    0.046813     |   0\n",
      "        833 |   0.291057  |    0.058438     |   2\n",
      "        834 |   0.117768  |    0.019527     |   2\n",
      "        835 |   0.401310  |    0.058677     |   0\n",
      "        836 |   0.163059  |    0.043356     |   2\n",
      "        837 |   0.576242  |    0.051872     |   0\n",
      "        838 |   0.595629  |    0.047984     |   0\n",
      "        839 |   0.497005  |    0.194715     |   1\n",
      "        840 |   0.127094  |    0.012279     |   2\n",
      "        841 |   0.604765  |    0.078155     |   0\n",
      "        842 |   0.046640  |    0.018214     |   2\n",
      "        843 |   0.130686  |    0.080414     |   2\n",
      "        844 |   0.691322  |    0.131714     |   1\n",
      "        845 |   0.123681  |    0.075013     |   2\n",
      "        846 |   0.000958  |    0.008015     |   2\n",
      "        847 |   0.001032  |    0.081159     |   2\n",
      "        848 |   0.000911  |    0.028401     |   2\n",
      "        849 |   0.001013  |    0.047363     |   2\n",
      "        850 |   0.001102  |    0.034197     |   2\n",
      "        851 |   0.000945  |    0.047817     |   2\n",
      "        852 |   0.170772  |    0.041261     |   2\n",
      "        853 |   0.583770  |    0.039522     |   0\n",
      "        854 |   0.208734  |    0.041574     |   2\n",
      "        855 |   0.466456  |    0.051780     |   0\n",
      "        856 |   0.481698  |    0.044845     |   0\n",
      "        857 |   0.516207  |    0.031195     |   0\n",
      "        858 |   0.590848  |    0.075111     |   0\n",
      "        859 |   0.603463  |    0.021326     |   0\n",
      "        860 |   0.483352  |    0.048163     |   0\n",
      "        861 |   0.519525  |    0.044046     |   0\n",
      "        862 |   0.432219  |    0.180749     |   1\n",
      "        863 |   0.677517  |    0.138970     |   1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 864: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        864 |   0.518287  |    0.017321     |   0\n",
      "        865 |   0.524559  |    0.188201     |   1\n",
      "        866 |   0.400800  |    0.024461     |   0\n",
      "        867 |   0.171717  |    0.083025     |   2\n",
      "        868 |   0.473514  |    0.021041     |   0\n",
      "        869 |   0.150604  |    0.074672     |   2\n",
      "        870 |   0.170683  |    0.042500     |   2\n",
      "        871 |   0.568483  |    0.152712     |   1\n",
      "        872 |   0.451517  |    0.044629     |   0\n",
      "        873 |   0.360861  |    0.153084     |   1\n",
      "        874 |   0.600209  |    0.165498     |   1\n",
      "        875 |   0.566035  |    0.134667     |   1\n",
      "        876 |   0.186700  |    0.017423     |   2\n",
      "        877 |   0.094730  |    0.077886     |   2\n",
      "        878 |   0.174979  |    0.030444     |   2\n",
      "        879 |   0.139576  |    0.045536     |   2\n",
      "        880 |   0.468820  |    0.042941     |   0\n",
      "        881 |   0.156455  |    0.082456     |   2\n",
      "        882 |   0.150791  |    0.006709     |   2\n",
      "        883 |   0.052378  |    0.093267     |   2\n",
      "        884 |   0.450347  |    0.143608     |   1\n",
      "        885 |   0.000876  |    0.053004     |   2\n",
      "        886 |   0.449659  |    0.158069     |   1\n",
      "        887 |   0.022765  |    0.049080     |   2\n",
      "        888 |   0.278059  |    0.028900     |   2\n",
      "        889 |   0.471124  |    0.191985     |   1\n",
      "        890 |   0.522448  |    0.139796     |   1\n",
      "        891 |   0.511088  |    0.133790     |   1\n",
      "        892 |   0.481937  |    0.053674     |   0\n",
      "        893 |   0.114105  |    0.045913     |   2\n",
      "        894 |   0.155591  |    0.044064     |   2\n",
      "        895 |   0.470062  |    0.038281     |   0\n",
      "        896 |   0.469922  |    0.076424     |   0\n",
      "        897 |   0.653127  |    0.026698     |   0\n",
      "        898 |   0.117534  |    0.072647     |   2\n",
      "        899 |   0.480662  |    0.025189     |   0\n",
      "        900 |   0.484604  |    0.076306     |   0\n",
      "        901 |   0.046431  |    0.025230     |   2\n",
      "        902 |   0.541988  |    0.206814     |   1\n",
      "        903 |   0.123754  |    0.027456     |   2\n",
      "        904 |   0.645357  |    0.042509     |   0\n",
      "        905 |   0.536488  |    0.083259     |   0\n",
      "        906 |   0.470846  |    0.148229     |   1\n",
      "        907 |   0.474974  |    0.024610     |   0\n",
      "        908 |   0.577798  |    0.048730     |   0\n",
      "        909 |   0.520865  |    0.205791     |   1\n",
      "        910 |   0.476127  |    0.146348     |   1\n",
      "        911 |   0.453349  |    0.158312     |   1\n",
      "        912 |   0.484883  |    0.028207     |   0\n",
      "        913 |   0.597448  |    0.142238     |   1\n",
      "        914 |   0.525905  |    0.045587     |   0\n",
      "        915 |   0.120395  |    0.048358     |   2\n",
      "        916 |   0.509797  |    0.044070     |   0\n",
      "        917 |   0.000885  |    0.050634     |   2\n",
      "        918 |   0.490631  |    0.150739     |   1\n",
      "        919 |   0.000967  |    0.044169     |   2\n",
      "        920 |   0.590187  |    0.042407     |   0\n",
      "        921 |   0.000875  |    0.078250     |   2\n",
      "        922 |   0.664677  |    0.168745     |   1\n",
      "        923 |   0.000978  |    0.004569     |   2\n",
      "        924 |   0.001045  |    0.081524     |   2\n",
      "        925 |   0.452203  |    0.169717     |   1\n",
      "        926 |   0.000910  |    0.004981     |   2\n",
      "        927 |   0.335445  |    0.031949     |   0\n",
      "        928 |   0.165844  |    0.040185     |   2\n",
      "        929 |   0.498191  |    0.077957     |   0\n",
      "        930 |   0.521227  |    0.012769     |   0\n",
      "        931 |   0.211172  |    0.075052     |   2\n",
      "        932 |   0.606651  |    0.032702     |   0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 933: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        933 |   0.588465  |    0.138816     |   1\n",
      "        934 |   0.169647  |    0.046863     |   2\n",
      "        935 |   0.386906  |    0.200434     |   1\n",
      "        936 |   0.484577  |    0.018116     |   0\n",
      "        937 |   0.594704  |    0.078179     |   0\n",
      "        938 |   0.143933  |    0.030458     |   2\n",
      "        939 |   0.492140  |    0.221183     |   1\n",
      "        940 |   0.517774  |    0.160732     |   1\n",
      "        941 |   0.592972  |    0.144553     |   1\n",
      "        942 |   0.163561  |    0.008222     |   2\n",
      "        943 |   0.426736  |    0.083596     |   0\n",
      "        944 |   0.182297  |    0.028246     |   2\n",
      "        945 |   0.582861  |    0.194457     |   1\n",
      "        946 |   0.090046  |    0.008842     |   2\n",
      "        947 |   0.471592  |    0.076828     |   0\n",
      "        948 |   0.413632  |    0.034748     |   0\n",
      "        949 |   0.164508  |    0.072909     |   2\n",
      "        950 |   0.134608  |    0.017020     |   2\n",
      "        951 |   0.147267  |    0.088963     |   2\n",
      "        952 |   0.536994  |    0.157191     |   1\n",
      "        953 |   0.144363  |    0.010635     |   2\n",
      "        954 |   0.457275  |    0.090453     |   0\n",
      "        955 |   0.409009  |    0.135524     |   1\n",
      "        956 |   0.425121  |    0.049309     |   0\n",
      "        957 |   0.468004  |    0.045753     |   0\n",
      "        958 |   0.641491  |    0.144736     |   1\n",
      "        959 |   0.053447  |    0.022420     |   2\n",
      "        960 |   0.000820  |    0.045270     |   2\n",
      "        961 |   0.018836  |    0.073310     |   2\n",
      "        962 |   0.417362  |    0.007161     |   0\n",
      "        963 |   0.483571  |    0.078312     |   0\n",
      "        964 |   0.434429  |    0.032928     |   0\n",
      "        965 |   0.268627  |    0.032475     |   2\n",
      "        966 |   0.575384  |    0.081993     |   0\n",
      "        967 |   0.112603  |    0.030859     |   2\n",
      "        968 |   0.415384  |    0.139736     |   1\n",
      "        969 |   0.359055  |    0.049509     |   0\n",
      "        970 |   0.491304  |    0.046327     |   0\n",
      "        971 |   0.151846  |    0.070929     |   2\n",
      "        972 |   0.636182  |    0.193005     |   1\n",
      "        973 |   0.114865  |    0.012840     |   2\n",
      "        974 |   0.640395  |    0.158845     |   1\n",
      "        975 |   0.496675  |    0.040492     |   0\n",
      "        976 |   0.046035  |    0.058037     |   2\n",
      "        977 |   0.123801  |    0.044629     |   2\n",
      "        978 |   0.465242  |    0.040681     |   0\n",
      "        979 |   0.492247  |    0.084268     |   0\n",
      "        980 |   0.117388  |    0.004404     |   2\n",
      "        981 |   0.000826  |    0.082129     |   2\n",
      "        982 |   0.566663  |    0.156682     |   1\n",
      "        983 |   0.505361  |    0.034201     |   0\n",
      "        984 |   0.554556  |    0.158928     |   1\n",
      "        985 |   0.548050  |    0.163772     |   1\n",
      "        986 |   0.519171  |    0.199338     |   1\n",
      "        987 |   0.392211  |    0.003967     |   0\n",
      "        988 |   0.000874  |    0.083050     |   2\n",
      "        989 |   0.454995  |    0.022939     |   0\n",
      "        990 |   0.427832  |    0.075316     |   0\n",
      "        991 |   0.477030  |    0.046207     |   0\n",
      "        992 |   0.541629  |    0.157226     |   1\n",
      "        993 |   0.463385  |    0.017336     |   0\n",
      "        994 |   0.535131  |    0.207157     |   1\n",
      "        995 |   0.465566  |    0.018543     |   0\n",
      "        996 |   0.000813  |    0.082685     |   2\n",
      "        997 |   0.000910  |    0.016991     |   2\n",
      "        998 |   0.513402  |    0.079438     |   0\n",
      "        999 |   0.379769  |    0.021410     |   0\n",
      "       1000 |   0.001024  |    0.080901     |   2\n",
      "       1001 |   0.171639  |    0.052030     |   2\n",
      "       1002 |   0.138976  |    0.081699     |   2\n",
      "       1003 |   0.484494  |    0.126484     |   1\n",
      "       1004 |   0.491631  |    0.020212     |   0\n",
      "       1005 |   0.163174  |    0.065270     |   2\n",
      "       1006 |   0.545978  |    0.073792     |   0\n",
      "       1007 |   0.419815  |    0.024866     |   0\n",
      "       1008 |   0.481067  |    0.048806     |   0\n",
      "       1009 |   0.540653  |    0.149357     |   1\n",
      "       1010 |   0.595765  |    0.153664     |   1\n",
      "       1011 |   0.179208  |    0.072376     |   2\n",
      "       1012 |   0.608031  |    0.195264     |   1\n",
      "       1013 |   0.086901  |    0.005298     |   2\n",
      "       1014 |   0.521672  |    0.096374     |   0\n",
      "       1015 |   0.500821  |    0.031126     |   0\n",
      "       1016 |   0.528513  |    0.207624     |   1\n",
      "       1017 |   0.521841  |    0.140441     |   1\n",
      "       1018 |   0.537677  |    0.008644     |   0\n",
      "       1019 |   0.489054  |    0.051618     |   0\n",
      "       1020 |   0.480286  |    0.215047     |   1\n",
      "       1021 |   0.159769  |    0.007561     |   2\n",
      "       1022 |   0.506708  |    0.084412     |   0\n",
      "       1023 |   0.489199  |    0.166670     |   1\n",
      "       1024 |   0.492760  |    0.004977     |   0\n",
      "       1025 |   0.134309  |    0.047550     |   2\n",
      "       1026 |   0.146313  |    0.051614     |   2\n",
      "       1027 |   0.460981  |    0.053740     |   0\n",
      "       1028 |   0.350415  |    0.051516     |   0\n",
      "       1029 |   0.439986  |    0.041752     |   0\n",
      "       1030 |   0.518267  |    0.042963     |   0\n",
      "       1031 |   0.137092  |    0.083200     |   2\n",
      "       1032 |   0.510762  |    0.159858     |   1\n",
      "       1033 |   0.594944  |    0.151861     |   1\n",
      "       1034 |   0.505826  |    0.139212     |   1"
     ]
    }
   ],
   "source": [
    "trainer.fit(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
