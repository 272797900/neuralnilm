{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from neuralnilm.data.loadactivations import load_nilmtk_activations\n",
    "from neuralnilm.data.syntheticaggregatesource import SyntheticAggregateSource\n",
    "from neuralnilm.data.datapipeline import DataPipeline\n",
    "from neuralnilm.data.processing import DivideBy\n",
    "from neuralnilm.data.datathread import DataThread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NILMTK_FILENAME = '/data/mine/vadeec/merged/ukdale.h5'\n",
    "TARGET_APPLIANCE = 'kettle'\n",
    "SEQ_LENGTH = 256\n",
    "SAMPLE_PERIOD = 6\n",
    "STRIDE = 4\n",
    "WINDOWS = {\n",
    "    'train': {\n",
    "        1: (\"2014-01-01\", \"2014-02-01\")\n",
    "    },\n",
    "    'unseen_activations_of_seen_appliances': {\n",
    "        1: (\"2014-02-02\", \"2014-02-08\")                \n",
    "    },\n",
    "    'unseen_appliances': {\n",
    "        2: (\"2013-06-01\", \"2013-06-07\")\n",
    "    }\n",
    "}\n",
    "\n",
    "LOADER_CONFIG = {\n",
    "    'nilmtk_activations': dict(\n",
    "        appliances=['kettle', 'microwave', 'washing machine'],\n",
    "        filename=NILMTK_FILENAME,\n",
    "        sample_period=SAMPLE_PERIOD,\n",
    "        windows=WINDOWS\n",
    "    )\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.data.loadactivations:Loading NILMTK activations...\n",
      "INFO:neuralnilm.data.loadactivations:Loading kettle for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 111 kettle activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Loading microwave for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 119 microwave activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Loading washing machine for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 23 washing machine activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Loading kettle for UK-DALE_building_2...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 31 kettle activations from UK-DALE_building_2.\n",
      "INFO:neuralnilm.data.loadactivations:Loading microwave for UK-DALE_building_2...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 30 microwave activations from UK-DALE_building_2.\n",
      "INFO:neuralnilm.data.loadactivations:Loading washing machine for UK-DALE_building_2...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 2 washing machine activations from UK-DALE_building_2.\n",
      "INFO:neuralnilm.data.loadactivations:Loading kettle for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 28 kettle activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Loading microwave for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 28 microwave activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Loading washing machine for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 5 washing machine activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Done loading NILMTK activations.\n"
     ]
    }
   ],
   "source": [
    "nilmtk_activations = load_nilmtk_activations(**LOADER_CONFIG['nilmtk_activations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'unseen_activations_of_seen_appliances', 'unseen_appliances']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nilmtk_activations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuralnilm.data.realaggregatesource import RealAggregateSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.data.realaggregatesource:Loading NILMTK mains...\n",
      "INFO:neuralnilm.data.realaggregatesource:Loading mains for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.realaggregatesource:Loaded mains data from building UK-DALE_building_1 for fold train from 2014-01-01 00:00:00+00:00 to 2014-01-31 23:59:54+00:00.\n",
      "INFO:neuralnilm.data.realaggregatesource:Loading mains for UK-DALE_building_2...\n",
      "INFO:neuralnilm.data.realaggregatesource:Loaded mains data from building UK-DALE_building_2 for fold unseen_appliances from 2013-06-01 00:00:00+01:00 to 2013-06-06 23:59:54+01:00.\n",
      "INFO:neuralnilm.data.realaggregatesource:Loading mains for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.realaggregatesource:Loaded mains data from building UK-DALE_building_1 for fold unseen_activations_of_seen_appliances from 2014-02-02 00:00:00+00:00 to 2014-02-07 23:59:54+00:00.\n",
      "INFO:neuralnilm.data.realaggregatesource:Done loading NILMTK mains data.\n",
      "INFO:neuralnilm.data.realaggregatesource:Found 89 sections without target for train UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.realaggregatesource:Found 20 sections without target for unseen_activations_of_seen_appliances UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.realaggregatesource:Found 24 sections without target for unseen_appliances UK-DALE_building_2.\n"
     ]
    }
   ],
   "source": [
    "ras = RealAggregateSource(\n",
    "    activations=nilmtk_activations,\n",
    "    target_appliance=TARGET_APPLIANCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    filename=NILMTK_FILENAME,\n",
    "    windows=WINDOWS,\n",
    "    sample_period=SAMPLE_PERIOD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ras.target_inclusion_prob = 0.5\n",
    "for i in range(50):\n",
    "    seq = ras.get_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2)\n",
    "axes[0].plot(seq.input)\n",
    "axes[1].plot(seq.target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'building', 'appliance', 'fold']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nilmtk_activations['train']['kettle']['UK-DALE_building_1'][0]._metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source = SyntheticAggregateSource(\n",
    "    activations=nilmtk_activations,\n",
    "    target_appliance=TARGET_APPLIANCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    allow_incomplete_target=False,\n",
    "    sample_period=SAMPLE_PERIOD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FOLD = 'train'\n",
    "#FOLD = 'unseen_activations_of_seen_appliances'\n",
    "#FOLD = 'unseen_appliances'\n",
    "seq = source.get_sequence(enable_all_appliances=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all(seq.all_appliances.sum(axis=1) == seq.input[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(2, sharex=True)\n",
    "#seq.all_appliances.plot(ax=axes[0])\n",
    "#axes[1].plot(seq.input)\n",
    "#fig.suptitle(FOLD)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = source.get_batch(num_seq_per_batch=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = DataPipeline(\n",
    "    [source],\n",
    "    num_seq_per_batch=64,\n",
    "    input_processing=[DivideBy(sample.before_processing.input.flatten().std())],\n",
    "    target_processing=[DivideBy(sample.before_processing.target.flatten().std())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "nilmtk_disag_source = NILMTKDisagSource(\n",
    "    filename=NILMTK_FILENAME,\n",
    "    target_appliance=TARGET_APPLIANCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    buildings=[5],\n",
    "    window_per_building={},\n",
    "    stride=STRIDE,\n",
    "    sample_period=SAMPLE_PERIOD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "disag_pipeline = deepcopy(pipeline)\n",
    "disag_pipeline.source = nilmtk_disag_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "disaggregator = Disaggregator(\n",
    "    pipeline=disag_pipeline,\n",
    "    output_path=PATH  # \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disagregator ideas:\n",
    "\n",
    "* make a copy of pipeline but swap source for a NILMTKDisagSource\n",
    "* NILMTKDisagSource loads all data into memory (?) and iterates over chunks of it (get seq_length from pipeline.source.seq_length)\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DataPipeline': {'SyntheticAggregateSource': {'allow_incomplete_distractors': True,\n",
       "   'allow_incomplete_target': False,\n",
       "   'distractor_inclusion_prob': 1.0,\n",
       "   'include_incomplete_target_in_output': True,\n",
       "   'rng_seed': None,\n",
       "   'sample_period': 6,\n",
       "   'seq_length': 256,\n",
       "   'target_appliance': 'kettle',\n",
       "   'target_inclusion_prob': 1.0,\n",
       "   'uniform_prob_of_selecting_each_building': True},\n",
       "  'input_processing': [{'DivideBy': {'divisor': 992.901}}],\n",
       "  'num_seq_per_batch': 64,\n",
       "  'target_processing': [{'DivideBy': {'divisor': 603.31958}}]},\n",
       " 'activations': {'nilmtk_activations': {'appliances': ['kettle',\n",
       "    'microwave',\n",
       "    'washing machine'],\n",
       "   'filename': '/data/mine/vadeec/merged/ukdale.h5',\n",
       "   'sample_period': 6,\n",
       "   'windows': {'train': {1: ('2014-01-01', '2014-02-01')},\n",
       "    'unseen_activations_of_seen_appliances': {1: ('2014-02-02', '2014-02-08')},\n",
       "    'unseen_appliances': {2: ('2013-06-01', '2013-06-07')}}}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pipeline.report()\n",
    "report['activations'] = LOADER_CONFIG\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer, RecurrentLayer, DenseLayer, ReshapeLayer\n",
    "\n",
    "def get_net_0(input_shape, target_shape=None):\n",
    "    NUM_UNITS = {\n",
    "        'dense_layer_0': 100,\n",
    "        'dense_layer_1':  50,\n",
    "        'dense_layer_2': 100\n",
    "    }\n",
    "\n",
    "    if target_shape is None:\n",
    "        target_shape = input_shape\n",
    "    \n",
    "    # Define layers\n",
    "    input_layer = InputLayer(\n",
    "        shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Dense layers\n",
    "    dense_layer_0 = DenseLayer(\n",
    "        input_layer, \n",
    "        num_units=NUM_UNITS['dense_layer_0']\n",
    "    )\n",
    "    dense_layer_1 = DenseLayer(\n",
    "        dense_layer_0, \n",
    "        num_units=NUM_UNITS['dense_layer_1']\n",
    "    )\n",
    "    dense_layer_2 = DenseLayer(\n",
    "        dense_layer_1, \n",
    "        num_units=NUM_UNITS['dense_layer_2']\n",
    "    )\n",
    "    \n",
    "    # Output\n",
    "    final_dense_layer = DenseLayer(\n",
    "        dense_layer_2,\n",
    "        num_units=target_shape[1] * target_shape[2],\n",
    "        nonlinearity=None\n",
    "    )\n",
    "    output_layer = ReshapeLayer(\n",
    "        final_dense_layer,\n",
    "        shape=target_shape\n",
    "    )\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuralnilm.net import Net\n",
    "\n",
    "batch = pipeline.get_batch()\n",
    "output_layer = get_net_0(\n",
    "    batch.after_processing.input.shape, \n",
    "    batch.after_processing.target.shape\n",
    ")\n",
    "net = Net(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "client.drop_database('neuralnilm_experiments')\n",
    "db = client.neuralnilm_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neuralnilm.trainer import Trainer\n",
    "from neuralnilm.metrics import Metrics\n",
    "\n",
    "trainer = Trainer(\n",
    "    net=net,\n",
    "    data_pipeline=pipeline,\n",
    "    experiment_id=[\"0\"],\n",
    "    metrics=Metrics(state_boundaries=[4]),\n",
    "    learning_rates={0: 1E-2},\n",
    "    repeat_callbacks=[\n",
    "        ( 100, Trainer.validate)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Starting training for 400 iterations.\n",
      "INFO:neuralnilm.trainer:Iteration 0: Change learning rate to 1.0E-02\n",
      "INFO:neuralnilm.trainer:Compiling train cost function...\n",
      "INFO:neuralnilm.trainer:Done compiling cost function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Update # |  Train cost  | Secs per update | Source ID\n",
      "------------|--------------|-----------------|-----------\n",
      "          0 | \u001b[94m  1.253457\u001b[0m  |   38.008859     |   0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.net:Compiling deterministic output function...\n",
      "INFO:neuralnilm.net:Done compiling deterministic output function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          1 | \u001b[94m  1.150133\u001b[0m  |    0.003681     |   0\n",
      "          2 |   1.250338  |    0.006174     |   0\n",
      "          3 |   1.205524  |    0.005648     |   0\n",
      "          4 | \u001b[94m  1.146284\u001b[0m  |    0.004330     |   0\n",
      "          5 |   1.232201  |    0.121888     |   0\n",
      "          6 |   1.175494  |    0.084214     |   0\n",
      "          7 | \u001b[94m  1.083879\u001b[0m  |    0.128562     |   0\n",
      "          8 |   1.167884  |    0.076053     |   0\n",
      "          9 |   1.128397  |    0.125010     |   0\n",
      "         10 | \u001b[94m  1.046033\u001b[0m  |    0.087040     |   0\n",
      "         11 |   1.104134  |    0.126984     |   0\n",
      "         12 |   1.148001  |    0.075073     |   0\n",
      "         13 |   1.093560  |    0.126743     |   0\n",
      "         14 |   1.121703  |    0.078304     |   0\n",
      "         15 |   1.094916  |    0.141745     |   0\n",
      "         16 |   1.147387  |    0.072106     |   0\n",
      "         17 | \u001b[94m  0.972002\u001b[0m  |    0.094218     |   0\n",
      "         18 |   1.070333  |    0.120608     |   0\n",
      "         19 |   1.009183  |    0.131283     |   0\n",
      "         20 |   1.033266  |    0.121845     |   0\n",
      "         21 |   1.138631  |    0.132819     |   0\n",
      "         22 |   1.029136  |    0.127831     |   0\n",
      "         23 |   1.109616  |    0.079447     |   0\n",
      "         24 |   1.117578  |    0.085718     |   0\n",
      "         25 |   1.006578  |    0.129858     |   0\n",
      "         26 |   1.155775  |    0.080536     |   0\n",
      "         27 |   1.087489  |    0.095432     |   0\n",
      "         28 |   0.992836  |    0.124851     |   0\n",
      "         29 | \u001b[94m  0.966191\u001b[0m  |    0.084840     |   0\n",
      "         30 |   1.092348  |    0.129155     |   0\n",
      "         31 |   1.008493  |    0.071239     |   0\n",
      "         32 |   1.068784  |    0.139445     |   0\n",
      "         33 | \u001b[94m  0.962119\u001b[0m  |    0.130485     |   0\n",
      "         34 |   1.100922  |    0.089170     |   0\n",
      "         35 |   0.982735  |    0.130132     |   0\n",
      "         36 |   1.078025  |    0.124163     |   0\n",
      "         37 |   1.053091  |    0.132569     |   0\n",
      "         38 |   1.084365  |    0.075174     |   0\n",
      "         39 |   1.088122  |    0.127140     |   0\n",
      "         40 |   1.029968  |    0.134524     |   0\n",
      "         41 |   1.153792  |    0.125700     |   0\n",
      "         42 |   0.979380  |    0.122928     |   0\n",
      "         43 |   1.004935  |    0.120261     |   0\n",
      "         44 |   1.079721  |    0.123949     |   0\n",
      "         45 |   1.033198  |    0.073103     |   0\n",
      "         46 |   1.083445  |    0.075931     |   0\n",
      "         47 |   1.088064  |    0.086218     |   0\n",
      "         48 |   1.057618  |    0.126794     |   0\n",
      "         49 |   1.029912  |    0.126795     |   0\n",
      "         50 |   1.053287  |    0.078589     |   0\n",
      "         51 |   1.019565  |    0.172778     |   0\n",
      "         52 |   1.065901  |    0.080575     |   0\n",
      "         53 |   1.080885  |    0.125027     |   0\n",
      "         54 |   1.052096  |    0.130014     |   0\n",
      "         55 |   1.052322  |    0.083159     |   0\n",
      "         56 |   1.028853  |    0.134734     |   0\n",
      "         57 |   1.109664  |    0.137178     |   0\n",
      "         58 |   1.039461  |    0.081786     |   0\n",
      "         59 |   1.040863  |    0.121852     |   0\n",
      "         60 |   1.027166  |    0.078868     |   0\n",
      "         61 |   1.115787  |    0.134678     |   0\n",
      "         62 | \u001b[94m  0.957900\u001b[0m  |    0.080948     |   0\n",
      "         63 |   1.116487  |    0.086800     |   0\n",
      "         64 |   1.044327  |    0.146804     |   0\n",
      "         65 |   1.134924  |    0.075068     |   0\n",
      "         66 |   1.007620  |    0.128595     |   0\n",
      "         67 | \u001b[94m  0.895090\u001b[0m  |    0.125868     |   0\n",
      "         68 |   1.038532  |    0.076860     |   0\n",
      "         69 |   1.055793  |    0.125939     |   0\n",
      "         70 |   1.062770  |    0.076557     |   0\n",
      "         71 |   1.067810  |    0.074840     |   0\n",
      "         72 |   1.137176  |    0.132433     |   0\n",
      "         73 |   1.078734  |    0.073173     |   0\n",
      "         74 |   0.959692  |    0.132607     |   0\n",
      "         75 |   1.011561  |    0.073153     |   0\n",
      "         76 |   0.991717  |    0.078053     |   0\n",
      "         77 |   1.085092  |    0.133743     |   0\n",
      "         78 |   0.949140  |    0.086653     |   0\n",
      "         79 | \u001b[94m  0.858881\u001b[0m  |    0.124035     |   0\n",
      "         80 |   1.025381  |    0.079521     |   0\n",
      "         81 |   0.974864  |    0.128080     |   0\n",
      "         82 |   0.994511  |    0.135965     |   0\n",
      "         83 |   1.030685  |    0.131934     |   0\n",
      "         84 |   1.085975  |    0.075915     |   0\n",
      "         85 |   1.021700  |    0.133605     |   0\n",
      "         86 |   0.983512  |    0.088577     |   0\n",
      "         87 |   1.036906  |    0.126827     |   0\n",
      "         88 |   1.092520  |    0.127812     |   0\n",
      "         89 |   0.921858  |    0.079704     |   0\n",
      "         90 |   1.021408  |    0.121102     |   0\n",
      "         91 |   0.931075  |    0.077259     |   0\n",
      "         92 |   1.113618  |    0.128785     |   0\n",
      "         93 |   0.998809  |    0.122215     |   0\n",
      "         94 |   0.971766  |    0.130424     |   0\n",
      "         95 |   1.022231  |    0.077171     |   0\n",
      "         96 |   1.028787  |    0.071342     |   0\n",
      "         97 |   0.979370  |    0.132272     |   0\n",
      "         98 |   1.044980  |    0.070282     |   0\n",
      "         99 |   0.975150  |    0.136605     |   0\n",
      "        100 |   1.052556  |    0.075230     |   0\n",
      "        101 |   1.055170  |    0.002928     |   0\n",
      "        102 |   1.114197  |    0.003057     |   0\n",
      "        103 |   1.008130  |    0.006938     |   0\n",
      "        104 |   1.138386  |    0.006717     |   0\n",
      "        105 |   1.161680  |    0.128445     |   0\n",
      "        106 |   1.098231  |    0.128914     |   0\n",
      "        107 |   1.067711  |    0.084539     |   0\n",
      "        108 |   1.043960  |    0.122755     |   0\n",
      "        109 |   0.979120  |    0.090147     |   0\n",
      "        110 |   1.154775  |    0.074558     |   0\n",
      "        111 |   1.079449  |    0.125243     |   0\n",
      "        112 |   0.979971  |    0.073399     |   0\n",
      "        113 |   0.973513  |    0.120027     |   0\n",
      "        114 |   1.031898  |    0.071973     |   0\n",
      "        115 |   0.898675  |    0.082017     |   0\n",
      "        116 |   1.023629  |    0.128308     |   0\n",
      "        117 |   0.932515  |    0.072581     |   0\n",
      "        118 |   1.045110  |    0.070670     |   0\n",
      "        119 |   0.946821  |    0.131278     |   0\n",
      "        120 |   1.048464  |    0.120145     |   0\n",
      "        121 |   1.041931  |    0.072430     |   0\n",
      "        122 |   0.955862  |    0.074221     |   0\n",
      "        123 |   1.038387  |    0.132923     |   0\n",
      "        124 |   1.022976  |    0.076512     |   0\n",
      "        125 |   1.002391  |    0.128526     |   0\n",
      "        126 |   1.022658  |    0.077464     |   0\n",
      "        127 |   1.014506  |    0.140129     |   0\n",
      "        128 |   1.007364  |    0.189855     |   0\n",
      "        129 |   1.008042  |    0.076114     |   0\n",
      "        130 |   1.039055  |    0.130438     |   0\n",
      "        131 |   0.936782  |    0.087200     |   0\n",
      "        132 |   1.053628  |    0.130629     |   0\n",
      "        133 |   1.065090  |    0.075201     |   0\n",
      "        134 |   0.949269  |    0.124627     |   0\n",
      "        135 |   1.025402  |    0.125414     |   0\n",
      "        136 |   0.997367  |    0.083001     |   0\n",
      "        137 |   0.991045  |    0.070719     |   0\n",
      "        138 |   1.033118  |    0.128063     |   0\n",
      "        139 |   0.957018  |    0.074360     |   0\n",
      "        140 |   0.911712  |    0.078452     |   0\n",
      "        141 |   0.988720  |    0.128018     |   0\n",
      "        142 |   1.079024  |    0.080857     |   0\n",
      "        143 |   0.955257  |    0.126570     |   0\n",
      "        144 |   0.970569  |    0.076521     |   0\n",
      "        145 |   1.023255  |    0.076049     |   0\n",
      "        146 |   1.034828  |    0.122062     |   0\n",
      "        147 |   0.953897  |    0.077143     |   0\n",
      "        148 |   1.002711  |    0.127176     |   0\n",
      "        149 |   0.913893  |    0.186091     |   0\n",
      "        150 |   0.979854  |    0.170632     |   0\n",
      "        151 |   0.986399  |    0.132854     |   0\n",
      "        152 |   0.946318  |    0.130983     |   0\n",
      "        153 |   0.935069  |    0.073405     |   0\n",
      "        154 |   1.041319  |    0.124891     |   0\n",
      "        155 |   1.005640  |    0.131918     |   0\n",
      "        156 |   0.994800  |    0.179899     |   0\n",
      "        157 |   1.015216  |    0.074432     |   0\n",
      "        158 |   1.038008  |    0.180514     |   0\n",
      "        159 |   1.074150  |    0.175919     |   0\n",
      "        160 |   1.030240  |    0.129848     |   0\n",
      "        161 |   0.966557  |    0.127633     |   0\n",
      "        162 |   0.983819  |    0.080757     |   0\n",
      "        163 |   1.011053  |    0.126569     |   0\n",
      "        164 |   1.017378  |    0.134027     |   0\n",
      "        165 |   1.022991  |    0.091421     |   0\n",
      "        166 |   0.954849  |    0.073697     |   0\n",
      "        167 |   1.016645  |    0.122192     |   0\n",
      "        168 |   0.977200  |    0.123165     |   0\n",
      "        169 |   1.013220  |    0.080062     |   0\n",
      "        170 |   0.931739  |    0.125426     |   0\n",
      "        171 |   1.046294  |    0.199740     |   0\n",
      "        172 |   1.012203  |    0.084738     |   0\n",
      "        173 |   1.011499  |    0.185101     |   0\n",
      "        174 |   1.001744  |    0.141255     |   0\n",
      "        175 |   0.993337  |    0.129127     |   0\n",
      "        176 |   0.992507  |    0.078804     |   0\n",
      "        177 |   0.992515  |    0.077981     |   0\n",
      "        178 |   0.975138  |    0.126401     |   0\n",
      "        179 |   0.960338  |    0.083517     |   0\n",
      "        180 |   0.986601  |    0.122721     |   0\n",
      "        181 |   0.985034  |    0.127971     |   0\n",
      "        182 |   0.871634  |    0.081899     |   0\n",
      "        183 |   0.942186  |    0.082174     |   0\n",
      "        184 |   0.954226  |    0.131636     |   0\n",
      "        185 |   0.975790  |    0.127094     |   0\n",
      "        186 |   0.924153  |    0.131915     |   0\n",
      "        187 |   0.952708  |    0.080524     |   0\n",
      "        188 |   0.922477  |    0.124395     |   0\n",
      "        189 |   0.940608  |    0.079962     |   0\n",
      "        190 |   1.038128  |    0.130758     |   0\n",
      "        191 |   0.989008  |    0.076120     |   0\n",
      "        192 |   0.958674  |    0.123495     |   0\n",
      "        193 |   0.974397  |    0.141741     |   0\n",
      "        194 |   0.937802  |    0.072834     |   0\n",
      "        195 |   1.035563  |    0.133485     |   0\n",
      "        196 |   0.957180  |    0.118865     |   0\n",
      "        197 |   0.999698  |    0.124010     |   0\n",
      "        198 |   0.977073  |    0.083763     |   0\n",
      "        199 |   0.948639  |    0.046367     |   0\n",
      "        200 |   0.973161  |    0.129888     |   0\n",
      "        201 |   0.970177  |    0.003154     |   0\n",
      "        202 |   0.956962  |    0.004399     |   0\n",
      "        203 |   0.999116  |    0.006458     |   0\n",
      "        204 |   0.976993  |    0.003227     |   0\n",
      "        205 |   0.929483  |    0.122460     |   0\n",
      "        206 |   1.033829  |    0.127423     |   0\n",
      "        207 |   0.965572  |    0.088499     |   0\n",
      "        208 |   0.955798  |    0.130481     |   0\n",
      "        209 |   0.924117  |    0.139957     |   0\n",
      "        210 |   0.906585  |    0.178675     |   0\n",
      "        211 |   0.911684  |    0.078662     |   0\n",
      "        212 |   0.975398  |    0.123265     |   0\n",
      "        213 |   0.925442  |    0.126035     |   0\n",
      "        214 |   1.007715  |    0.083111     |   0\n",
      "        215 |   0.991327  |    0.129734     |   0\n",
      "        216 |   0.916324  |    0.129539     |   0\n",
      "        217 |   0.900962  |    0.078156     |   0\n",
      "        218 |   0.925425  |    0.130358     |   0\n",
      "        219 |   0.972095  |    0.184053     |   0\n",
      "        220 |   1.030695  |    0.077433     |   0\n",
      "        221 |   0.997911  |    0.131248     |   0\n",
      "        222 |   0.973015  |    0.072794     |   0\n",
      "        223 |   0.975742  |    0.122442     |   0\n",
      "        224 |   0.981516  |    0.126100     |   0\n",
      "        225 |   0.963812  |    0.071983     |   0\n",
      "        226 |   0.949284  |    0.127465     |   0\n",
      "        227 |   0.879276  |    0.078911     |   0\n",
      "        228 |   1.002341  |    0.129453     |   0\n",
      "        229 |   1.059376  |    0.135960     |   0\n",
      "        230 |   0.973802  |    0.121077     |   0\n",
      "        231 |   1.051641  |    0.129470     |   0\n",
      "        232 |   1.010451  |    0.073072     |   0\n",
      "        233 |   1.003942  |    0.122042     |   0\n",
      "        234 |   0.982050  |    0.077830     |   0\n",
      "        235 |   0.886540  |    0.125495     |   0\n",
      "        236 |   0.930494  |    0.070164     |   0\n",
      "        237 |   0.862982  |    0.132800     |   0\n",
      "        238 |   0.953692  |    0.122884     |   0\n",
      "        239 |   0.961281  |    0.085355     |   0\n",
      "        240 |   0.962162  |    0.138074     |   0\n",
      "        241 |   0.937969  |    0.132207     |   0\n",
      "        242 |   0.951694  |    0.078664     |   0\n",
      "        243 |   0.902844  |    0.125067     |   0\n",
      "        244 |   0.948215  |    0.126434     |   0\n",
      "        245 |   0.955002  |    0.072426     |   0\n",
      "        246 |   0.981106  |    0.094420     |   0\n",
      "        247 |   0.923962  |    0.130169     |   0\n",
      "        248 |   0.872630  |    0.127741     |   0\n",
      "        249 |   0.935070  |    0.137620     |   0\n",
      "        250 |   0.905295  |    0.068618     |   0\n",
      "        251 |   0.955225  |    0.123926     |   0\n",
      "        252 |   0.956015  |    0.101166     |   0\n",
      "        253 |   0.895627  |    0.127634     |   0\n",
      "        254 |   0.985693  |    0.128902     |   0\n",
      "        255 |   0.922321  |    0.085482     |   0\n",
      "        256 |   0.909929  |    0.070037     |   0\n",
      "        257 |   0.938421  |    0.073082     |   0\n",
      "        258 |   0.927049  |    0.125897     |   0\n",
      "        259 | \u001b[94m  0.829354\u001b[0m  |    0.134986     |   0\n",
      "        260 |   0.945196  |    0.125396     |   0\n",
      "        261 |   0.990062  |    0.133642     |   0\n",
      "        262 |   0.929287  |    0.123415     |   0\n",
      "        263 |   0.915439  |    0.078529     |   0\n",
      "        264 |   0.943772  |    0.124841     |   0\n",
      "        265 |   0.973920  |    0.075723     |   0\n",
      "        266 |   0.927077  |    0.184120     |   0\n",
      "        267 |   0.925268  |    0.126194     |   0\n",
      "        268 |   0.982618  |    0.128598     |   0\n",
      "        269 |   0.926608  |    0.077292     |   0\n",
      "        270 |   0.900411  |    0.120143     |   0\n",
      "        271 |   0.958216  |    0.122747     |   0\n",
      "        272 |   0.904503  |    0.121734     |   0\n",
      "        273 |   0.999131  |    0.083524     |   0\n",
      "        274 |   0.992580  |    0.131466     |   0\n",
      "        275 |   0.934003  |    0.129016     |   0\n",
      "        276 |   0.925701  |    0.129390     |   0\n",
      "        277 |   0.918935  |    0.083124     |   0\n",
      "        278 |   0.960660  |    0.121636     |   0\n",
      "        279 |   0.919502  |    0.070117     |   0\n",
      "        280 |   0.898601  |    0.125191     |   0\n",
      "        281 |   0.959114  |    0.074455     |   0\n",
      "        282 |   0.851783  |    0.130891     |   0\n",
      "        283 |   0.891851  |    0.085265     |   0\n",
      "        284 |   0.858230  |    0.075178     |   0\n",
      "        285 |   0.927916  |    0.122412     |   0\n",
      "        286 |   0.881238  |    0.075922     |   0\n",
      "        287 | \u001b[94m  0.804510\u001b[0m  |    0.128115     |   0\n",
      "        288 |   0.981251  |    0.075917     |   0\n",
      "        289 |   0.865581  |    0.134591     |   0\n",
      "        290 |   0.920139  |    0.125945     |   0\n",
      "        291 |   0.871052  |    0.073041     |   0\n",
      "        292 |   0.934670  |    0.124142     |   0\n",
      "        293 |   0.922177  |    0.074488     |   0\n",
      "        294 |   0.945532  |    0.129786     |   0\n",
      "        295 |   0.858360  |    0.120059     |   0\n",
      "        296 |   0.937649  |    0.073144     |   0\n",
      "        297 |   0.955210  |    0.131570     |   0\n",
      "        298 |   0.883751  |    0.071754     |   0\n",
      "        299 |   0.967648  |    0.120957     |   0\n",
      "        300 |   0.851768  |    0.124096     |   0\n",
      "        301 |   0.910957  |    0.002923     |   0\n",
      "        302 |   0.941706  |    0.003005     |   0\n",
      "        303 |   0.880281  |    0.004712     |   0\n",
      "        304 |   0.941766  |    0.004583     |   0\n",
      "        305 |   0.923945  |    0.134594     |   0\n",
      "        306 |   0.837834  |    0.123623     |   0\n",
      "        307 |   0.816212  |    0.072923     |   0\n",
      "        308 |   0.929226  |    0.122417     |   0\n",
      "        309 |   0.883814  |    0.124196     |   0\n",
      "        310 |   0.925365  |    0.073471     |   0\n",
      "        311 |   0.893563  |    0.123902     |   0\n",
      "        312 |   0.827336  |    0.129572     |   0\n",
      "        313 |   1.004813  |    0.073825     |   0\n",
      "        314 |   0.915557  |    0.076670     |   0\n",
      "        315 |   0.995108  |    0.123188     |   0\n",
      "        316 |   0.843533  |    0.125715     |   0\n",
      "        317 |   0.832808  |    0.077890     |   0\n",
      "        318 |   0.931103  |    0.123515     |   0\n",
      "        319 |   0.888434  |    0.124384     |   0\n",
      "        320 |   0.933720  |    0.074916     |   0\n",
      "        321 |   0.864393  |    0.125583     |   0\n",
      "        322 |   0.911695  |    0.132601     |   0\n",
      "        323 |   0.907326  |    0.073219     |   0\n",
      "        324 |   0.886005  |    0.085880     |   0\n",
      "        325 |   0.879241  |    0.125120     |   0\n",
      "        326 |   0.961794  |    0.072727     |   0\n",
      "        327 |   0.947544  |    0.126159     |   0\n",
      "        328 |   0.885343  |    0.172524     |   0\n",
      "        329 |   0.880539  |    0.133313     |   0\n",
      "        330 |   0.844408  |    0.075946     |   0\n",
      "        331 |   0.916638  |    0.130643     |   0\n",
      "        332 |   0.898387  |    0.124829     |   0\n",
      "        333 |   0.840438  |    0.079045     |   0\n",
      "        334 |   0.941516  |    0.122136     |   0\n",
      "        335 |   0.890228  |    0.130438     |   0\n",
      "        336 |   0.827139  |    0.076051     |   0\n",
      "        337 |   0.811563  |    0.133673     |   0\n",
      "        338 |   0.892658  |    0.131013     |   0\n",
      "        339 |   0.875596  |    0.126322     |   0\n",
      "        340 | \u001b[94m  0.790016\u001b[0m  |    0.123275     |   0\n",
      "        341 |   0.963419  |    0.077791     |   0\n",
      "        342 |   0.964177  |    0.144161     |   0\n",
      "        343 |   0.855870  |    0.132171     |   0\n",
      "        344 |   0.893116  |    0.139504     |   0\n",
      "        345 |   0.853677  |    0.185042     |   0\n",
      "        346 |   0.869920  |    0.134201     |   0\n",
      "        347 |   0.893289  |    0.125596     |   0\n",
      "        348 |   0.805445  |    0.126603     |   0\n",
      "        349 |   0.797114  |    0.131070     |   0\n",
      "        350 |   0.882231  |    0.124974     |   0\n",
      "        351 |   0.828955  |    0.129435     |   0\n",
      "        352 |   0.871970  |    0.073344     |   0\n",
      "        353 |   0.808749  |    0.126235     |   0\n",
      "        354 |   0.883899  |    0.077092     |   0\n",
      "        355 |   0.821246  |    0.121826     |   0\n",
      "        356 |   0.905847  |    0.082587     |   0\n",
      "        357 |   0.824989  |    0.135252     |   0\n",
      "        358 |   0.891373  |    0.077469     |   0\n",
      "        359 |   0.906454  |    0.125837     |   0\n",
      "        360 |   0.849881  |    0.079875     |   0\n",
      "        361 |   0.815636  |    0.122931     |   0\n",
      "        362 |   0.887166  |    0.073671     |   0\n",
      "        363 |   0.876718  |    0.122985     |   0\n",
      "        364 |   0.919198  |    0.135537     |   0\n",
      "        365 |   0.853500  |    0.127685     |   0\n",
      "        366 |   0.916747  |    0.132051     |   0\n",
      "        367 |   0.850783  |    0.136761     |   0\n",
      "        368 |   0.795408  |    0.080147     |   0\n",
      "        369 |   0.841255  |    0.131912     |   0\n",
      "        370 |   0.911379  |    0.153345     |   0\n",
      "        371 |   0.843032  |    0.128100     |   0\n",
      "        372 |   0.800652  |    0.173514     |   0\n",
      "        373 |   0.893269  |    0.125538     |   0\n",
      "        374 |   0.840184  |    0.197214     |   0\n",
      "        375 |   0.919366  |    0.186231     |   0\n",
      "        376 |   0.875879  |    0.122155     |   0\n",
      "        377 |   0.837847  |    0.141758     |   0\n",
      "        378 |   0.865108  |    0.126605     |   0\n",
      "        379 |   0.916819  |    0.176243     |   0\n",
      "        380 |   0.894622  |    0.132618     |   0\n",
      "        381 |   0.813122  |    0.127192     |   0\n",
      "        382 |   0.857516  |    0.140556     |   0\n",
      "        383 |   0.852804  |    0.175387     |   0\n",
      "        384 |   0.873777  |    0.090067     |   0\n",
      "        385 | \u001b[94m  0.784209\u001b[0m  |    0.142647     |   0\n",
      "        386 |   0.854569  |    0.171128     |   0\n",
      "        387 |   0.917545  |    0.127872     |   0\n",
      "        388 | \u001b[94m  0.755543\u001b[0m  |    0.131822     |   0\n",
      "        389 |   0.881536  |    0.133410     |   0\n",
      "        390 |   0.875128  |    0.124832     |   0\n",
      "        391 |   0.788573  |    0.124525     |   0\n",
      "        392 |   0.832693  |    0.134606     |   0\n",
      "        393 |   0.780449  |    0.133411     |   0\n",
      "        394 |   0.851799  |    0.126247     |   0\n",
      "        395 |   0.847819  |    0.081034     |   0\n",
      "        396 |   0.806622  |    0.137892     |   0\n",
      "        397 |   0.805741  |    0.189663     |   0\n",
      "        398 |   0.848144  |    0.124866     |   0\n",
      "        399 |   0.794064  |    0.125272     |   0\n",
      "        400 |   0.817488  |    0.074921     |   0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "INFO:neuralnilm.trainer:Stopped training. Completed 400 iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.train_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuralnilm.monitor.monitor import Monitor\n",
    "\n",
    "mon = Monitor(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mon._plot_train_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mon._plot_validation_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'regression.mean_squared_error',\n",
       " u'regression.relative_error_in_total_energy',\n",
       " u'regression.mean_absolute_error',\n",
       " u'classification_2_state.f1_score',\n",
       " u'classification_2_state.recall_score',\n",
       " u'classification_2_state.accuracy_score',\n",
       " u'classification_2_state.precision_score']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon.validation_metric_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
