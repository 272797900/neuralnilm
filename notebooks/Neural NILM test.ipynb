{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from neuralnilm.data.loadactivations import load_nilmtk_activations\n",
    "from neuralnilm.data.syntheticaggregatesource import SyntheticAggregateSource\n",
    "from neuralnilm.data.datapipeline import DataPipeline\n",
    "from neuralnilm.data.processing import DivideBy, IndependentlyCenter\n",
    "from neuralnilm.data.datathread import DataThread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NILMTK_FILENAME = '/data/mine/vadeec/merged/ukdale.h5'\n",
    "TARGET_APPLIANCE = 'kettle'\n",
    "SEQ_LENGTH = 256\n",
    "SAMPLE_PERIOD = 6\n",
    "STRIDE = SEQ_LENGTH\n",
    "WINDOWS = {\n",
    "    'train': {\n",
    "        1: (\"2014-01-01\", \"2014-02-01\")\n",
    "    },\n",
    "    'unseen_activations_of_seen_appliances': {\n",
    "        1: (\"2014-02-02\", \"2014-02-08\")                \n",
    "    },\n",
    "    'unseen_appliances': {\n",
    "        2: (\"2013-06-01\", \"2013-06-07\")\n",
    "    }\n",
    "}\n",
    "\n",
    "LOADER_CONFIG = {\n",
    "    'nilmtk_activations': dict(\n",
    "        appliances=['kettle', 'microwave', 'washing machine'],\n",
    "        filename=NILMTK_FILENAME,\n",
    "        sample_period=SAMPLE_PERIOD,\n",
    "        windows=WINDOWS\n",
    "    )\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuralnilm.data.stridesource import StrideSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.data.stridesource:Loading NILMTK data...\n",
      "INFO:neuralnilm.data.stridesource:Loading data for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.stridesource:Loaded data from building UK-DALE_building_1 for fold train from 2014-01-01 00:00:06+00:00 to 2014-01-31 23:59:54+00:00.\n",
      "INFO:neuralnilm.data.stridesource:Loading data for UK-DALE_building_2...\n",
      "INFO:neuralnilm.data.stridesource:Loaded data from building UK-DALE_building_2 for fold unseen_appliances from 2013-06-01 00:00:00+01:00 to 2013-06-06 23:59:54+01:00.\n",
      "INFO:neuralnilm.data.stridesource:Loading data for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.stridesource:Loaded data from building UK-DALE_building_1 for fold unseen_activations_of_seen_appliances from 2014-02-02 00:00:00+00:00 to 2014-02-07 23:59:54+00:00.\n",
      "INFO:neuralnilm.data.stridesource:Done loading NILMTK mains data.\n"
     ]
    }
   ],
   "source": [
    "stride_source = StrideSource(\n",
    "    target_appliance=TARGET_APPLIANCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    filename=NILMTK_FILENAME,\n",
    "    windows=WINDOWS,\n",
    "    sample_period=SAMPLE_PERIOD,\n",
    "    stride=STRIDE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.data.loadactivations:Loading NILMTK activations...\n",
      "INFO:neuralnilm.data.loadactivations:Loading kettle for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 111 kettle activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Loading microwave for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 114 microwave activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Loading washing machine for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 23 washing machine activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Loading kettle for UK-DALE_building_2...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 31 kettle activations from UK-DALE_building_2.\n",
      "INFO:neuralnilm.data.loadactivations:Loading microwave for UK-DALE_building_2...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 31 microwave activations from UK-DALE_building_2.\n",
      "INFO:neuralnilm.data.loadactivations:Loading washing machine for UK-DALE_building_2...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 2 washing machine activations from UK-DALE_building_2.\n",
      "INFO:neuralnilm.data.loadactivations:Loading kettle for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 28 kettle activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Loading microwave for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 30 microwave activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Loading washing machine for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.loadactivations:Loaded 5 washing machine activations from UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.loadactivations:Done loading NILMTK activations.\n"
     ]
    }
   ],
   "source": [
    "nilmtk_activations = load_nilmtk_activations(**LOADER_CONFIG['nilmtk_activations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'unseen_activations_of_seen_appliances', 'unseen_appliances']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nilmtk_activations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuralnilm.data.realaggregatesource import RealAggregateSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.data.realaggregatesource:Loading NILMTK mains...\n",
      "INFO:neuralnilm.data.realaggregatesource:Loading mains for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.realaggregatesource:Loaded mains data from building UK-DALE_building_1 for fold train from 2014-01-01 00:00:00+00:00 to 2014-01-31 23:59:54+00:00.\n",
      "INFO:neuralnilm.data.realaggregatesource:Loading mains for UK-DALE_building_2...\n",
      "INFO:neuralnilm.data.realaggregatesource:Loaded mains data from building UK-DALE_building_2 for fold unseen_appliances from 2013-06-01 00:00:00+01:00 to 2013-06-06 23:59:54+01:00.\n",
      "INFO:neuralnilm.data.realaggregatesource:Loading mains for UK-DALE_building_1...\n",
      "INFO:neuralnilm.data.realaggregatesource:Loaded mains data from building UK-DALE_building_1 for fold unseen_activations_of_seen_appliances from 2014-02-02 00:00:00+00:00 to 2014-02-07 23:59:54+00:00.\n",
      "INFO:neuralnilm.data.realaggregatesource:Done loading NILMTK mains data.\n",
      "INFO:neuralnilm.data.realaggregatesource:Found 89 sections without target for train UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.realaggregatesource:Found 20 sections without target for unseen_activations_of_seen_appliances UK-DALE_building_1.\n",
      "INFO:neuralnilm.data.realaggregatesource:Found 24 sections without target for unseen_appliances UK-DALE_building_2.\n"
     ]
    }
   ],
   "source": [
    "ras = RealAggregateSource(\n",
    "    activations=nilmtk_activations,\n",
    "    target_appliance=TARGET_APPLIANCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    filename=NILMTK_FILENAME,\n",
    "    windows=WINDOWS,\n",
    "    sample_period=SAMPLE_PERIOD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ras.target_inclusion_prob = 0.5\n",
    "for i in range(50):\n",
    "    seq = ras.get_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(2)\n",
    "#axes[0].plot(seq.input)\n",
    "#axes[1].plot(seq.target)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'building', 'appliance', 'fold']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nilmtk_activations['train']['kettle']['UK-DALE_building_1'][0]._metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source = SyntheticAggregateSource(\n",
    "    activations=nilmtk_activations,\n",
    "    target_appliance=TARGET_APPLIANCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    allow_incomplete_target=False,\n",
    "    sample_period=SAMPLE_PERIOD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FOLD = 'train'\n",
    "#FOLD = 'unseen_activations_of_seen_appliances'\n",
    "#FOLD = 'unseen_appliances'\n",
    "seq = source.get_sequence(enable_all_appliances=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all(seq.all_appliances.sum(axis=1) == seq.input[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(2, sharex=True)\n",
    "#seq.all_appliances.plot(ax=axes[0])\n",
    "#axes[1].plot(seq.input)\n",
    "#fig.suptitle(FOLD)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = source.get_batch(num_seq_per_batch=1024).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = DataPipeline(\n",
    "    sources=[source, ras, stride_source],\n",
    "    num_seq_per_batch=64,\n",
    "    input_processing=[DivideBy(sample.before_processing.input.flatten().std()), IndependentlyCenter()],\n",
    "    target_processing=[DivideBy(sample.before_processing.target.flatten().std())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "nilmtk_disag_source = NILMTKDisagSource(\n",
    "    filename=NILMTK_FILENAME,\n",
    "    target_appliance=TARGET_APPLIANCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    buildings=[5],\n",
    "    window_per_building={},\n",
    "    stride=STRIDE,\n",
    "    sample_period=SAMPLE_PERIOD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "disag_pipeline = deepcopy(pipeline)\n",
    "disag_pipeline.source = nilmtk_disag_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "disaggregator = Disaggregator(\n",
    "    pipeline=disag_pipeline,\n",
    "    output_path=PATH  # \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disagregator ideas:\n",
    "\n",
    "* make a copy of pipeline but swap source for a NILMTKDisagSource\n",
    "* NILMTKDisagSource loads all data into memory (?) and iterates over chunks of it (get seq_length from pipeline.source.seq_length)\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pipeline': {'input_processing': [{'divisor': 639.70892, 'name': 'DivideBy'},\n",
       "   {'name': 'IndependentlyCenter'}],\n",
       "  'num_seq_per_batch': 64,\n",
       "  'rng_seed': None,\n",
       "  'source_probabilities': [0.3333333333333333,\n",
       "   0.3333333333333333,\n",
       "   0.3333333333333333],\n",
       "  'sources': {0: {'allow_incomplete_distractors': True,\n",
       "    'allow_incomplete_target': False,\n",
       "    'distractor_inclusion_prob': 0.25,\n",
       "    'include_incomplete_target_in_output': True,\n",
       "    'name': 'SyntheticAggregateSource',\n",
       "    'num_batches_for_validation': 16,\n",
       "    'rng_seed': None,\n",
       "    'sample_period': 6,\n",
       "    'seq_length': 256,\n",
       "    'target_appliance': 'kettle',\n",
       "    'target_inclusion_prob': 0.5,\n",
       "    'uniform_prob_of_selecting_each_building': True},\n",
       "   1: {'allow_incomplete_target': True,\n",
       "    'allow_multiple_target_activations_in_aggregate': False,\n",
       "    'filename': '/data/mine/vadeec/merged/ukdale.h5',\n",
       "    'include_incomplete_target_in_output': True,\n",
       "    'include_multiple_targets_in_output': False,\n",
       "    'name': 'RealAggregateSource',\n",
       "    'num_batches_for_validation': 16,\n",
       "    'rng_seed': None,\n",
       "    'sample_period': 6,\n",
       "    'seq_length': 256,\n",
       "    'target_appliance': 'kettle',\n",
       "    'target_inclusion_prob': 0.5,\n",
       "    'uniform_prob_of_selecting_each_building': True,\n",
       "    'windows': {'train': {1: ('2014-01-01', '2014-02-01')},\n",
       "     'unseen_activations_of_seen_appliances': {1: ('2014-02-02',\n",
       "       '2014-02-08')},\n",
       "     'unseen_appliances': {2: ('2013-06-01', '2013-06-07')}}},\n",
       "   2: {'filename': '/data/mine/vadeec/merged/ukdale.h5',\n",
       "    'name': 'StrideSource',\n",
       "    'num_batches_for_validation': None,\n",
       "    'rng_seed': None,\n",
       "    'sample_period': 6,\n",
       "    'seq_length': 256,\n",
       "    'stride': 256,\n",
       "    'target_appliance': 'kettle',\n",
       "    'windows': {'train': {1: ('2014-01-01', '2014-02-01')},\n",
       "     'unseen_activations_of_seen_appliances': {1: ('2014-02-02',\n",
       "       '2014-02-08')},\n",
       "     'unseen_appliances': {2: ('2013-06-01', '2013-06-07')}}}},\n",
       "  'target_processing': [{'divisor': 437.0192, 'name': 'DivideBy'}]}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pipeline.report()\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer, RecurrentLayer, DenseLayer, ReshapeLayer\n",
    "\n",
    "def get_net_0(input_shape, target_shape=None):\n",
    "    NUM_UNITS = {\n",
    "        'dense_layer_0': 100,\n",
    "        'dense_layer_1':  50,\n",
    "        'dense_layer_2': 100\n",
    "    }\n",
    "\n",
    "    if target_shape is None:\n",
    "        target_shape = input_shape\n",
    "    \n",
    "    # Define layers\n",
    "    input_layer = InputLayer(\n",
    "        shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Dense layers\n",
    "    dense_layer_0 = DenseLayer(\n",
    "        input_layer, \n",
    "        num_units=NUM_UNITS['dense_layer_0']\n",
    "    )\n",
    "    dense_layer_1 = DenseLayer(\n",
    "        dense_layer_0, \n",
    "        num_units=NUM_UNITS['dense_layer_1']\n",
    "    )\n",
    "    dense_layer_2 = DenseLayer(\n",
    "        dense_layer_1, \n",
    "        num_units=NUM_UNITS['dense_layer_2']\n",
    "    )\n",
    "    \n",
    "    # Output\n",
    "    final_dense_layer = DenseLayer(\n",
    "        dense_layer_2,\n",
    "        num_units=target_shape[1] * target_shape[2],\n",
    "        nonlinearity=None\n",
    "    )\n",
    "    output_layer = ReshapeLayer(\n",
    "        final_dense_layer,\n",
    "        shape=target_shape\n",
    "    )\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuralnilm.net import Net\n",
    "\n",
    "batch = pipeline.get_batch()\n",
    "output_layer = get_net_0(\n",
    "    batch.after_processing.input.shape, \n",
    "    batch.after_processing.target.shape\n",
    ")\n",
    "net = Net(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neuralnilm.trainer import Trainer\n",
    "from neuralnilm.metrics import Metrics\n",
    "\n",
    "trainer = Trainer(\n",
    "    net=net,\n",
    "    data_pipeline=pipeline,\n",
    "    experiment_id=[\"2\"],\n",
    "    metrics=Metrics(state_boundaries=[4]),\n",
    "    learning_rates={0: 1E-2},\n",
    "    repeat_callbacks=[\n",
    "        (1000, Trainer.validate)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '2',\n",
       " 'data': {'activations': {'nilmtk_activations': {'appliances': ['kettle',\n",
       "     'microwave',\n",
       "     'washing machine'],\n",
       "    'filename': '/data/mine/vadeec/merged/ukdale.h5',\n",
       "    'sample_period': 6,\n",
       "    'windows': {'train': {'1': ('2014-01-01', '2014-02-01')},\n",
       "     'unseen_activations_of_seen_appliances': {'1': ('2014-02-02',\n",
       "       '2014-02-08')},\n",
       "     'unseen_appliances': {'2': ('2013-06-01', '2013-06-07')}}}},\n",
       "  'pipeline': {'input_processing': [{'divisor': 639.7089233398438,\n",
       "     'name': 'DivideBy'},\n",
       "    {'name': 'IndependentlyCenter'}],\n",
       "   'num_seq_per_batch': 64,\n",
       "   'rng_seed': None,\n",
       "   'source_probabilities': [0.3333333333333333,\n",
       "    0.3333333333333333,\n",
       "    0.3333333333333333],\n",
       "   'sources': {'0': {'allow_incomplete_distractors': True,\n",
       "     'allow_incomplete_target': False,\n",
       "     'distractor_inclusion_prob': 0.25,\n",
       "     'include_incomplete_target_in_output': True,\n",
       "     'name': 'SyntheticAggregateSource',\n",
       "     'num_batches_for_validation': 16,\n",
       "     'rng_seed': None,\n",
       "     'sample_period': 6,\n",
       "     'seq_length': 256,\n",
       "     'target_appliance': 'kettle',\n",
       "     'target_inclusion_prob': 0.5,\n",
       "     'uniform_prob_of_selecting_each_building': True},\n",
       "    '1': {'allow_incomplete_target': True,\n",
       "     'allow_multiple_target_activations_in_aggregate': False,\n",
       "     'filename': '/data/mine/vadeec/merged/ukdale.h5',\n",
       "     'include_incomplete_target_in_output': True,\n",
       "     'include_multiple_targets_in_output': False,\n",
       "     'name': 'RealAggregateSource',\n",
       "     'num_batches_for_validation': 16,\n",
       "     'rng_seed': None,\n",
       "     'sample_period': 6,\n",
       "     'seq_length': 256,\n",
       "     'target_appliance': 'kettle',\n",
       "     'target_inclusion_prob': 0.5,\n",
       "     'uniform_prob_of_selecting_each_building': True,\n",
       "     'windows': {'train': {'1': ('2014-01-01', '2014-02-01')},\n",
       "      'unseen_activations_of_seen_appliances': {'1': ('2014-02-02',\n",
       "        '2014-02-08')},\n",
       "      'unseen_appliances': {'2': ('2013-06-01', '2013-06-07')}}},\n",
       "    '2': {'filename': '/data/mine/vadeec/merged/ukdale.h5',\n",
       "     'name': 'StrideSource',\n",
       "     'num_batches_for_validation': None,\n",
       "     'rng_seed': None,\n",
       "     'sample_period': 6,\n",
       "     'seq_length': 256,\n",
       "     'stride': 256,\n",
       "     'target_appliance': 'kettle',\n",
       "     'windows': {'train': {'1': ('2014-01-01', '2014-02-01')},\n",
       "      'unseen_activations_of_seen_appliances': {'1': ('2014-02-02',\n",
       "        '2014-02-08')},\n",
       "      'unseen_appliances': {'2': ('2013-06-01', '2013-06-07')}}}},\n",
       "   'target_processing': [{'divisor': 437.0191955566406, 'name': 'DivideBy'}]}},\n",
       " 'trainer': {'learning_rates': {'0': 0.01},\n",
       "  'loss_aggregation_mode': 'mean',\n",
       "  'loss_func_name': 'squared_error',\n",
       "  'metrics': {'clip_to_zero': False,\n",
       "   'name': 'Metrics',\n",
       "   'state_boundaries': [4]},\n",
       "  'min_train_cost': inf,\n",
       "  'output_path': '~/temp/neural_nilm/output/2',\n",
       "  'updates_func_kwards': {},\n",
       "  'updates_func_name': 'nesterov_momentum'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = trainer.report()\n",
    "report['data']['activations'] = LOADER_CONFIG\n",
    "from neuralnilm.utils import sanitise_dict_for_mongo\n",
    "sanitise_dict_for_mongo(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Starting training for 50000 iterations.\n",
      "INFO:neuralnilm.trainer:Iteration 0: Change learning rate to 1.0E-02\n",
      "INFO:neuralnilm.trainer:Compiling train cost function...\n",
      "INFO:neuralnilm.trainer:Done compiling cost function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Update # |  Train cost  | Secs per update | Source ID\n",
      "------------|--------------|-----------------|-----------\n",
      "          0 | \u001b[94m  0.256377\u001b[0m  |    3.999298     |   2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.net:Compiling deterministic output function...\n",
      "INFO:neuralnilm.net:Done compiling deterministic output function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          1 |   1.193617  |    0.084810     |   0\n",
      "          2 |   1.092931  |    0.073353     |   0\n",
      "          3 |   0.268355  |    0.056668     |   2\n",
      "          4 |   1.228214  |    0.053694     |   1\n",
      "          5 |   0.861982  |    0.057887     |   1\n",
      "          6 | \u001b[94m  0.156734\u001b[0m  |    0.073883     |   2\n",
      "          7 |   0.898247  |    0.078130     |   0\n",
      "          8 |   0.181534  |    0.081200     |   2\n",
      "          9 |   0.246389  |    0.094582     |   2\n",
      "         10 |   0.969262  |    0.195059     |   1\n",
      "         11 |   0.830716  |    0.063007     |   0\n",
      "         12 |   0.938520  |    0.181302     |   1\n",
      "         13 |   0.927985  |    0.164641     |   1\n",
      "         14 |   0.842436  |    0.093595     |   0\n",
      "         15 |   0.938825  |    0.186558     |   1\n",
      "         16 |   0.960895  |    0.126861     |   1\n",
      "         17 |   0.228539  |    0.057509     |   2\n",
      "         18 |   0.853574  |    0.157425     |   1\n",
      "         19 | \u001b[94m  0.124568\u001b[0m  |    0.063648     |   2\n",
      "         20 |   0.346752  |    0.076634     |   2\n",
      "         21 |   1.259535  |    0.228093     |   1\n",
      "         22 |   1.191586  |    0.133327     |   1\n",
      "         23 |   1.085355  |    0.066997     |   0\n",
      "         24 |   0.893611  |    0.086638     |   0\n",
      "         25 |   0.184150  |    0.059094     |   2\n",
      "         26 |   0.901549  |    0.188140     |   1\n",
      "         27 |   0.272870  |    0.057893     |   2\n",
      "         28 |   1.048540  |    0.230005     |   1\n",
      "         29 |   0.233746  |    0.070190     |   2\n",
      "         30 |   0.938214  |    0.135341     |   1\n",
      "         31 |   1.593831  |    0.152849     |   1\n",
      "         32 |   0.727134  |    0.162939     |   1\n",
      "         33 |   0.883771  |    0.070357     |   0\n",
      "         34 |   0.924311  |    0.212378     |   1\n",
      "         35 |   1.129864  |    0.066369     |   0\n",
      "         36 |   1.100926  |    0.109108     |   1\n",
      "         37 |   1.028888  |    0.074740     |   0\n",
      "         38 |   1.210904  |    0.097539     |   0\n",
      "         39 |   0.939144  |    0.190597     |   1\n",
      "         40 | \u001b[94m  0.072866\u001b[0m  |    0.053694     |   2\n",
      "         41 |   1.230720  |    0.134768     |   1\n",
      "         42 |   1.030459  |    0.068509     |   0\n",
      "         43 |   0.923368  |    0.088276     |   0\n",
      "         44 |   1.071403  |    0.097017     |   0\n",
      "         45 |   0.983989  |    0.130590     |   1\n",
      "         46 |   0.991945  |    0.061429     |   0\n",
      "         47 |   1.079809  |    0.200953     |   1\n",
      "         48 |   1.391397  |    0.074716     |   0\n",
      "         49 |   0.791240  |    0.132751     |   1\n",
      "         50 |   0.962523  |    0.062304     |   0\n",
      "         51 | \u001b[94m  0.000140\u001b[0m  |    0.066334     |   2\n",
      "         52 |   0.797263  |    0.088385     |   0\n",
      "         53 |   0.051411  |    0.068598     |   2\n",
      "         54 |   0.679527  |    0.192426     |   1\n",
      "         55 |   0.815499  |    0.048410     |   0\n",
      "         56 |   1.126462  |    0.068762     |   0\n",
      "         57 |   0.859680  |    0.230833     |   1\n",
      "         58 |   0.409078  |    0.052814     |   2\n",
      "         59 |   0.207739  |    0.055502     |   2\n",
      "         60 |   0.206662  |    0.078372     |   2\n",
      "         61 |   0.881031  |    0.140839     |   1\n",
      "         62 |   1.263608  |    0.088605     |   0\n",
      "         63 |   1.079496  |    0.075945     |   0\n",
      "         64 |   0.147781  |    0.069687     |   2\n",
      "         65 |   0.846943  |    0.075542     |   0\n",
      "         66 |   0.077789  |    0.053127     |   2\n",
      "         67 |   1.003385  |    0.092628     |   0\n",
      "         68 |   0.904863  |    0.072017     |   0\n",
      "         69 |   0.096057  |    0.081466     |   2\n",
      "         70 |   0.142747  |    0.076949     |   2\n",
      "         71 |   0.923787  |    0.097095     |   0\n",
      "         72 | \u001b[94m  0.000138\u001b[0m  |    0.058599     |   2\n",
      "         73 |   0.919766  |    0.104632     |   0\n",
      "         74 |   0.000181  |    0.056824     |   2\n",
      "         75 |   0.706767  |    0.082145     |   0\n",
      "         76 | \u001b[94m  0.000114\u001b[0m  |    0.070440     |   2\n",
      "         77 |   0.863570  |    0.093611     |   0\n",
      "         78 |   0.000145  |    0.062881     |   2\n",
      "         79 |   0.000263  |    0.072718     |   2\n",
      "         80 |   0.793701  |    0.078192     |   0\n",
      "         81 |   0.000291  |    0.057731     |   2\n",
      "         82 |   1.161872  |    0.151567     |   0\n",
      "         83 |   0.905170  |    0.131067     |   1\n",
      "         84 |   0.268663  |    0.060099     |   2\n",
      "         85 |   0.780703  |    0.182460     |   1\n",
      "         86 |   1.040029  |    0.139014     |   1\n",
      "         87 |   0.940682  |    0.059987     |   0\n",
      "         88 |   0.919352  |    0.117914     |   0\n",
      "         89 |   0.885534  |    0.059523     |   0\n",
      "         90 |   0.199849  |    0.089773     |   2\n",
      "         91 |   0.680172  |    0.073577     |   0\n",
      "         92 |   0.713898  |    0.251684     |   1\n",
      "         93 |   0.678370  |    0.075147     |   0\n",
      "         94 |   0.955990  |    0.100733     |   0\n",
      "         95 |   1.052078  |    0.128637     |   1\n",
      "         96 |   0.958893  |    0.079606     |   0\n",
      "         97 |   0.885495  |    0.136086     |   1\n",
      "         98 |   0.979962  |    0.068067     |   0\n",
      "         99 |   0.705689  |    0.080131     |   0\n",
      "        100 |   0.927213  |    0.117530     |   0\n",
      "        101 |   0.889838  |    0.152021     |   1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "INFO:neuralnilm.trainer:Iteration 102: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        102 |   0.156378  |    0.061059     |   2\n",
      "        103 |   0.770864  |    0.191319     |   1\n",
      "        104 |   0.805885  |    0.059191     |   0\n",
      "        105 |   0.180175  |    0.057878     |   2\n",
      "        106 |   0.225349  |    0.074923     |   2\n",
      "        107 |   0.207533  |    0.087915     |   2\n",
      "        108 |   0.105121  |    0.061663     |   2\n",
      "        109 |   0.228529  |    0.094829     |   2\n",
      "        110 |   0.170395  |    0.058731     |   2\n",
      "        111 |   0.252372  |    0.093406     |   2\n",
      "        112 |   0.217753  |    0.074637     |   2\n",
      "        113 |   0.929164  |    0.182568     |   1\n",
      "        114 |   0.781108  |    0.143233     |   1\n",
      "        115 |   1.096337  |    0.074164     |   0\n",
      "        116 |   1.092788  |    0.061412     |   0\n",
      "        117 |   0.989811  |    0.086154     |   0\n",
      "        118 |   0.829713  |    0.072450     |   0\n",
      "        119 |   1.016446  |    0.111142     |   0\n",
      "        120 |   1.038134  |    0.137255     |   1\n",
      "        121 |   0.970947  |    0.074231     |   0\n",
      "        122 |   0.935042  |    0.185017     |   1\n",
      "        123 |   0.856498  |    0.050453     |   0\n",
      "        124 |   0.048432  |    0.053510     |   2\n",
      "        125 |   0.000297  |    0.089797     |   2\n",
      "        126 |   0.045895  |    0.080673     |   2\n",
      "        127 |   0.839701  |    0.073471     |   0\n",
      "        128 |   0.367722  |    0.068300     |   2\n",
      "        129 |   0.190592  |    0.073348     |   2\n",
      "        130 |   0.184944  |    0.064763     |   2\n",
      "        131 |   0.841852  |    0.110761     |   0\n",
      "        132 |   0.134421  |    0.056740     |   2\n",
      "        133 |   0.046600  |    0.070885     |   2\n",
      "        134 |   0.097836  |    0.075921     |   2\n",
      "        135 |   0.769421  |    0.087013     |   0\n",
      "        136 |   0.729592  |    0.175965     |   1\n",
      "        137 |   0.746945  |    0.141150     |   1\n",
      "        138 |   1.122657  |    0.060496     |   0\n",
      "        139 |   1.071780  |    0.174096     |   1\n",
      "        140 |   0.939445  |    0.210758     |   1\n",
      "        141 |   0.131218  |    0.056840     |   2\n",
      "        142 |   0.000277  |    0.054060     |   2\n",
      "        143 |   0.850583  |    0.094078     |   0\n",
      "        144 |   0.717285  |    0.087909     |   0\n",
      "        145 |   0.514747  |    0.203937     |   1\n",
      "        146 |   0.910387  |    0.062889     |   0\n",
      "        147 |   0.936444  |    0.141061     |   1\n",
      "        148 |   1.047336  |    0.179400     |   1\n",
      "        149 |   0.693925  |    0.059539     |   0\n",
      "        150 |   1.120326  |    0.148866     |   1\n",
      "        151 |   0.000395  |    0.055366     |   2\n",
      "        152 |   0.855200  |    0.094021     |   0\n",
      "        153 |   0.000269  |    0.059758     |   2\n",
      "        154 |   0.000350  |    0.090493     |   2\n",
      "        155 |   0.790003  |    0.194523     |   1\n",
      "        156 |   0.920550  |    0.072052     |   0\n",
      "        157 |   1.022416  |    0.123208     |   1\n",
      "        158 |   0.805225  |    0.150379     |   1\n",
      "        159 |   0.709581  |    0.103659     |   0\n",
      "        160 |   0.698765  |    0.166953     |   1\n",
      "        161 |   0.000704  |    0.062887     |   2\n",
      "        162 |   0.867505  |    0.126609     |   0\n",
      "        163 |   0.754640  |    0.138996     |   1\n",
      "        164 |   0.909510  |    0.139917     |   1\n",
      "        165 |   0.000563  |    0.083406     |   2\n",
      "        166 |   0.950083  |    0.073723     |   0\n",
      "        167 |   0.232855  |    0.071612     |   2\n",
      "        168 |   0.663678  |    0.093981     |   0\n",
      "        169 |   0.199814  |    0.078255     |   2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 170: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        170 |   0.633075  |    0.134624     |   1\n",
      "        171 |   0.157052  |    0.067067     |   2\n",
      "        172 |   0.830406  |    0.086658     |   0\n",
      "        173 |   0.912978  |    0.187764     |   1\n",
      "        174 |   0.148854  |    0.053741     |   2\n",
      "        175 |   0.873756  |    0.074728     |   0\n",
      "        176 |   0.670064  |    0.192327     |   1\n",
      "        177 |   0.719824  |    0.055503     |   0\n",
      "        178 |   0.215775  |    0.061745     |   2\n",
      "        179 |   0.910318  |    0.190709     |   1\n",
      "        180 |   0.916770  |    0.053404     |   0\n",
      "        181 |   0.936953  |    0.181507     |   1\n",
      "        182 |   0.199903  |    0.054755     |   2\n",
      "        183 |   0.763368  |    0.062563     |   0\n",
      "        184 |   0.754860  |    0.191015     |   1\n",
      "        185 |   0.830238  |    0.125520     |   1\n",
      "        186 |   0.090104  |    0.084637     |   2\n",
      "        187 |   0.711079  |    0.147356     |   1\n",
      "        188 |   0.873023  |    0.080052     |   0\n",
      "        189 |   0.934790  |    0.093937     |   0\n",
      "        190 |   0.806083  |    0.084141     |   0\n",
      "        191 |   0.727348  |    0.188360     |   1\n",
      "        192 |   0.199407  |    0.049411     |   2\n",
      "        193 |   0.823553  |    0.058720     |   0\n",
      "        194 |   0.701920  |    0.106423     |   0\n",
      "        195 |   0.762927  |    0.133886     |   1\n",
      "        196 |   0.758229  |    0.053981     |   0\n",
      "        197 |   0.908240  |    0.088054     |   0\n",
      "        198 |   0.889859  |    0.095726     |   0\n",
      "        199 |   0.164784  |    0.064549     |   2\n",
      "        200 |   0.800467  |    0.197627     |   1\n",
      "        201 |   0.716823  |    0.059493     |   0\n",
      "        202 |   0.217463  |    0.086904     |   2\n",
      "        203 |   0.749739  |    0.076221     |   0\n",
      "        204 |   0.644457  |    0.077265     |   0\n",
      "        205 |   0.187431  |    0.058985     |   2\n",
      "        206 |   0.804610  |    0.097990     |   0\n",
      "        207 |   0.050117  |    0.064156     |   2\n",
      "        208 |   0.000663  |    0.089307     |   2\n",
      "        209 |   0.708218  |    0.073175     |   0\n",
      "        210 |   0.761899  |    0.222937     |   1\n",
      "        211 |   0.980466  |    0.127818     |   1\n",
      "        212 |   0.747237  |    0.186336     |   1\n",
      "        213 |   0.789128  |    0.118183     |   1\n",
      "        214 |   0.034474  |    0.064447     |   2\n",
      "        215 |   0.342054  |    0.073582     |   2\n",
      "        216 |   0.171668  |    0.075669     |   2\n",
      "        217 |   0.673397  |    0.074032     |   0\n",
      "        218 |   0.161593  |    0.077337     |   2\n",
      "        219 |   0.117019  |    0.073789     |   2\n",
      "        220 |   0.644539  |    0.193777     |   1\n",
      "        221 |   0.812737  |    0.131120     |   1\n",
      "        222 |   0.617807  |    0.175351     |   1\n",
      "        223 |   0.038207  |    0.056197     |   2\n",
      "        224 |   0.892133  |    0.059809     |   0\n",
      "        225 |   0.100351  |    0.088861     |   2\n",
      "        226 |   0.878202  |    0.196710     |   1\n",
      "        227 |   0.124461  |    0.055689     |   2\n",
      "        228 |   0.000481  |    0.052163     |   2\n",
      "        229 |   0.000595  |    0.074373     |   2\n",
      "        230 |   0.000405  |    0.061223     |   2\n",
      "        231 |   0.598255  |    0.181982     |   1\n",
      "        232 |   1.011063  |    0.076656     |   0\n",
      "        233 |   0.777418  |    0.094820     |   0\n",
      "        234 |   0.764414  |    0.059384     |   0\n",
      "        235 |   0.000599  |    0.096287     |   2\n",
      "        236 |   0.000970  |    0.066282     |   2\n",
      "        237 |   0.691495  |    0.207276     |   1\n",
      "        238 |   0.764801  |    0.144225     |   1\n",
      "        239 |   0.622955  |    0.141100     |   1\n",
      "        240 |   0.000833  |    0.092536     |   2\n",
      "        241 |   0.752723  |    0.196685     |   1\n",
      "        242 |   0.548558  |    0.074669     |   0\n",
      "        243 |   0.721756  |    0.187958     |   1\n",
      "        244 |   0.586257  |    0.054521     |   0\n",
      "        245 |   0.195836  |    0.068482     |   2\n",
      "        246 |   0.831639  |    0.194757     |   1\n",
      "        247 |   0.193401  |    0.051926     |   2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 248: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        248 |   0.744507  |    0.064169     |   0\n",
      "        249 |   0.788490  |    0.085704     |   0\n",
      "        250 |   0.650721  |    0.180269     |   1\n",
      "        251 |   0.738201  |    0.123349     |   1\n",
      "        252 |   0.543132  |    0.078262     |   0\n",
      "        253 |   0.843455  |    0.081553     |   0\n",
      "        254 |   0.150966  |    0.080961     |   2\n",
      "        255 |   0.860842  |    0.099858     |   0\n",
      "        256 |   0.729276  |    0.092723     |   0\n",
      "        257 |   0.785636  |    0.192490     |   1\n",
      "        258 |   0.129639  |    0.064023     |   2\n",
      "        259 |   0.695353  |    0.179263     |   1\n",
      "        260 |   0.793437  |    0.054409     |   0\n",
      "        261 |   0.195219  |    0.086638     |   2\n",
      "        262 |   0.192372  |    0.082314     |   2\n",
      "        263 |   0.776403  |    0.186134     |   1\n",
      "        264 |   0.603309  |    0.144452     |   1\n",
      "        265 |   0.623775  |    0.182167     |   1\n",
      "        266 |   0.584732  |    0.064349     |   0\n",
      "        267 |   0.812206  |    0.128322     |   1\n",
      "        268 |   0.572693  |    0.185793     |   1\n",
      "        269 |   0.078059  |    0.058356     |   2\n",
      "        270 |   0.830466  |    0.182406     |   1\n",
      "        271 |   0.180100  |    0.066105     |   2\n",
      "        272 |   0.763148  |    0.100305     |   0\n",
      "        273 |   0.698576  |    0.076768     |   0\n",
      "        274 |   0.751018  |    0.184349     |   1\n",
      "        275 |   0.696610  |    0.068749     |   0\n",
      "        276 |   0.678182  |    0.174720     |   1\n",
      "        277 |   0.152169  |    0.071220     |   2\n",
      "        278 |   0.725716  |    0.181611     |   1\n",
      "        279 |   0.668099  |    0.131373     |   1\n",
      "        280 |   0.186287  |    0.083975     |   2\n",
      "        281 |   0.605842  |    0.206965     |   1\n",
      "        282 |   0.636349  |    0.163987     |   1\n",
      "        283 |   0.536726  |    0.052725     |   0\n",
      "        284 |   0.166151  |    0.063209     |   2\n",
      "        285 |   0.732870  |    0.077555     |   0\n",
      "        286 |   0.579139  |    0.100799     |   0\n",
      "        287 |   0.573681  |    0.067076     |   0\n",
      "        288 |   0.734732  |    0.205369     |   1\n",
      "        289 |   0.038569  |    0.062473     |   2\n",
      "        290 |   0.720878  |    0.148782     |   1\n",
      "        291 |   0.000698  |    0.062002     |   2\n",
      "        292 |   0.026268  |    0.100257     |   2\n",
      "        293 |   0.319197  |    0.077613     |   2\n",
      "        294 |   0.650218  |    0.185745     |   1\n",
      "        295 |   0.720075  |    0.146525     |   1\n",
      "        296 |   0.153602  |    0.092412     |   2\n",
      "        297 |   0.759708  |    0.181883     |   1\n",
      "        298 |   0.748124  |    0.054635     |   0\n",
      "        299 |   0.594321  |    0.060921     |   0\n",
      "        300 |   0.148022  |    0.081664     |   2\n",
      "        301 |   0.661397  |    0.090862     |   0\n",
      "        302 |   0.674144  |    0.090642     |   0\n",
      "        303 |   0.102051  |    0.123975     |   2\n",
      "        304 |   0.600492  |    0.128236     |   1\n",
      "        305 |   0.039847  |    0.095288     |   2\n",
      "        306 |   0.692272  |    0.069566     |   0\n",
      "        307 |   0.109491  |    0.090335     |   2\n",
      "        308 |   0.601889  |    0.186090     |   1\n",
      "        309 |   0.607918  |    0.057659     |   0\n",
      "        310 |   0.659994  |    0.201919     |   1\n",
      "        311 |   0.387202  |    0.170598     |   1\n",
      "        312 |   0.617598  |    0.056873     |   0\n",
      "        313 |   0.716007  |    0.130662     |   1\n",
      "        314 |   0.700582  |    0.073054     |   0\n",
      "        315 |   0.571436  |    0.194275     |   1\n",
      "        316 |   0.508244  |    0.131177     |   1\n",
      "        317 |   0.116716  |    0.064690     |   2\n",
      "        318 |   0.709930  |    0.190680     |   1\n",
      "        319 |   0.516085  |    0.193163     |   1\n",
      "        320 |   0.000574  |    0.053813     |   2\n",
      "        321 |   0.511242  |    0.068597     |   0\n",
      "        322 |   0.000669  |    0.093490     |   2\n",
      "        323 |   0.000501  |    0.059031     |   2\n",
      "        324 |   0.000684  |    0.059048     |   2\n",
      "        325 |   0.828305  |    0.089981     |   0\n",
      "        326 |   0.000952  |    0.078178     |   2\n",
      "        327 |   0.720414  |    0.188251     |   1\n",
      "        328 |   0.748741  |    0.127096     |   1\n",
      "        329 |   0.000866  |    0.057215     |   2\n",
      "        330 |   0.700172  |    0.098791     |   0\n",
      "        331 |   0.162527  |    0.056945     |   2\n",
      "        332 |   0.504182  |    0.097671     |   0\n",
      "        333 |   0.549073  |    0.222848     |   1\n",
      "        334 |   0.552888  |    0.132805     |   1\n",
      "        335 |   0.484297  |    0.201821     |   1\n",
      "        336 |   0.787132  |    0.131751     |   1\n",
      "        337 |   0.668990  |    0.154503     |   1\n",
      "        338 |   0.794797  |    0.177150     |   1\n",
      "        339 |   0.614996  |    0.054335     |   0\n",
      "        340 |   0.782158  |    0.057988     |   0\n",
      "        341 |   0.678657  |    0.094345     |   0\n",
      "        342 |   0.534844  |    0.064560     |   0\n",
      "        343 |   0.569383  |    0.108358     |   0\n",
      "        344 |   0.678736  |    0.138598     |   1\n",
      "        345 |   0.192571  |    0.057061     |   2\n",
      "        346 |   0.627076  |    0.091055     |   0\n",
      "        347 |   0.564329  |    0.080890     |   0\n",
      "        348 |   0.660996  |    0.074896     |   0\n",
      "        349 |   0.453346  |    0.078193     |   0\n",
      "        350 |   0.603221  |    0.187443     |   1\n",
      "        351 |   0.357085  |    0.058659     |   0\n",
      "        352 |   0.657284  |    0.064295     |   0\n",
      "        353 |   0.648771  |    0.086860     |   0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 354: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        354 |   0.155881  |    0.072002     |   2\n",
      "        355 |   0.648370  |    0.091960     |   0\n",
      "        356 |   0.537856  |    0.145057     |   1\n",
      "        357 |   0.662863  |    0.143266     |   1\n",
      "        358 |   0.125009  |    0.063656     |   2\n",
      "        359 |   0.635757  |    0.194778     |   1\n",
      "        360 |   0.553798  |    0.146309     |   1\n",
      "        361 |   0.670696  |    0.217795     |   1\n",
      "        362 |   0.692971  |    0.096796     |   1\n",
      "        363 |   0.547596  |    0.081220     |   0\n",
      "        364 |   0.480325  |    0.190007     |   1\n",
      "        365 |   0.587993  |    0.056026     |   0\n",
      "        366 |   0.172975  |    0.063159     |   2\n",
      "        367 |   0.531608  |    0.196569     |   1\n",
      "        368 |   0.535114  |    0.142145     |   1\n",
      "        369 |   0.177192  |    0.060552     |   2\n",
      "        370 |   0.072625  |    0.076885     |   2\n",
      "        371 |   0.655201  |    0.104842     |   0\n",
      "        372 |   0.154957  |    0.067431     |   2\n",
      "        373 |   0.724325  |    0.197021     |   1\n",
      "        374 |   0.618240  |    0.103974     |   1\n",
      "        375 |   0.515475  |    0.077214     |   0\n",
      "        376 |   0.141988  |    0.068440     |   2\n",
      "        377 |   0.486643  |    0.080894     |   0\n",
      "        378 |   0.668651  |    0.148857     |   1\n",
      "        379 |   0.601402  |    0.088084     |   0\n",
      "        380 |   0.589288  |    0.153397     |   1\n",
      "        381 |   0.156456  |    0.055624     |   2\n",
      "        382 |   0.526407  |    0.092472     |   0\n",
      "        383 |   0.448220  |    0.197890     |   1\n",
      "        384 |   0.550239  |    0.099288     |   1\n",
      "        385 |   0.133762  |    0.082684     |   2\n",
      "        386 |   0.040153  |    0.074564     |   2\n",
      "        387 |   0.675001  |    0.191773     |   1\n",
      "        388 |   0.630380  |    0.053916     |   0\n",
      "        389 |   0.000850  |    0.064325     |   2\n",
      "        390 |   0.523451  |    0.192234     |   1\n",
      "        391 |   0.432614  |    0.052057     |   0\n",
      "        392 |   0.017050  |    0.066868     |   2\n",
      "        393 |   0.269145  |    0.070525     |   2\n",
      "        394 |   0.614058  |    0.214912     |   1\n",
      "        395 |   0.604201  |    0.094424     |   0\n",
      "        396 |   0.655775  |    0.154787     |   1\n",
      "        397 |   0.673212  |    0.193071     |   1\n",
      "        398 |   0.131703  |    0.061708     |   2\n",
      "        399 |   0.542414  |    0.116557     |   0\n",
      "        400 |   0.466015  |    0.149862     |   1\n",
      "        401 |   0.564583  |    0.061375     |   0\n",
      "        402 |   0.582810  |    0.091761     |   0\n",
      "        403 |   0.516634  |    0.066232     |   0\n",
      "        404 |   0.134289  |    0.078943     |   2\n",
      "        405 |   0.604866  |    0.193317     |   1\n",
      "        406 |   0.110749  |    0.049733     |   2\n",
      "        407 |   0.546272  |    0.054631     |   0\n",
      "        408 |   0.538677  |    0.091840     |   0\n",
      "        409 |   0.038529  |    0.064064     |   2\n",
      "        410 |   0.113353  |    0.084446     |   2\n",
      "        411 |   0.506821  |    0.075435     |   0\n",
      "        412 |   0.114767  |    0.064546     |   2\n",
      "        413 |   0.000675  |    0.063488     |   2\n",
      "        414 |   0.000782  |    0.078353     |   2\n",
      "        415 |   0.575151  |    0.205591     |   1\n",
      "        416 |   0.491771  |    0.187831     |   1\n",
      "        417 |   0.413162  |    0.134697     |   1\n",
      "        418 |   0.494070  |    0.064343     |   0\n",
      "        419 |   0.000505  |    0.067022     |   2\n",
      "        420 |   0.437522  |    0.095716     |   0\n",
      "        421 |   0.674861  |    0.158486     |   1\n",
      "        422 |   0.591225  |    0.139884     |   1\n",
      "        423 |   0.000702  |    0.053914     |   2\n",
      "        424 |   0.490934  |    0.088329     |   0\n",
      "        425 |   0.000932  |    0.065292     |   2\n",
      "        426 |   0.637039  |    0.213556     |   1\n",
      "        427 |   0.433590  |    0.142320     |   1\n",
      "        428 |   0.491085  |    0.077479     |   0\n",
      "        429 |   0.570137  |    0.068046     |   0\n",
      "        430 |   0.000838  |    0.075056     |   2\n",
      "        431 |   0.142660  |    0.077861     |   2\n",
      "        432 |   0.675713  |    0.148201     |   1\n",
      "        433 |   0.502790  |    0.093378     |   0\n",
      "        434 |   0.451957  |    0.190285     |   1\n",
      "        435 |   0.174601  |    0.049993     |   2\n",
      "        436 |   0.482496  |    0.066020     |   0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 437: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        437 |   0.603690  |    0.184558     |   1\n",
      "        438 |   0.550887  |    0.057520     |   0\n",
      "        439 |   0.453298  |    0.054121     |   0\n",
      "        440 |   0.473305  |    0.087495     |   0\n",
      "        441 |   0.566711  |    0.075351     |   0\n",
      "        442 |   0.506461  |    0.093721     |   0\n",
      "        443 |   0.155398  |    0.054431     |   2\n",
      "        444 |   0.497247  |    0.081111     |   0\n",
      "        445 |   0.115853  |    0.077398     |   2\n",
      "        446 |   0.161964  |    0.070587     |   2\n",
      "        447 |   0.580853  |    0.095612     |   0\n",
      "        448 |   0.435876  |    0.205904     |   1\n",
      "        449 |   0.603808  |    0.105653     |   1\n",
      "        450 |   0.597706  |    0.202702     |   1\n",
      "        451 |   0.570732  |    0.057621     |   0\n",
      "        452 |   0.660202  |    0.209776     |   1\n",
      "        453 |   0.382336  |    0.052802     |   0\n",
      "        454 |   0.174785  |    0.070274     |   2\n",
      "        455 |   0.535391  |    0.201250     |   1\n",
      "        456 |   0.427185  |    0.071427     |   0\n",
      "        457 |   0.527889  |    0.096218     |   0\n",
      "        458 |   0.590123  |    0.179686     |   1\n",
      "        459 |   0.474967  |    0.120542     |   1\n",
      "        460 |   0.062461  |    0.089613     |   2\n",
      "        461 |   0.134527  |    0.064557     |   2\n",
      "        462 |   0.585912  |    0.080544     |   0\n",
      "        463 |   0.484109  |    0.234808     |   1\n",
      "        464 |   0.553275  |    0.082682     |   1\n",
      "        465 |   0.566497  |    0.074414     |   0\n",
      "        466 |   0.427757  |    0.076177     |   0\n",
      "        467 |   0.116188  |    0.076577     |   2\n",
      "        468 |   0.547225  |    0.087047     |   0\n",
      "        469 |   0.584357  |    0.070844     |   0\n",
      "        470 |   0.517145  |    0.067953     |   0\n",
      "        471 |   0.437741  |    0.076626     |   0\n",
      "        472 |   0.523844  |    0.162834     |   1\n",
      "        473 |   0.513332  |    0.187229     |   1\n",
      "        474 |   0.136644  |    0.058736     |   2\n",
      "        475 |   0.538548  |    0.187034     |   1\n",
      "        476 |   0.528027  |    0.053603     |   0\n",
      "        477 |   0.132718  |    0.061527     |   2\n",
      "        478 |   0.035840  |    0.092387     |   2\n",
      "        479 |   0.000878  |    0.053208     |   2\n",
      "        480 |   0.418504  |    0.099287     |   0\n",
      "        481 |   0.694021  |    0.138053     |   1\n",
      "        482 |   0.439520  |    0.064711     |   0\n",
      "        483 |   0.566198  |    0.094397     |   0\n",
      "        484 |   0.550586  |    0.060926     |   0\n",
      "        485 |   0.014786  |    0.085010     |   2\n",
      "        486 |   0.481661  |    0.088162     |   0\n",
      "        487 |   0.470409  |    0.090379     |   0\n",
      "        488 |   0.513244  |    0.216832     |   1\n",
      "        489 |   0.256541  |    0.051137     |   2\n",
      "        490 |   0.120647  |    0.060918     |   2\n",
      "        491 |   0.119965  |    0.055405     |   2\n",
      "        492 |   0.476718  |    0.089190     |   0\n",
      "        493 |   0.477708  |    0.205429     |   1\n",
      "        494 |   0.084126  |    0.048004     |   2\n",
      "        495 |   0.037211  |    0.053012     |   2\n",
      "        496 |   0.443526  |    0.215914     |   1\n",
      "        497 |   0.101224  |    0.053298     |   2\n",
      "        498 |   0.451569  |    0.051382     |   0\n",
      "        499 |   0.433326  |    0.202379     |   1\n",
      "        500 |   0.474426  |    0.137227     |   1\n",
      "        501 |   0.564306  |    0.122868     |   1\n",
      "        502 |   0.455115  |    0.076225     |   0\n",
      "        503 |   0.410547  |    0.188421     |   1\n",
      "        504 |   0.508523  |    0.048183     |   0\n",
      "        505 |   0.531487  |    0.094147     |   0\n",
      "        506 |   0.472179  |    0.207999     |   1\n",
      "        507 |   0.483822  |    0.093551     |   0\n",
      "        508 |   0.427841  |    0.128838     |   1\n",
      "        509 |   0.410996  |    0.085201     |   0\n",
      "        510 |   0.507975  |    0.187295     |   1\n",
      "        511 |   0.496111  |    0.058825     |   0\n",
      "        512 |   0.575243  |    0.199977     |   1\n",
      "        513 |   0.105278  |    0.061744     |   2\n",
      "        514 |   0.504310  |    0.137335     |   1\n",
      "        515 |   0.000657  |    0.069207     |   2\n",
      "        516 |   0.519033  |    0.151479     |   1\n",
      "        517 |   0.000734  |    0.104288     |   2\n",
      "        518 |   0.394099  |    0.064416     |   0\n",
      "        519 |   0.000526  |    0.076163     |   2\n",
      "        520 |   0.496341  |    0.186579     |   1\n",
      "        521 |   0.000756  |    0.060963     |   2\n",
      "        522 |   0.520367  |    0.132308     |   1\n",
      "        523 |   0.511234  |    0.059120     |   0\n",
      "        524 |   0.000953  |    0.080114     |   2\n",
      "        525 |   0.502062  |    0.191040     |   1\n",
      "        526 |   0.000879  |    0.050602     |   2\n",
      "        527 |   0.118033  |    0.053475     |   2\n",
      "        528 |   0.174668  |    0.077671     |   2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 529: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        529 |   0.149814  |    0.075467     |   2\n",
      "        530 |   0.535266  |    0.192678     |   1\n",
      "        531 |   0.481042  |    0.139578     |   1\n",
      "        532 |   0.382412  |    0.073447     |   0\n",
      "        533 |   0.491251  |    0.190658     |   1\n",
      "        534 |   0.430680  |    0.054395     |   0\n",
      "        535 |   0.444029  |    0.059347     |   0\n",
      "        536 |   0.547698  |    0.103502     |   0\n",
      "        537 |   0.405801  |    0.189465     |   1\n",
      "        538 |   0.104193  |    0.067250     |   2\n",
      "        539 |   0.512203  |    0.171393     |   1\n",
      "        540 |   0.147798  |    0.057495     |   2\n",
      "        541 |   0.639356  |    0.200842     |   1\n",
      "        542 |   0.469249  |    0.182107     |   1\n",
      "        543 |   0.161860  |    0.048432     |   2\n",
      "        544 |   0.059101  |    0.051085     |   2\n",
      "        545 |   0.130530  |    0.076575     |   2\n",
      "        546 |   0.600565  |    0.196853     |   1\n",
      "        547 |   0.309366  |    0.059922     |   0\n",
      "        548 |   0.486831  |    0.127885     |   1\n",
      "        549 |   0.512916  |    0.182689     |   1\n",
      "        550 |   0.463087  |    0.051833     |   0\n",
      "        551 |   0.116369  |    0.074499     |   2\n",
      "        552 |   0.453726  |    0.184663     |   1\n",
      "        553 |   0.128850  |    0.072382     |   2\n",
      "        554 |   0.450749  |    0.205733     |   1\n",
      "        555 |   0.455558  |    0.181541     |   1\n",
      "        556 |   0.517131  |    0.062114     |   0\n",
      "        557 |   0.121456  |    0.055838     |   2\n",
      "        558 |   0.489933  |    0.189867     |   1\n",
      "        559 |   0.560138  |    0.056338     |   0\n",
      "        560 |   0.041781  |    0.081604     |   2\n",
      "        561 |   0.332507  |    0.153318     |   1\n",
      "        562 |   0.486393  |    0.069545     |   0\n",
      "        563 |   0.449163  |    0.132986     |   0\n",
      "        564 |   0.487956  |    0.066368     |   0\n",
      "        565 |   0.438493  |    0.200008     |   1\n",
      "        566 |   0.577024  |    0.142046     |   1\n",
      "        567 |   0.000672  |    0.059036     |   2\n",
      "        568 |   0.015034  |    0.099222     |   2\n",
      "        569 |   0.441699  |    0.059180     |   0\n",
      "        570 |   0.311294  |    0.092461     |   0\n",
      "        571 |   0.442385  |    0.075941     |   0\n",
      "        572 |   0.481371  |    0.093016     |   0\n",
      "        573 |   0.466431  |    0.081559     |   0\n",
      "        574 |   0.562891  |    0.183072     |   1\n",
      "        575 |   0.243753  |    0.051822     |   2\n",
      "        576 |   0.598302  |    0.137587     |   1\n",
      "        577 |   0.398082  |    0.085520     |   0\n",
      "        578 |   0.408670  |    0.195166     |   1\n",
      "        579 |   0.453857  |    0.125141     |   1\n",
      "        580 |   0.108157  |    0.060630     |   2\n",
      "        581 |   0.453028  |    0.191135     |   1\n",
      "        582 |   0.112978  |    0.053561     |   2\n",
      "        583 |   0.074865  |    0.060357     |   2\n",
      "        584 |   0.532699  |    0.094626     |   0\n",
      "        585 |   0.438843  |    0.107477     |   0\n",
      "        586 |   0.483075  |    0.187817     |   1\n",
      "        587 |   0.350600  |    0.060116     |   0\n",
      "        588 |   0.398652  |    0.065094     |   0\n",
      "        589 |   0.036641  |    0.064633     |   2\n",
      "        590 |   0.101031  |    0.096331     |   2\n",
      "        591 |   0.098654  |    0.071064     |   2\n",
      "        592 |   0.000548  |    0.095602     |   2\n",
      "        593 |   0.386303  |    0.078396     |   0\n",
      "        594 |   0.371173  |    0.090642     |   0\n",
      "        595 |   0.386592  |    0.143030     |   1\n",
      "        596 |   0.545546  |    0.183250     |   1\n",
      "        597 |   0.447875  |    0.057567     |   0\n",
      "        598 |   0.000581  |    0.085161     |   2\n",
      "        599 |   0.410170  |    0.094227     |   0\n",
      "        600 |   0.000457  |    0.065929     |   2\n",
      "        601 |   0.000594  |    0.071515     |   2\n",
      "        602 |   0.431056  |    0.086831     |   0\n",
      "        603 |   0.000802  |    0.067656     |   2\n",
      "        604 |   0.000912  |    0.083385     |   2\n",
      "        605 |   0.415489  |    0.074125     |   0\n",
      "        606 |   0.562271  |    0.191518     |   1\n",
      "        607 |   0.367879  |    0.055868     |   0\n",
      "        608 |   0.123100  |    0.057240     |   2\n",
      "        609 |   0.293033  |    0.077551     |   0\n",
      "        610 |   0.172317  |    0.068283     |   2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 611: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        611 |   0.136120  |    0.076273     |   2\n",
      "        612 |   0.093338  |    0.062099     |   2\n",
      "        613 |   0.408764  |    0.070936     |   0\n",
      "        614 |   0.385570  |    0.089311     |   0\n",
      "        615 |   0.124995  |    0.062657     |   2\n",
      "        616 |   0.488505  |    0.190644     |   1\n",
      "        617 |   0.152711  |    0.057935     |   2\n",
      "        618 |   0.475325  |    0.194860     |   1\n",
      "        619 |   0.380423  |    0.049043     |   0\n",
      "        620 |   0.049860  |    0.056646     |   2\n",
      "        621 |   0.462185  |    0.229534     |   1\n",
      "        622 |   0.453679  |    0.146456     |   1\n",
      "        623 |   0.388571  |    0.138410     |   1\n",
      "        624 |   0.119593  |    0.057819     |   2\n",
      "        625 |   0.106719  |    0.079552     |   2\n",
      "        626 |   0.120580  |    0.102637     |   2\n",
      "        627 |   0.402404  |    0.165324     |   1\n",
      "        628 |   0.113273  |    0.053530     |   2\n",
      "        629 |   0.399663  |    0.105509     |   0\n",
      "        630 |   0.384351  |    0.055395     |   0\n",
      "        631 |   0.439971  |    0.069429     |   0\n",
      "        632 |   0.033082  |    0.073075     |   2\n",
      "        633 |   0.376865  |    0.076466     |   0\n",
      "        634 |   0.000584  |    0.078236     |   2\n",
      "        635 |   0.409279  |    0.203671     |   1\n",
      "        636 |   0.448094  |    0.156490     |   1\n",
      "        637 |   0.403108  |    0.052665     |   0\n",
      "        638 |   0.381007  |    0.090592     |   0\n",
      "        639 |   0.483290  |    0.075776     |   0\n",
      "        640 |   0.489100  |    0.073082     |   0\n",
      "        641 |   0.015469  |    0.085360     |   2\n",
      "        642 |   0.229798  |    0.074133     |   2\n",
      "        643 |   0.463706  |    0.095956     |   0\n",
      "        644 |   0.114586  |    0.073410     |   2\n",
      "        645 |   0.310928  |    0.178083     |   1\n",
      "        646 |   0.417407  |    0.065155     |   0\n",
      "        647 |   0.117761  |    0.071061     |   2\n",
      "        648 |   0.393448  |    0.197679     |   1\n",
      "        649 |   0.076182  |    0.054943     |   2\n",
      "        650 |   0.420403  |    0.088639     |   0\n",
      "        651 |   0.424971  |    0.062804     |   0\n",
      "        652 |   0.320522  |    0.093571     |   0\n",
      "        653 |   0.385827  |    0.073821     |   0\n",
      "        654 |   0.523699  |    0.191443     |   1\n",
      "        655 |   0.430802  |    0.050998     |   0\n",
      "        656 |   0.414575  |    0.054998     |   0\n",
      "        657 |   0.419868  |    0.191797     |   1\n",
      "        658 |   0.426344  |    0.183479     |   1\n",
      "        659 |   0.416423  |    0.048429     |   0\n",
      "        660 |   0.366965  |    0.068798     |   0\n",
      "        661 |   0.031279  |    0.064674     |   2\n",
      "        662 |   0.089229  |    0.105490     |   2\n",
      "        663 |   0.101074  |    0.059523     |   2\n",
      "        664 |   0.436204  |    0.093470     |   0\n",
      "        665 |   0.000504  |    0.077906     |   2\n",
      "        666 |   0.478662  |    0.181081     |   1\n",
      "        667 |   0.347633  |    0.050347     |   0\n",
      "        668 |   0.000648  |    0.072532     |   2\n",
      "        669 |   0.486748  |    0.195160     |   1\n",
      "        670 |   0.474395  |    0.125634     |   1\n",
      "        671 |   0.412721  |    0.060352     |   0\n",
      "        672 |   0.000432  |    0.070071     |   2\n",
      "        673 |   0.464704  |    0.206644     |   1\n",
      "        674 |   0.345181  |    0.105409     |   1\n",
      "        675 |   0.360893  |    0.071106     |   0\n",
      "        676 |   0.323054  |    0.073661     |   0\n",
      "        677 |   0.525819  |    0.208937     |   1\n",
      "        678 |   0.000607  |    0.052567     |   2\n",
      "        679 |   0.364995  |    0.057637     |   0\n",
      "        680 |   0.328366  |    0.100921     |   0\n",
      "        681 |   0.000709  |    0.072145     |   2\n",
      "        682 |   0.435971  |    0.194297     |   1\n",
      "        683 |   0.531369  |    0.134452     |   1\n",
      "        684 |   0.469619  |    0.134055     |   1\n",
      "        685 |   0.000774  |    0.085639     |   2\n",
      "        686 |   0.112752  |    0.054572     |   2\n",
      "        687 |   0.397922  |    0.090474     |   0\n",
      "        688 |   0.430935  |    0.207405     |   1\n",
      "        689 |   0.450915  |    0.133653     |   1\n",
      "        690 |   0.153070  |    0.056310     |   2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:neuralnilm.trainer:Iteration 691: Finished training epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        691 |   0.367234  |    0.141237     |   1\n",
      "        692 |   0.124598  |    0.112258     |   2\n",
      "        693 |   0.423927  |    0.194221     |   1\n",
      "        694 |   0.434589  |    0.142052     |   1\n",
      "        695 |   0.087513  |    0.068532     |   2\n",
      "        696 |   0.307049  |    0.093813     |   0\n",
      "        697 |   0.506635  |    0.139655     |   1\n",
      "        698 |   0.334359  |    0.100110     |   0\n",
      "        699 |   0.483845  |    0.130169     |   1\n",
      "        700 |   0.338111  |    0.088383     |   0\n",
      "        701 |   0.443748  |    0.075803     |   0\n",
      "        702 |   0.348102  |    0.074604     |   0\n",
      "        703 |   0.401426  |    0.104297     |   0\n",
      "        704 |   0.350167  |    0.201606     |   1\n",
      "        705 |   0.110229  |    0.054582     |   2\n",
      "        706 |   0.291360  |    0.063789     |   0\n",
      "        707 |   0.132819  |    0.072980     |   2\n",
      "        708 |   0.405597  |    0.080455     |   0\n",
      "        709 |   0.470280  |    0.188954     |   1\n",
      "        710 |   0.502434  |    0.143380     |   1\n",
      "        711 |   0.411168  |    0.138353     |   1\n",
      "        712 |   0.052968  |    0.061442     |   2\n",
      "        713 |   0.477391  |    0.185616     |   1\n",
      "        714 |   0.415272  |    0.199208     |   1\n",
      "        715 |   0.386265  |    0.099772     |   1\n",
      "        716 |   0.105372  |    0.062584     |   2\n",
      "        717 |   0.293303  |    0.162492     |   1\n",
      "        718 |   0.099501  |    0.068564     |   2\n",
      "        719 |   0.512170  |    0.184517     |   1\n",
      "        720 |   0.342313  |    0.058954     |   0\n",
      "        721 |   0.371157  |    0.135093     |   1\n",
      "        722 |   0.115601  |    0.075991     |   2\n",
      "        723 |   0.433486  |    0.073343     |   0\n",
      "        724 |   0.397458  |    0.072534     |   0\n",
      "        725 |   0.364042  |    0.065189     |   0\n",
      "        726 |   0.111669  |    0.082472     |   2\n",
      "        727 |   0.035299  |    0.070162     |   2\n",
      "        728 |   0.000507  |    0.068411     |   2\n",
      "        729 |   0.398039  |    0.191215     |   1\n",
      "        730 |   0.354528  |    0.066135     |   0\n",
      "        731 |   0.014673  |    0.062001     |   2\n",
      "        732 |   0.505446  |    0.151759     |   1\n",
      "        733 |   0.421408  |    0.132132     |   1\n",
      "        734 |   0.452906  |    0.065617     |   0\n",
      "        735 |   0.469719  |    0.186040     |   1\n",
      "        736 |   0.205064  |    0.048739     |   2\n",
      "        737 |   0.410940  |    0.061461     |   0\n",
      "        738 |   0.084397  |    0.074964     |   2\n",
      "        739 |   0.371409  |    0.103767     |   0\n",
      "        740 |   0.401756  |    0.140348     |   1\n",
      "        741 |   0.356970  |    0.132790     |   1\n",
      "        742 |   0.282304  |    0.077153     |   0\n",
      "        743 |   0.097699  |    0.082810     |   2\n",
      "        744 |   0.368247  |    0.109654     |   0\n",
      "        745 |   0.353917  |    0.065627     |   0\n",
      "        746 |   0.310339  |    0.082942     |   0\n",
      "        747 |   0.302560  |    0.203807     |   1\n",
      "        748 |   0.076189  |    0.052794     |   2\n",
      "        749 |   0.032034  |    0.061310     |   2\n",
      "        750 |   0.435195  |    0.185040     |   1\n",
      "        751 |   0.388685  |    0.100547     |   1\n",
      "        752 |   0.350234  |    0.088806     |   0\n",
      "        753 |   0.079237  |    0.067386     |   2\n",
      "        754 |   0.271741  |    0.107145     |   0\n",
      "        755 |   0.298784  |    0.148358     |   1\n",
      "        756 |   0.445204  |    0.150996     |   1\n",
      "        757 |   0.430048  |    0.078281     |   0\n",
      "        758 |   0.091402  |    0.067361     |   2\n",
      "        759 |   0.257971  |    0.189167     |   1\n",
      "        760 |   0.370946  |    0.148804     |   1\n",
      "        761 |   0.000456  |    0.054863     |   2\n",
      "        762 |   0.407959  |    0.093338     |   0\n",
      "        763 |   0.356697  |    0.146137     |   1\n",
      "        764 |   0.372731  |    0.078120     |   0\n",
      "        765 |   0.327147  |    0.214010     |   1\n",
      "        766 |   0.420417  |    0.124749     |   1\n",
      "        767 |   0.320782  |    0.234218     |   1\n",
      "        768 |   0.514678  |    0.113955     |   1\n",
      "        769 |   0.000555  |    0.078354     |   2"
     ]
    }
   ],
   "source": [
    "trainer.fit(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.train_iterations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
